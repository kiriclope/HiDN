#+STARTUP: fold
#+PROPERTY: header-args:ipython :results both :exports both :async yes :session overlap :kernel dual_data

* Notebook Settings

#+begin_src ipython
%load_ext autoreload
%autoreload 2
%reload_ext autoreload

%run /home/leon/dual_task/dual_data/notebooks/setup.py
%matplotlib inline
%config InlineBackend.figure_format = 'png'
#+end_src

#+RESULTS:
:RESULTS:
: The autoreload extension is already loaded. To reload it, use:
:   %reload_ext autoreload
: Python exe
: /home/leon/mambaforge/envs/dual_data/bin/python
: <Figure size 700x432.624 with 0 Axes>
:END:

* Imports

#+begin_src ipython
import sys
sys.path.insert(0, '/home/leon/dual_task/dual_data/')

import pickle as pkl
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import circmean
from time import perf_counter

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, TensorDataset, DataLoader
from skorch import NeuralNetClassifier

from sklearn.base import clone
from sklearn.ensemble import BaggingClassifier
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, RepeatedStratifiedKFold, StratifiedKFold, LeaveOneOut

from mne.decoding import SlidingEstimator, cross_val_multiscore, GeneralizingEstimator, get_coef

from src.common.plot_utils import add_vlines, add_vdashed
from src.attractor.energy import run_energy, plot_energy
from src.common.options import set_options
from src.stats.bootstrap import my_boots_ci
from src.decode.bump import decode_bump, circcvl
from src.common.get_data import get_X_y_days, get_X_y_S1_S2
from src.common.options import set_options
from src.preprocess.helpers import avg_epochs

DEVICE = 'cuda:1'
#+end_src

#+RESULTS:

* Helpers
#+begin_src ipython
  def rescale_coefs(model, coefs, bias):

          try:
                  means = model.named_steps["scaler"].mean_
                  scales = model.named_steps["scaler"].scale_

                  # Rescale the coefficients
                  rescaled_coefs = np.true_divide(coefs, scales)

                  # Adjust the intercept
                  rescaled_bias = bias - np.sum(rescaled_coefs * means)

                  return rescaled_coefs, rescaled_bias
          except:
                  return coefs, bias

#+end_src

#+RESULTS:

** Statistics
#+begin_src ipython
  from scipy.stats import bootstrap

  def get_bootstrap_ci(data, statistic=np.mean, confidence_level=0.95, n_resamples=1000, random_state=None):
      result = bootstrap((data,), statistic)
      ci_lower, ci_upper = result.confidence_interval
      return np.array([ci_lower, ci_upper])
#+end_src

#+RESULTS:

** Other
#+begin_src ipython
def convert_seconds(seconds):
    h = seconds // 3600
    m = (seconds % 3600) // 60
    s = seconds % 60
    return h, m, s
#+end_src

#+RESULTS:

#+begin_src ipython
def get_theta(a, b, GM=0, IF_NORM=0):

    u, v = a, b

    if GM:
        v = b - np.dot(b, a) / np.dot(a, a) * a

    if IF_NORM:
        u = a / np.linalg.norm(a)
        v = b / np.linalg.norm(b)

    return np.arctan2(v, u) % (2.0 * np.pi)
#+end_src

#+RESULTS:

#+begin_src ipython
import scipy.stats as stats

def plot_smooth(data, ax, color):
    mean = data.mean(axis=0)
    ci = smooth.std(axis=0, ddof=1) * 1.96

    # Plot
    ax.plot(mean, color=color)
    ax.fill_between(range(data.shape[1]), mean - ci, mean + ci, alpha=0.25, color=color)

#+end_src

#+RESULTS:

** plots
#+begin_src ipython
  def get_energy(X, y, task, num_bins, bins, window, IF_BOOT=0, IF_NORM=0, IF_HMM=0, n_iter=10):
      ci_ = None
      energy_ = run_energy(X, num_bins, bins, task, window, VERBOSE=0, IF_HMM=IF_HMM, n_iter=n_iter)
      if IF_BOOT:
          _, ci_ = my_boots_ci(X, lambda x: run_energy(x, num_bins, bins, task, window, IF_HMM=IF_HMM, n_iter=n_iter), n_samples=1000)
      if ci_ is not None:
          ci_ = ci_ / 2.0
      return energy_, ci_
#+end_src

#+RESULTS:

#+begin_src ipython
def plot_theta_energy(theta, energy, ci=None, window=.9, ax=None, SMOOTH=0, color='r'):
    if ax is None:
        fig, ax = plt.subplots()

    theta = np.linspace(0, 360, energy.shape[0], endpoint=False)
    energy = energy[1:]
    theta = theta[1:]

    windowSize = int(window * energy.shape[0])
    if SMOOTH:
        # window = np.ones(windowSize) / windowSize
        # energy = np.convolve(energy, window, mode='same')
        energy = circcvl(energy, windowSize=windowSize)

    ax.plot(theta, energy * 100, lw=4, color=color)

    if ci is not None:
        ax.fill_between(
            theta,
            (energy - ci[:, 0]) * 100,
            (energy + ci[:, 1]) * 100,
            alpha=0.1, color=color
        )

    ax.set_ylabel('Energy')
    ax.set_xlabel('Pref. Location (°)')
    ax.set_xticks([0, 90, 180, 270, 360])
#+end_src

#+RESULTS:

#+begin_src ipython
def pkl_save(obj, name, path="."):
    pkl.dump(obj, open(path + "/" + name + ".pkl", "wb"))


def pkl_load(name, path="."):
    return pkl.load(open(path + "/" + name, "rb"))

#+end_src

#+RESULTS:

* Perceptron

#+begin_src ipython
class CustomBCEWithLogitsLoss(nn.BCEWithLogitsLoss):
    def forward(self, input, target):
        target = target.view(-1, 1)  # Make sure target shape is (n_samples, 1)
        return super().forward(input.to(torch.float32), target.to(torch.float32))
#+end_src

#+RESULTS:

#+begin_src ipython :tangle ../src/decode/perceptron.py
class Perceptron(nn.Module):
    def __init__(self, num_features, dropout_rate=0.0):
        super(Perceptron, self).__init__()
        self.linear = nn.Linear(num_features, 1)
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, x):
        x = self.dropout(x)
        hidden = self.linear(x)
        return hidden
#+end_src

#+RESULTS:

#+begin_src ipython
  class MLP(nn.Module):
      def __init__(self, num_features, hidden_units=64, dropout_rate=0.5):
          super(MLP, self).__init__()
          self.linear = nn.Linear(num_features, hidden_units)
          self.dropout = nn.Dropout(dropout_rate)
          self.relu = nn.ReLU()
          self.linear2 = nn.Linear(hidden_units, 1)

      def forward(self, x):
          x = self.dropout(x)
          x = self.relu(self.linear(x))
          x = self.dropout(x)
          hidden = self.linear2(x)
          return hidden
#+end_src

#+RESULTS:


#+begin_src ipython
from skorch.callbacks import Callback
from skorch.callbacks import EarlyStopping

early_stopping = EarlyStopping(
    monitor='train_loss',    # Metric to monitor
    patience=5,              # Number of epochs to wait for improvement
    threshold=0.001,       # Minimum change to qualify as an improvement
    threshold_mode='rel',    # 'rel' for relative change, 'abs' for absolute change
    lower_is_better=True     # Set to True if lower metric values are better
)

#+end_src

#+RESULTS:


#+begin_src ipython
class RegularizedNet(NeuralNetClassifier):
    def __init__(self, module, alpha=0.001, l1_ratio=0.95, **kwargs):
        self.alpha = alpha  # Regularization strength
        self.l1_ratio = l1_ratio # Balance between L1 and L2 regularization

        super().__init__(module, **kwargs)

    def get_loss(self, y_pred, y_true, X=None, training=False):
        # Call super method to compute primary loss
        if y_pred.shape != y_true.shape:
            y_true = y_true.unsqueeze(-1)

        loss = super().get_loss(y_pred, y_true, X=X, training=training)

        if self.alpha>0:
            elastic_net_reg = 0
            for param in self.module_.parameters():
                elastic_net_reg += self.alpha * self.l1_ratio * torch.sum(torch.abs(param))
                elastic_net_reg += self.alpha * (1 - self.l1_ratio) * torch.sum(param ** 2)

        # Add the elastic net regularization term to the primary loss
        return loss + elastic_net_reg
#+end_src

#+RESULTS:

* Decoding vs days
** Helpers

#+begin_src ipython
  def hyper_tune(model, epoch, params, scoring, **options):

      # load data
      X_days, y_days = get_X_y_days(**options)
      X, y = get_X_y_S1_S2(X_days, y_days, **options)
      y[y==-1] = 0

      options['epochs'] = [epoch]
      X_avg = avg_epochs(X, **options).astype('float32')
      print('X', X.shape, 'y', y.shape)

      # cv = 3
      cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=10)
      # cv = LeaveOneOut()
      # Perform grid search
      grid = GridSearchCV(model, params, refit=True, cv=cv, scoring=scoring, n_jobs=30)
      start = perf_counter()
      print('hyperparam fitting ...')
      grid.fit(X_avg, y)
      end = perf_counter()
      print("Elapsed (with compilation) = %dh %dm %ds" % convert_seconds(end - start))

      best_model = grid.best_estimator_
      best_params = grid.best_params_
      print(best_params)

      scores = None
      # if refit true the best model is refitted to the whole dataset
      # coefs = best_model.named_steps['net'].module_.linear.weight.data.cpu().detach().numpy()[0]
      # bias = best_model.named_steps['net'].module_.linear.bias.data.cpu().detach().numpy()[0]

      # coefs, bias = rescale_coefs(best_model, coefs, bias)

      # print('Computing overlaps')
      # X_T = torch.transpose(torch.tensor(X, device=DEVICE), 1, 2).to(torch.float32)
      # overlaps = best_model.named_steps['net'].module_(X_T).detach().cpu().numpy()
      # overlaps = (np.swapaxes(X, 1, -1) @ coefs + bias) / np.linalg.norm(coefs)

      # # bootstrapped coefficients
      start = perf_counter()
      print('Bagging best model ...')
      bagging_clf = BaggingClassifier(base_estimator=best_model, n_estimators=64)
      bagging_clf.fit(X_avg, y)
      end = perf_counter()
      print("Elapsed (with compilation) = %dh %dm %ds" % convert_seconds(end - start))

      coefs, bias = get_bagged_coefs(bagging_clf, n_estimators=64)

      # overlaps
      # print('Computing overlaps')
      # overlaps = model.named_steps['net'].module_(torch.transpose(torch.tensor(X, device=DEVICE), 1, 2)).detach().cpu().numpy()
      # overlaps = -(np.swapaxes(X, 1, -1) @ coefs + bias) / coefs.shape[-1] * 100 # / np.linalg.norm(coefs)
      if options['trials'] == 'correct':
          options['trials'] = ''
          X, y = get_X_y_S1_S2(X_days, y_days, **options)

      overlaps = -(np.swapaxes(X, 1, -1) @ coefs) / np.linalg.norm(coefs)

      return overlaps, scores, coefs, bias
#+end_src

#+RESULTS:

#+begin_src ipython
  def get_bagged_coefs(clf, n_estimators):
      coefs_list = []
      bias_list = []
      for i in range(n_estimators):
          model = clf.estimators_[i]
          coefs = model.named_steps['net'].module_.linear.weight.data.cpu().detach().numpy()[0]
          bias = model.named_steps['net'].module_.linear.bias.data.cpu().detach().numpy()[0]

          coefs, bias = rescale_coefs(model, coefs, bias)

          coefs_list.append(coefs)
          bias_list.append(bias)

      return np.array(coefs_list).mean(0), np.array(bias_list).mean(0)
#+end_src

#+RESULTS:

#+begin_src ipython
  from sklearn.base import BaseEstimator, TransformerMixin

  class LinearLayerScorer(BaseEstimator, TransformerMixin):
      def __init__(self, model):
          self.model = model

      def fit(self, X, y=None):
          return self

      def transform(self, X, y=None):
          # Ensure the model is in evaluation mode
          self.model.net.module_.eval()
          with torch.no_grad():
              # Retrieve the linear layer from the model
              linear_layer = self.model.net.module_.linear
              # Compute the output of the linear layer
              linear_output = linear_layer(torch.tensor(X, dtype=torch.float32))
          return -linear_output.numpy()
#+end_src

#+RESULTS:

** Parameters

#+begin_src ipython
  mice = ['ChRM04','JawsM15', 'JawsM18', 'ACCM03', 'ACCM04']
  tasks = ['DPA', 'DualGo', 'DualNoGo']

  kwargs = {
      'mouse': 'ACCM03',
      'trials': '', 'reload': 0, 'data_type': 'dF', 'preprocess': False,
      'scaler_BL': 'robust', 'avg_noise':True, 'unit_var_BL':False,
      'random_state': None, 'T_WINDOW': 0.0,
      'l1_ratio': 0.95,
  }
#+end_src

#+RESULTS:

** Fit

#+begin_src ipython
  options = set_options(**kwargs)
  options['day'] = 1
  X_days, y_days = get_X_y_days(**options)
  X_data, y_data = get_X_y_S1_S2(X_days, y_days, **options)

  net = RegularizedNet(
      module=Perceptron,
      module__num_features=X_data.shape[1],
      module__dropout_rate=0.0,
      alpha=0.01,
      l1_ratio=options['l1_ratio'],
      criterion=CustomBCEWithLogitsLoss,
      optimizer=optim.Adam,
      optimizer__lr=0.1,
      max_epochs=1000,
      callbacks=[early_stopping],
      train_split=None,
      iterator_train__shuffle=False,  # Ensure the data is shuffled each epoch
      verbose=0,
      device= DEVICE if torch.cuda.is_available() else 'cpu',  # Assuming you might want to use CUDA
  )

  pipe = []
  # pipe.append(("scaler", StandardScaler()))
  pipe.append(("net", net))
  pipe = Pipeline(pipe)
#+end_src

#+RESULTS:
: Loading files from /home/leon/dual_task/dual_data/data/ACCM03
: DATA: FEATURES sample TASK DualGo TRIALS  DAYS 1 LASER 0

#+begin_src ipython
  params = {
      'net__alpha': np.logspace(-4, 4, 10),
      # 'net__l1_ratio': np.linspace(0, 1, 10),
      # 'net__module__dropout_rate': np.linspace(0, 1, 10),
  }

  scores_sample = []
  overlaps_sample = []
  coefs_sample = []
  bias_sample = []

  scores_dist = []
  overlaps_dist = []
  coefs_dist = []
  bias_dist = []

  scores_choice = []
  overlaps_choice = []
  coefs_choice = []
  bias_choice = []

  options['reload'] = 0
  options['task'] = 'Dual'
  scoring = 'f1_weighted'

  # days = ['first', 'last']
  days = np.arange(1, options['n_days']+1)

  for day in days:
      options['day'] = day

      # options['task'] = 'all'
      options['features'] = 'sample'
      overlaps, scores, coefs, bias = hyper_tune(pipe, epoch='ED', params=params, scoring=scoring, **options)

      scores_sample.append(scores)
      overlaps_sample.append(overlaps)
      coefs_sample.append(coefs)
      bias_sample.append(bias)

      options['task'] = 'Dual'
      options['features'] = 'distractor'
      overlaps, scores, coefs, bias = hyper_tune(pipe, epoch='MD', params=params, scoring=scoring, **options)

      scores_dist.append(scores)
      overlaps_dist.append(overlaps)
      coefs_dist.append(coefs)
      bias_dist.append(bias)

      # options['task'] = 'all'
      options['features'] = 'choice'
      overlaps, scores, coefs, bias = hyper_tune(pipe, epoch='CHOICE', params=params, scoring=scoring, **options)

      scores_choice.append(scores)
      overlaps_choice.append(overlaps)
      coefs_choice.append(coefs)
      bias_choice.append(bias)
#+end_src

#+RESULTS:
#+begin_example
  Loading files from /home/leon/dual_task/dual_data/data/ACCM03
  DATA: FEATURES sample TASK Dual TRIALS  DAYS 1 LASER 0
  X (128, 361, 84) y (128,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 2s
  {'net__alpha': 0.000774263682681127}
  Bagging best model ...
  Elapsed (with compilation) = 0h 0m 2s
  Loading files from /home/leon/dual_task/dual_data/data/ACCM03
  DATA: FEATURES distractor TASK Dual TRIALS  DAYS 1 LASER 0
  X (128, 361, 84) y (128,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 7s
  {'net__alpha': 0.005994842503189409}
  Bagging best model ...
  Elapsed (with compilation) = 0h 0m 16s
  Loading files from /home/leon/dual_task/dual_data/data/ACCM03
  DATA: FEATURES choice TASK Dual TRIALS  DAYS 1 LASER 0
  X (128, 361, 84) y (128,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 4s
  {'net__alpha': 0.046415888336127774}
  Bagging best model ...
  Elapsed (with compilation) = 0h 0m 1s
  Loading files from /home/leon/dual_task/dual_data/data/ACCM03
  DATA: FEATURES sample TASK Dual TRIALS  DAYS 2 LASER 0
  X (128, 361, 84) y (128,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 2s
  {'net__alpha': 0.005994842503189409}
  Bagging best model ...
  Elapsed (with compilation) = 0h 0m 17s
  Loading files from /home/leon/dual_task/dual_data/data/ACCM03
  DATA: FEATURES distractor TASK Dual TRIALS  DAYS 2 LASER 0
  X (128, 361, 84) y (128,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 2s
  {'net__alpha': 0.005994842503189409}
  Bagging best model ...
  Elapsed (with compilation) = 0h 0m 15s
  Loading files from /home/leon/dual_task/dual_data/data/ACCM03
  DATA: FEATURES choice TASK Dual TRIALS  DAYS 2 LASER 0
  X (128, 361, 84) y (128,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 2s
  {'net__alpha': 0.005994842503189409}
  Bagging best model ...
  Elapsed (with compilation) = 0h 0m 10s
  Loading files from /home/leon/dual_task/dual_data/data/ACCM03
  DATA: FEATURES sample TASK Dual TRIALS  DAYS 3 LASER 0
  X (128, 361, 84) y (128,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 2s
  {'net__alpha': 0.046415888336127774}
  Bagging best model ...
  Elapsed (with compilation) = 0h 0m 7s
  Loading files from /home/leon/dual_task/dual_data/data/ACCM03
  DATA: FEATURES distractor TASK Dual TRIALS  DAYS 3 LASER 0
  X (128, 361, 84) y (128,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 4s
  {'net__alpha': 0.005994842503189409}
  Bagging best model ...
  Elapsed (with compilation) = 0h 0m 13s
  Loading files from /home/leon/dual_task/dual_data/data/ACCM03
  DATA: FEATURES choice TASK Dual TRIALS  DAYS 3 LASER 0
  X (128, 361, 84) y (128,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 3s
  {'net__alpha': 0.000774263682681127}
  Bagging best model ...
  Elapsed (with compilation) = 0h 0m 3s
  Loading files from /home/leon/dual_task/dual_data/data/ACCM03
  DATA: FEATURES sample TASK Dual TRIALS  DAYS 4 LASER 0
  X (128, 361, 84) y (128,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 3s
  {'net__alpha': 0.005994842503189409}
  Bagging best model ...
  Elapsed (with compilation) = 0h 0m 18s
  Loading files from /home/leon/dual_task/dual_data/data/ACCM03
  DATA: FEATURES distractor TASK Dual TRIALS  DAYS 4 LASER 0
  X (128, 361, 84) y (128,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 2s
  {'net__alpha': 0.005994842503189409}
  Bagging best model ...
  Elapsed (with compilation) = 0h 0m 10s
  Loading files from /home/leon/dual_task/dual_data/data/ACCM03
  DATA: FEATURES choice TASK Dual TRIALS  DAYS 4 LASER 0
  X (128, 361, 84) y (128,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 3s
  {'net__alpha': 9.999999999999999e-05}
  Bagging best model ...
  Elapsed (with compilation) = 0h 0m 11s
  Loading files from /home/leon/dual_task/dual_data/data/ACCM03
  DATA: FEATURES sample TASK Dual TRIALS  DAYS 5 LASER 0
  X (128, 361, 84) y (128,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 2s
  {'net__alpha': 0.000774263682681127}
  Bagging best model ...
  Elapsed (with compilation) = 0h 0m 2s
  Loading files from /home/leon/dual_task/dual_data/data/ACCM03
  DATA: FEATURES distractor TASK Dual TRIALS  DAYS 5 LASER 0
  X (128, 361, 84) y (128,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 1s
  {'net__alpha': 0.000774263682681127}
  Bagging best model ...
  Elapsed (with compilation) = 0h 0m 2s
  Loading files from /home/leon/dual_task/dual_data/data/ACCM03
  DATA: FEATURES choice TASK Dual TRIALS  DAYS 5 LASER 0
  X (128, 361, 84) y (128,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 4s
  {'net__alpha': 9.999999999999999e-05}
  Bagging best model ...
  Elapsed (with compilation) = 0h 0m 19s
#+end_example

#+begin_src ipython
  try:
      overlaps_sample = np.array(overlaps_sample)
      overlaps_dist = np.array(overlaps_dist)
      overlaps_choice = np.array(overlaps_choice)

      scores_sample = np.array(scores_sample)
      scores_dist = np.array(scores_dist)
      scores_choice = np.array(scores_choice)

      coefs_sample = np.array(coefs_sample)
      coefs_dist = np.array(coefs_dist)
      coefs_choice = np.array(coefs_choice)
  except:
      pass
#+end_src

#+RESULTS:

#+begin_src ipython
  try:
      print('overlaps', overlaps_sample.shape, overlaps_dist.shape, overlaps_choice.shape)
      print('scores', scores_sample.shape, scores_dist.shape, scores_choice.shape)
      print('coefs', coefs_sample.shape, coefs_dist.shape, coefs_choice.shape)
  except:
      pass
#+end_src

#+RESULTS:
: overlaps (5, 128, 84) (5, 128, 84) (5, 128, 84)
: scores (5,) (5,) (5,)
: coefs (5, 361) (5, 361) (5, 361)

* Save

#+begin_src ipython
  overlaps_save = np.stack((overlaps_sample, overlaps_dist, overlaps_choice))
  print(overlaps_save.shape)
  pkl_save(overlaps_save, '%s_overlaps_%.2f_l1_ratio' % (options['mouse'], options['l1_ratio']), path="../data/%s/" % options['mouse'])
  # pkl_save(overlaps, '%s_overlaps_%.2f_l1_ratio_loocv' % (options['mouse'], options['l1_ratio']), path="../data/%s/" % options['mouse'])
  # options['l1_ratio'] = 'tuned'
  # pkl_save(overlaps, '%s_overlaps_%s_l1_ratio' % (options['mouse'], options['l1_ratio']), path="../data/%s/" % options['mouse'])

  # scores = np.stack((scores_sample, scores_dist, scores_choice))
  # print(scores.shape)
  # pkl_save(scores, '%s_scores' % options['mouse'], path="../data/%s/" % options['mouse'])

  coefs_save = np.stack((coefs_sample, coefs_dist, coefs_choice))
  print(coefs_save.shape)
  pkl_save(coefs, '%s_coefs_%.2f_l1_ratio' % (options['mouse'], options['l1_ratio']), path="../data/%s/" % options['mouse'])
  # pkl_save(coefs, '%s_coefs_%.2f_l1_ratio_loocv' % (options['mouse'], options['l1_ratio']), path="../data/%s/" % options['mouse'])
  # pkl_save(coefs, '%s_coefs_%s_l1_ratio' % (options['mouse'], options['l1_ratio']), path="../data/%s/" % options['mouse'])

  #+end_src

#+RESULTS:
: (3, 5, 128, 84)
: (3, 5, 361)

#+begin_src ipython
  # overlaps = pkl_load('%s_overlaps_%.2f_l1_ratio_loocv' % (options['mouse'], options['l1_ratio']), path="../data/%s/" % options['mouse'])
#+end_src

#+RESULTS:

* Overlaps

#+begin_src ipython
  def get_overlaps(coefs, bias, **options):
          X_days, y_days = get_X_y_days(**options)
          X, y = get_X_y_S1_S2(X_days, y_days, **options)
          print(X.shape)
          return (np.swapaxes(X, 1, -1) @ coefs + bias) / np.linalg.norm(coefs)
#+end_src

#+RESULTS:

#+begin_comment
#+begin_src ipython
  options['features'] = 'sample'
  options['task'] = 'DualGo'

  overlaps_sample2 = []
  for day in range(1, 7):
      options['day'] = day
      overlaps_sample2.append(get_overlaps(coefs_sample[day-1], bias_sample[day-1], **options))
  overlaps_sample2 = np.array(overlaps_sample2)

  print(overlaps_sample2.shape)

  options['features'] = 'choice'
  options['task'] = 'DualGo'

  overlaps_choice2 = []
  for day in range(1, 7):
      options['day'] = day
      overlaps_choice2.append(get_overlaps(coefs_choice[day-1], bias_choice[day-1], **options))
  overlaps_choice2 = np.array(overlaps_choice2)

  print(overlaps_choice2.shape)
    #+end_src
#+END_comment

#+begin_src ipython
  cmap = plt.get_cmap('Blues')
  colors = [cmap((i+1) / options['n_days'] ) for i in range(options['n_days'])]
  cmap = plt.get_cmap('Reds')
  colors2 = [cmap((i+1) / options['n_days'] ) for i in range(options['n_days'])]
  width = 6
  golden_ratio = (5**.5 - 1) / 2
  size = overlaps_sample.shape[1] // 2

  fig, ax = plt.subplots(1, 3, figsize= [2.5 * width, height])

  for i in range(options['n_days']):
      ax[0].plot(circcvl(overlaps_sample[i][:size].mean(0), windowSize=2), label=i+1, color = colors[i]);
      ax[1].plot(circcvl(overlaps_dist[i][:size].mean(0), windowSize=2), label=i+1, color = colors[i]);
      ax[2].plot(circcvl(overlaps_choice[i][:size].mean(0), windowSize=2), label=i+1, color = colors[i]);

      ax[0].plot(circcvl(overlaps_sample[i][size:].mean(0), windowSize=2), label=i+1, color = colors2[i]);
      ax[1].plot(circcvl(overlaps_dist[i][size:].mean(0), windowSize=2), label=i+1, color = colors2[i]);
      ax[2].plot(circcvl(overlaps_choice[i][size:].mean(0), windowSize=2), label=i+1, color = colors2[i]);

  # ax[2].legend(fontsize=10)
  ax[0].set_xlabel('Step')
  ax[1].set_xlabel('Step')
  ax[2].set_xlabel('Step')
  ax[0].set_ylabel('Sample Overlap')
  ax[1].set_ylabel('Distractor Overlap')
  ax[2].set_ylabel('Choice Overlap')

  plt.savefig('%s_overlaps.svg' % options['mouse'], dpi=300)
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/d60561ff9215b99d81739629f7a99df27ca43f64.png]]

#+begin_src ipython
    options['T_WINDOW'] = 0
    size = overlaps_sample.shape[1] // 2
    options['epochs'] = ['STIM_ED']
    sample_avg = []
    sample_ci = []

    for i in range(options['n_days']):
        sample_epoch = avg_epochs(-overlaps_sample[i][size:] + overlaps_sample[i][:size], **options) / 2.0
        sample_avg.append(sample_epoch.mean(0))
        sample_ci.append(get_bootstrap_ci(sample_epoch))

    sample_avg = np.array(sample_avg)
    sample_ci = np.array(sample_ci).T

    plt.plot(np.arange(1, options['n_days']+1), sample_avg, '-o', label='%s Sample' % options['epochs'][0], color='r')
    plt.fill_between(np.arange(1, options['n_days']+1), sample_ci[0], sample_ci[1], color='r', alpha=0.1)

    size = overlaps_dist.shape[1] // 2
    options['epochs'] = ['STIM_ED']
    dist_avg = []
    dist_ci = []
    for i in range(options['n_days']):
        dist_epoch = avg_epochs(overlaps_dist[i][size:] + overlaps_dist[i][:size], **options) / 2.0
        dist_avg.append(dist_epoch.mean(0))
        dist_ci.append(get_bootstrap_ci(dist_epoch))

    dist_avg = np.array(dist_avg)
    dist_ci = np.array(dist_ci).T

    plt.plot(np.arange(1, options['n_days']+1), dist_avg, '-o', label='%s Distractor' % options['epochs'][0], color='b')
    plt.fill_between(np.arange(1, options['n_days']+1), dist_ci[0], dist_ci[1], color='b', alpha=0.1)

    size = overlaps_choice.shape[1] // 2
    options['epochs'] = ['ED']
    choice_avg = []
    choice_ci = []
    for i in range(options['n_days']):
        choice_epoch = avg_epochs(overlaps_choice[i][size:] + overlaps_choice[i][:size], **options) / 2.0
        choice_avg.append(choice_epoch.mean(0))
        choice_ci.append(get_bootstrap_ci(choice_epoch))

    choice_avg = np.array(choice_avg)
    choice_ci = np.array(choice_ci).T

    plt.plot(np.arange(1, options['n_days']+1), choice_avg, '-o', label='%s Choice' % options['epochs'][0], color='g')
    plt.fill_between(np.arange(1, options['n_days']+1), choice_ci[0], choice_ci[1], color='g', alpha=0.1)

    plt.axhline(y=0.0, color='k', linestyle='--')

    plt.legend(fontsize=10)
    plt.xticks(np.arange(1, options['n_days']+1))
    plt.xlabel('Day')
    plt.ylabel('Overlap')
    plt.savefig('%s_overlaps_avg.svg' % options['mouse'], dpi=300)
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/ddc1c93f120e25f8841b831d97dd88806b68bc89.png]]

#+begin_src ipython

#+end_src

#+RESULTS:

* COMMENT Overlaps mice

#+begin_src ipython
  mice = ['ChRM04','JawsM15', 'JawsM18', 'ACCM03', 'ACCM04']
  # mice = ['JawsM15']
  l1_ratio = 0.95

  overlaps_mice = []

  for mouse in mice:
      print('mouse', mouse)
      try:
          # overlaps = pkl_load('%s_overlaps_tuned_l1_ratio.pkl' % mouse, path="../data/%s/" % mouse)
          overlaps = pkl_load('%s_overlaps_%.2f_l1_ratio.pkl' % (mouse, l1_ratio), path="../data/%s/" % mouse)
          print(overlaps.shape)
          overlaps_mice.append(overlaps)
      except:
          print('file not found')
          overlaps_mice.append(np.ones((6, 2, 84)))
#+end_src

#+RESULTS:
: mouse ChRM04
: (3, 6, 64, 84)
: mouse JawsM15
: (3, 6, 64, 84)
: mouse JawsM18
: (3, 6, 64, 84)
: mouse ACCM03
: (3, 5, 128, 84)
: mouse ACCM04
: (3, 5, 128, 84)

#+begin_src ipython
  options = set_options(**kwargs)
  options['epochs'] = ['LD']

  sample_mice = []
  for i in range(len(mice)):
      overlaps_sample = overlaps_mice[i][0]
      size = overlaps_sample.shape[1] // 2
      sample_avg = []
      sample_ci = []
      for j in range(overlaps_sample.shape[0]):
          sample_epoch = avg_epochs(-overlaps_sample[j][size:] + overlaps_sample[j][:size], **options) / 2.0
          sample_avg.append(sample_epoch.mean(0))
          sample_ci.append(get_bootstrap_ci(sample_epoch))

      sample_avg = np.array(sample_avg)
      if sample_avg.shape[0] !=6:
          sample_avg = np.append(sample_avg, np.nan)

      sample_mice.append(sample_avg)

      sample_ci = np.array(sample_ci).T

      plt.plot(np.arange(1, 6+1), sample_avg, '-', label='%s Sample' % options['epochs'][0], color='b', alpha=0.05)
      # plt.fill_between(np.arange(1, overlaps_sample.shape[0]+1), sample_ci[0], sample_ci[1], color='b', alpha=0.1)

  sample_mice = np.array(sample_mice)
  sample_ci = get_bootstrap_ci(sample_mice)
  sample_ci_last = get_bootstrap_ci(sample_mice[:3][-1])
  sample_ci[0][-1] = sample_ci_last[0]
  sample_ci[1][-1] = sample_ci_last[1]

  plt.plot(np.arange(1, 7), np.nanmean(sample_mice, 0), '-or')
  plt.fill_between(np.arange(1, 7), sample_ci[0], sample_ci[1], color='r', alpha=0.1)
  plt.axhline(y=0.0, color='k', linestyle='--')
  plt.xlabel('Day')
  plt.ylabel('Sample Overlap')
  plt.savefig('overlaps_mice.svg', dpi=300)

  plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: /home/leon/mambaforge/envs/dual_data/lib/python3.11/site-packages/scipy/stats/_resampling.py:100: DegenerateDataWarning: The BCa confidence interval cannot be calculated. This problem is known to occur when the distribution is degenerate or the statistic is np.min.
:   warnings.warn(DegenerateDataWarning(msg))
[[file:./.ob-jupyter/06829d8e10157db63a94585a96ea05565cb934ab.png]]
:END:

#+begin_src ipython
  options['T_WINDOW'] = 0
  options = set_options(**kwargs)
  options['epochs'] = ['STIM_ED']

  dist_mice = []
  for i in range(len(mice)):
      overlaps_dist = overlaps_mice[i][1]

      if i==2:
          overlaps_dist = - overlaps_dist

      size = overlaps_dist.shape[1] // 2

      dist_avg = []
      dist_ci = []
      for j in range(overlaps_dist.shape[0]):
          dist_epoch = avg_epochs(overlaps_dist[j][size:] + overlaps_dist[j][:size], **options) / 2.0
          dist_avg.append(dist_epoch.mean(0))
          dist_ci.append(get_bootstrap_ci(dist_epoch))

      dist_avg = np.array(dist_avg)
      if dist_avg.shape[0] !=6:
          dist_avg = np.append(dist_avg, np.nan)

      dist_mice.append(dist_avg)

      dist_ci = np.array(dist_ci).T

      plt.plot(np.arange(1, 6+1), dist_avg, '-', label='%s Distractor' % options['epochs'][0], color='b', alpha=0.1)
      # plt.fill_between(np.arange(1, overlaps_dist.shape[0]+1), dist_ci[0], dist_ci[1], color='b', alpha=0.1)

  dist_mice = np.array(dist_mice)
  dist_ci = get_bootstrap_ci(dist_mice, statistic=np.nanmean)
  dist_ci_last = get_bootstrap_ci(dist_mice[:3][-1], statistic=np.nanmean)
  dist_ci[0][-1] = dist_ci_last[0]
  dist_ci[1][-1] = dist_ci_last[1]

  plt.plot(np.arange(1, 7), np.nanmean(dist_mice, 0), '-ob')
  plt.fill_between(np.arange(1, 7), dist_ci[0], dist_ci[1], color='b', alpha=0.1)
  plt.axhline(y=0.0, color='k', linestyle='--')
  plt.xlabel('Day')
  plt.ylabel('Dist. Overlap')
  plt.xticks(np.arange(1,7))
  plt.savefig('overlaps_mice.svg', dpi=300)

  plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: /home/leon/mambaforge/envs/dual_data/lib/python3.11/site-packages/scipy/stats/_resampling.py:630: RuntimeWarning: Mean of empty slice
:   theta_hat_b.append(statistic(*resampled_data, axis=-1))
[[file:./.ob-jupyter/39fb9a6257f2cc3093c2b86b7c52221113918029.png]]
:END:

#+begin_src ipython
  options = set_options(**kwargs)
  options['epochs'] = ['LD']

  choice_mice = []
  for i in range(len(mice)):
      overlaps_choice = overlaps_mice[i][2]
      size = overlaps_choice.shape[1] // 2
      choice_avg = []
      choice_ci = []
      for j in range(overlaps_choice.shape[0]):
          choice_epoch = avg_epochs(-overlaps_choice[j][size:] + overlaps_choice[j][:size], **options) / 2.0
          choice_avg.append(choice_epoch.mean(0))
          choice_ci.append(get_bootstrap_ci(choice_epoch))

      choice_avg = np.array(choice_avg)
      if choice_avg.shape[0] !=6:
          choice_avg = np.append(choice_avg, np.nan)

      choice_mice.append(choice_avg)

      choice_ci = np.array(choice_ci).T

      plt.plot(np.arange(1, 6+1), choice_avg, 'x-', label='%s Choice' % options['epochs'][0], color='g', alpha=0.05)
      # plt.fill_between(np.arange(1, overlaps_choice.shape[0]+1), choice_ci[0], choice_ci[1], color='g', alpha=0.05)

  choice_mice = np.array(choice_mice)

  choice_ci = get_bootstrap_ci(choice_mice, statistic=np.nanmean, n_resamples=10000)
  choice_ci_last = get_bootstrap_ci(choice_mice[:3][-1], statistic=np.nanmean, n_resamples=10000)
  choice_ci[0][-1] = choice_ci_last[0]
  choice_ci[1][-1] = choice_ci_last[1]

  plt.plot(np.arange(1, 7), np.nanmean(choice_mice, 0), '-og')
  plt.fill_between(np.arange(1, 7), choice_ci[0], choice_ci[1], color='g', alpha=0.1)
  plt.axhline(y=0.0, color='k', linestyle='--')
  # plt.yticks([0.4, 0.6, 0.8, 1.0])
  plt.xticks(np.arange(1,7))
  plt.xlabel('Day')
  plt.ylabel('Choice Overlap')
  plt.savefig('mice_overlaps_choice.svg', dpi=300)

  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/988086f33ad87e960ae8f6207ba47afff9732578.png]]

#+begin_src ipython

#+end_src

#+RESULTS:

* Cosine Mice

#+begin_src ipython
  def angle_AB(A, B):
      A_norm = A / (np.linalg.norm(A) + 1e-5)
      B_norm = B / (np.linalg.norm(B) + 1e-5)

      return np.arccos(A_norm @ B_norm) * 180 / np.pi
#+end_src

#+RESULTS:

#+begin_src ipython
  mice = ['ChRM04','JawsM15', 'JawsM18', 'ACCM03', 'ACCM04']
  # mice = ['ChRM04', 'JawsM15', 'JawsM18']
  l1_ratio = 0.95

  coefs_mice = []
  angles_mice = []

  for mouse in mice:
      print(mouse)
      try:
          coefs = pkl_load('%s_coefs_%.2f_l1_ratio_loocv.pkl' % (mouse, l1_ratio), path="../data/%s/" % mouse)
          angles = np.diag(angle_AB(coefs[0], coefs[1].T))
          if angles.shape[0]!=6:
              angles = np.append(angles, np.nan)

          print(coefs.shape, angles.shape)

          coefs_mice.append(coefs)
          angles_mice.append(angles)
      except:
           print('file not found')

  angles_mice = np.array(angles_mice)
#+end_src

#+RESULTS:
: ChRM04
: (3, 6, 668) (6,)
: JawsM15
: (3, 6, 693) (6,)
: JawsM18
: (3, 6, 444) (6,)
: ACCM03
: (3, 5, 361) (6,)
: ACCM04
: (3, 5, 113) (6,)

#+RESULTS:

#+begin_src ipython
  # print(angles_mice.shape)
  for i in range(len(mice)):
      plt.plot(np.arange(1, 7), angles_mice[i], alpha=0.1)

  angles_ci = get_bootstrap_ci(angles_mice, statistic=np.nanmean, n_resamples=10000)

  angles_ci_last = get_bootstrap_ci(angles_mice[:3][-1], statistic=np.nanmean, n_resamples=10000)
  angles_ci[0][-1] = angles_ci_last[0]
  angles_ci[1][-1] = angles_ci_last[1]

  plt.plot(np.arange(1, 7), np.nanmean(angles_mice, 0), 'k')
  plt.fill_between(np.arange(1, 7), angles_ci[0], angles_ci[1], color='g', alpha=0.25)
  plt.axhline(y=90.0, color='k', linestyle='--')

  plt.xticks(np.arange(1,7))
  plt.xlabel('Day')
  plt.ylabel('Angle Sample/Dist. (°)')
  plt.savefig('mice_angles.svg', dpi=300)

  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/bb41a280f15d2cb709acb36e754b4de06425925c.png]]

#+begin_src ipython

#+end_src

#+RESULTS:
