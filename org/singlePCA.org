#+STARTUP: fold
#+PROPERTY: header-args:jupyter-python :results both :exports both :async yes :session single :kernel dual :output-dir ./figures/pca :file (lc/org-babel-tangle-figure-filename)

* Notebook Settings

#+begin_src jupyter-python
%load_ext autoreload
%autoreload 2
%reload_ext autoreload

%run /home/leon/dual_task/dual_data/notebooks/setup.py
%config InlineBackend.figure_format = 'png'
#+end_src

#+RESULTS:
: The autoreload extension is already loaded. To reload it, use:
:   %reload_ext autoreload
: Python exe
: /home/leon/mambaforge/envs/dual/bin/python

* Imports

#+begin_src jupyter-python
  from sklearn.exceptions import ConvergenceWarning
  warnings.filterwarnings("ignore")
  import traceback

  import sys
  sys.path.insert(0, '/home/leon/dual_task/dual_data/')

  import os
  if not sys.warnoptions:
    warnings.simplefilter("ignore")
    os.environ["PYTHONWARNINGS"] = "ignore"

  import pickle as pkl
  import numpy as np
  import matplotlib.pyplot as plt
  import pandas as pd
  import seaborn as sns

  from time import perf_counter

  from sklearn.base import clone
  from sklearn.metrics import make_scorer, roc_auc_score
  from sklearn.preprocessing import StandardScaler, RobustScaler
  from sklearn.model_selection import RepeatedStratifiedKFold, LeaveOneOut, StratifiedKFold

  from src.common.plot_utils import add_vlines, add_vdashed
  from src.common.options import set_options
  from src.stats.bootstrap import my_boots_ci
  from src.common.get_data import get_X_y_days, get_X_y_S1_S2
  from src.preprocess.helpers import avg_epochs
  from src.decode.bump import circcvl
  from src.torch.classificationCV import ClassificationCV
  from src.torch.classify import get_classification
#+end_src

#+RESULTS:

* Helpers
** scalers
#+begin_src jupyter-python
import numpy as np

class StandardScaler:
    def __init__(self, axis=0, if_scale=1):
        self.axis = axis
        self.center_ = None
        self.scale_ = None
        self.if_scale_ = if_scale

    def fit(self, X):
        self.center_ = np.nanmean(X, axis=self.axis, keepdims=True)
        self.scale_ = np.nanstd(X, axis=(0, -1), keepdims=True)
        # Prevent division by zero
        self.scale_ = np.where(self.scale_==0, 1, self.scale_)
        # self.scale_ = np.where(np.abs(self.scale_)<1e-3, 1, self.scale_)
        return self

    def transform(self, X):
        if self.if_scale_:
            return (X - self.center_) / self.scale_
        return (X - self.center_)

    def fit_transform(self, X):
        self.fit(X)
        return self.transform(X)
#+end_src

#+RESULTS:

#+begin_src jupyter-python
import numpy as np

class RobustScaler:
    def __init__(self, axis=0):
        self.axis = axis
        self.center_ = None
        self.scale_ = None

    def fit(self, X):
        self.center_ = np.nanmedian(X, axis=self.axis, keepdims=True)
        q75 = np.nanpercentile(X, 75, axis=self.axis, keepdims=True)
        q25 = np.nanpercentile(X, 25, axis=self.axis, keepdims=True)
        self.scale_ = q75 - q25
        # Prevent division by zero
        self.scale_ = np.where(self.scale_ == 0, 1, self.scale_)
        return self

    def transform(self, X):
        return (X - self.center_) / self.scale_

    def fit_transform(self, X):
        self.fit(X)
        return self.transform(X)
#+end_src

#+RESULTS:

** pad

#+begin_src jupyter-python
def pad_list(arrays, axis=0, max_len=None):
    """
    Pads a list of arrays along the specified axis with NaNs so all have the same size along that axis.
    Returns a list of padded arrays.
    """
    # Find maximum size along specified axis
    if max_len is None:
        max_len = max(arr.shape[axis] for arr in arrays)

    padded = []
    for arr in arrays:
        pad_width = [(0, 0)] * arr.ndim


        n_pad = max_len - arr.shape[axis]

        if n_pad > 0:
            pad_width[axis] = (0, n_pad)
            arr_padded = np.pad(arr, pad_width, mode='constant', constant_values=np.nan)
        else:
            arr_padded = arr
        padded.append(arr_padded)

    return padded
#+end_src

#+RESULTS:

#+begin_src jupyter-python
def pad_with_nans(array, target_shape):
    result = np.full(target_shape, np.nan)  # Create an array filled with NaNs
    print(result.shape)
    slices = tuple(slice(0, min(dim, target)) for dim, target in zip(array.shape, target_shape))
    result[slices] = array[slices]
    return result
#+end_src

#+RESULTS:

** save

#+begin_src jupyter-python
  def convert_seconds(seconds):
      h = seconds // 3600
      m = (seconds % 3600) // 60
      s = seconds % 60
      return h, m, s
#+end_src

#+RESULTS:

#+begin_src jupyter-python
  import pickle as pkl

  def pkl_save(obj, name, path="."):
      os.makedirs(path, exist_ok=True)
      destination = path + "/" + name + ".pkl"
      print("saving to", destination)
      pkl.dump(obj, open(destination, "wb"))


  def pkl_load(name, path="."):
      source = path + "/" + name + '.pkl'
      print('loading from', source)
      return pkl.load(open( source, "rb"))

#+end_src

#+RESULTS:

** cv pca

#+begin_src jupyter-python
def cv_avg_cond(X, y, condition='odor_pair', levels=None):
    if isinstance(condition, str):
        condition = [condition]

    # fixed levels (recommended): dict {col: [ordered levels]}
    if levels is None:
        levels = {c: np.sort(y[c].astype(str).unique()) for c in condition}
    else:
        levels = {c: np.array(levels[c]).astype(str) for c in condition}

    combos = list(itertools.product(*[levels[c] for c in condition]))

    X_avg = []
    for combo in combos:
        idx = np.ones(len(y), dtype=bool)
        for c, v in zip(condition, combo):
            idx &= (y[c].astype(str).values == str(v))

        if not idx.any():
            raise ValueError(f"Missing condition combo in this split: {dict(zip(condition, combo))}")

        X_avg.append(np.nanmean(X[idx], axis=0))

    return np.stack(X_avg, axis=0)
#+end_src

#+RESULTS:

#+begin_src jupyter-python
import numpy as np
import matplotlib.pyplot as plt
from scipy.interpolate import griddata
from scipy.ndimage import gaussian_filter

def polar_velocities(x, y, sigma_r=1, sigma_t=1, sigma_time=1):
    """
    x, y: (n_traj, n_points)
    sigma: gaussian smoothing in time (in points)
    Returns:
      dr_s, dtheta_s: (n_traj, n_points): smoothed radial and angular velocities
      r, theta: positions at each point, for later conversion
    """

    r = np.sqrt(x**2 + y**2)
    theta = np.arctan2(y, x)
    theta_u = np.unwrap(theta, axis=1)

    # r_s = gaussian_filter1d(r, sigma=sigma_r)
    # theta_s = gaussian_filter1d(theta_u, sigma=sigma_t)

    r_s = gaussian_filter(r, sigma=(sigma_r, sigma_time))
    theta_s = gaussian_filter(theta_u, sigma=(sigma_t, sigma_time))

    dr_s = np.gradient(r_s, axis=1)
    dtheta_s = np.gradient(theta_s, axis=1)

    return dr_s, dtheta_s, r_s, theta_s
#+end_src

#+RESULTS:

#+begin_src jupyter-python
def pol2cart(dr, dtheta, r, theta):
    """
    Converts smoothed polar velocities to Cartesian velocities.
    All arrays shape: (n_traj, n_points)
    """
    u = dr * np.cos(theta) - r * dtheta * np.sin(theta)
    v = dr * np.sin(theta) + r * dtheta * np.cos(theta)
    return u, v
#+end_src

#+RESULTS:

#+begin_src jupyter-python
def create_field(x, y, u, v, grid_size=100, method='linear', z_lim=5):
    """
    Interpolate velocity field to a regular grid.
    Inputs: x,y,u,v all (n_samples,) or (n_traj, n_points) -- flatten them first.
    Returns: xi, yi, ui, vi: all (grid_size, grid_size) arrays.
    """
    x_flat = x.flatten()
    y_flat = y.flatten()
    u_flat = u.flatten()
    v_flat = v.flatten()

    # xi, yi = np.meshgrid(
    #     np.linspace(np.min(x_flat), np.max(x_flat), grid_size),
    #     np.linspace(np.min(y_flat), np.max(y_flat), grid_size),
    # )

    x_min, x_max = np.min(x_flat)-1, np.max(x_flat)+1
    y_min, y_max = np.min(y_flat)-1, np.max(y_flat)+1

    if z_lim is 0:
        z_min = np.min((x_min, y_min))
        z_max = np.min((x_max, y_max))
    else:
        z_min = -z_lim
        z_max = z_lim

    xi, yi = np.meshgrid(np.linspace(z_min, z_max, grid_size),
                         np.linspace(z_min, z_max, grid_size))


    ui = griddata((x_flat, y_flat), u_flat, (xi, yi), method=method, fill_value=np.nan)
    vi = griddata((x_flat, y_flat), v_flat, (xi, yi), method=method, fill_value=np.nan)

    # Fill nans with nearest
    mask = np.isnan(ui)
    if np.any(mask):
        ui[mask] = griddata((x_flat, y_flat), u_flat, (xi, yi), method='nearest')[mask]
        vi[mask] = griddata((x_flat, y_flat), v_flat, (xi, yi), method='nearest')[mask]

    return xi, yi, ui, vi

#+end_src

#+RESULTS:

#+begin_src jupyter-python
def plot_field(xi, yi, ui, vi, ax=None, density=1.0, show_cbar=0):
    speed = np.sqrt(ui**2 + vi**2)
    if ax is None:
        fig, ax = plt.subplots(figsize=(5,5))
    # Normalize for coloring
    import matplotlib as mpl
    vmin, vmax = np.nanpercentile(speed, [5, 95])
    norm = mpl.colors.Normalize(vmin, vmax)
    # strm = ax.streamplot(xi, yi, ui, vi, density=density, color=speed, cmap='coolwarm', norm=norm)
    heatmap = ax.pcolormesh(xi, yi, speed, cmap='coolwarm', shading='gouraud', norm=norm)

    if show_cbar:
        plt.colorbar(strm.lines, ax=ax, label='Speed')

    ax.set_aspect('equal')
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    return ax
#+end_src

#+RESULTS:

#+begin_src jupyter-python
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl

from scipy.ndimage import gaussian_filter1d, gaussian_filter
from scipy.interpolate import Rbf

def create_field_rbf(
    x, y, u, v,
    grid_size=32,
    z_lim=5,
    method='multiquadric',
    epsilon=None,
    smooth=0.0
):
    """
    Interpolate velocity field to a regular grid using RBFs.

    Inputs:
      x, y, u, v: (n_traj, n_points) or (n_samples,) arrays
      grid_size: number of grid points per dimension
      z_lim: if 0, use bounding box of data; otherwise [-z_lim, z_lim]^2
      function: RBF type ('multiquadric', 'inverse', 'gaussian', 'linear', etc.)
      epsilon: shape parameter; if None, Rbf chooses something heuristic
      smooth: smoothing parameter passed to Rbf (regularization)

    Returns:
      xi, yi, ui, vi: all (grid_size, grid_size) arrays
    """
    x_flat = x.flatten()
    y_flat = y.flatten()
    u_flat = u.flatten()
    v_flat = v.flatten()

    # Domain limits
    if z_lim == 0:
        x_min, x_max = x_flat.min() - 1, x_flat.max() + 1
        y_min, y_max = y_flat.min() - 1, y_flat.max() + 1
        z_min = min(x_min, y_min)
        z_max = max(x_max, y_max)
    else:
        z_min, z_max = -z_lim, z_lim

    xi, yi = np.meshgrid(np.linspace(z_min, z_max, grid_size),
                         np.linspace(z_min, z_max, grid_size))

    # One RBF for u, one for v
    rbfu = Rbf(
        x_flat, y_flat, u_flat,
        function=method,
        epsilon=epsilon,
        smooth=smooth
    )
    rbfv = Rbf(
        x_flat, y_flat, v_flat,
        function=method,
        epsilon=epsilon,
        smooth=smooth
    )

    ui = rbfu(xi, yi)
    vi = rbfv(xi, yi)

    return xi, yi, ui, vi
#+end_src

#+RESULTS:

#+begin_src jupyter-python
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl

from scipy.ndimage import gaussian_filter1d, gaussian_filter
from scipy.interpolate import LinearNDInterpolator, NearestNDInterpolator

def create_field(x, y, u, v, grid_size=100, method='linear', z_lim=5):
    """
    Interpolate velocity field to a regular grid.

    Inputs:
      x, y, u, v: (n_traj, n_points) or (n_samples,) arrays
      grid_size: number of grid points per dimension
      method: 'linear' (via LinearNDInterpolator) or any method supported
              by griddata ('linear', 'nearest', 'cubic') if not 'linear' here.
      z_lim: if 0, use bounding box of data; otherwise domain = [-z_lim, z_lim]^2

    Returns:
      xi, yi, ui, vi: all (grid_size, grid_size) arrays
    """
    x_flat = x.flatten()
    y_flat = y.flatten()
    u_flat = u.flatten()
    v_flat = v.flatten()

    # Domain limits
    if z_lim == 0:
        x_min, x_max = x_flat.min() - 1, x_flat.max() + 1
        y_min, y_max = y_flat.min() - 1, y_flat.max() + 1
        z_min = min(x_min, y_min)
        z_max = max(x_max, y_max)
    else:
        z_min, z_max = -z_lim, z_lim

    xi, yi = np.meshgrid(np.linspace(z_min, z_max, grid_size),
                         np.linspace(z_min, z_max, grid_size))

    # Interpolation
    if method == 'linear':
        # Use LinearNDInterpolator + NearestNDInterpolator fallback
        pts = np.column_stack((x_flat, y_flat))
        lin_u = LinearNDInterpolator(pts, u_flat)
        lin_v = LinearNDInterpolator(pts, v_flat)

        ui = lin_u(xi, yi)
        vi = lin_v(xi, yi)

        mask = np.isnan(ui)
        if np.any(mask):
            near_u = NearestNDInterpolator(pts, u_flat)
            near_v = NearestNDInterpolator(pts, v_flat)
            ui[mask] = near_u(xi[mask], yi[mask])
            vi[mask] = near_v(xi[mask], yi[mask])
    else:
        from scipy.interpolate import griddata
        ui = griddata((x_flat, y_flat), u_flat, (xi, yi),
                      method=method, fill_value=np.nan)
        vi = griddata((x_flat, y_flat), v_flat, (xi, yi),
                      method=method, fill_value=np.nan)

        mask = np.isnan(ui)
        if np.any(mask):
            ui[mask] = griddata((x_flat, y_flat), u_flat, (xi, yi),
                                method='nearest')[mask]
            vi[mask] = griddata((x_flat, y_flat), v_flat, (xi, yi),
                                method='nearest')[mask]

    return xi, yi, ui, vi
#+end_src

#+RESULTS:

#+begin_src jupyter-python
def get_mean_velo(r_s, dr_s, n_bins=32, z_lim=10):

    r_all  = r_s.flatten()
    dr_all = dr_s.flatten()

    bins = np.linspace(np.nanmin(r_all), np.nanmax(r_all), n_bins)

    bin_centers = 0.5*(bins[:-1] + bins[1:])
    digitized = np.digitize(r_all, bins)

    dr_mean = np.array([np.nanmean(dr_all[digitized == i]) for i in range(1, len(bins))])

    return dr_mean, bins, bin_centers, digitized
#+end_src

#+RESULTS:

* Parameters

#+begin_src jupyter-python
old_mice = ['ChRM04','JawsM15', 'JawsM18', 'ACCM03', 'ACCM04']
Jaws_mice = ['JawsM01', 'JawsM06', 'JawsM12', 'JawsM15', 'JawsM18']

mice = ['JawsM01', 'JawsM06', 'JawsM12', 'JawsM15', 'JawsM18', 'ChRM04', 'ChRM23', 'ACCM03', 'ACCM04']
laser_mice = ['JawsM01', 'JawsM06', 'JawsM12', 'JawsM15', 'JawsM18', 'ChRM04', 'ChRM23']
# mice = ['JawsM01', 'JawsM06', 'JawsM12', 'JawsM15', 'JawsM18', 'ChRM04', 'ChRM23']
# mice = ['JawsM15', 'JawsM18', 'ChRM04']

tasks = ['Dual'] # all

kwargs = {
   'mice': mice,
   'tasks': tasks,
   'mouse': mice[0], 'laser': 0,
   'trials': '', 'reload': 0, 'data_type': 'dF',
   'prescreen': None, 'pval': 0.05,
   'preprocess': False, 'scaler_BL': 'standard',
   'avg_noise':False, 'unit_var_BL': False,
   'random_state': None, 'T_WINDOW': 0.0,
   'l1_ratio': 0.95,
   'n_comp': 3, 'pca': 'pca',
   'scaler': None,
   'bootstrap': 1, 'n_boots': 128,
   'n_splits': 5, 'n_repeats': 10,
   'class_weight': 0,
   'multilabel': 0,
   'mne_estimator':'generalizing', # sliding or generalizing
   'n_jobs': 64,
}

# kwargs['days'] = ['first', 'middle', 'last']
kwargs['days'] = ['first', 'last']
# kwargs['days'] = 'all'
options = set_options(**kwargs)
options['cv_B'] = False
#+end_src

#+RESULTS:

* Load Data

#+begin_src jupyter-python
X_trials, y_trials = [], []

n_neurons = 3319
counter = 0
for mouse in options['mice']:

    options['mouse'] = mouse
    options = set_options(**options)
    X, y = get_X_y_days(**options)

    print(mouse, X.shape, y.shape, options['n_days'])

    X_scale = X.copy()
    for day in range(1, options['n_days']+1):
        idx = (y.day==day) & (y.laser==0)

        mean_ = np.nanmean(X[idx], 0, keepdims=1)
        std_ = np.nanstd(X[idx], 0, ddof=1, keepdims=1)

        X_scale[idx] = (X[idx] - mean_) / std_
        std_floor = np.percentile(std_, 5)
        std_safe = np.maximum(std_, std_floor)


        idx = (y.day==day) & (y.laser==1)
        X_scale[idx] = (X[idx] - mean_) / std_safe

    X_trial = np.zeros((X.shape[0], n_neurons, X.shape[-1])) * np.nan
    X_trial[:, counter:X.shape[1]+counter, :] = X_scale

    counter += X.shape[1]

    X_trials.append(X_trial)

    y['mouse'] = mouse
    y_trials.append(y)

X_trials = np.array(X_trials, dtype=object)
y_trials = np.array(y_trials, dtype=object)
#+end_src

#+RESULTS:
: JawsM01 (768, 184, 84) (768, 15) 4
: JawsM06 (1152, 201, 84) (1152, 15) 6
: JawsM12 (960, 423, 84) (960, 15) 5
: JawsM15 (1152, 693, 84) (1152, 15) 6
: JawsM18 (1152, 444, 84) (1152, 15) 6
: ChRM04 (1152, 668, 84) (1152, 15) 6
: ChRM23 (960, 232, 84) (960, 15) 5
: ACCM03 (960, 361, 84) (960, 15) 5
: ACCM04 (960, 113, 84) (960, 15) 5

#+begin_src jupyter-python
X_all = np.concatenate(X_trials, 0)
y_all = pd.concat(y_trials)
print(X_all.shape, y_all.shape)
#+end_src

#+RESULTS:
: (9216, 3319, 84) (9216, 16)

#+begin_src jupyter-python
pkl_save(X_all, 'X_all_nan', path="../data/pca")
pkl_save(y_all, 'y_all_nan', path="../data/pca")
#+end_src

#+RESULTS:
: saving to ../data/pca/X_all_nan.pkl
: saving to ../data/pca/y_all_nan.pkl

* Single mouse
** Load

#+begin_src jupyter-python
X_all = pkl_load('X_all_nan', path="../data/pca")
y_all = pkl_load('y_all_nan', path="../data/pca")
#+end_src

#+RESULTS:
: loading from ../data/pca/X_all_nan.pkl
: loading from ../data/pca/y_all_nan.pkl

#+begin_src jupyter-python
y_all['sample'] = y_all.sample_odor
y_all['test'] = y_all.test_odor
#+end_src

#+RESULTS:

#+begin_src jupyter-python
print(y_all.keys())
#+end_src

#+RESULTS:
: Index(['sample_odor', 'dist_odor', 'test_odor', 'tasks', 'response', 'laser',
:        'day', 'choice', 'pair', 'odr_perf', 'odr_choice', 'odr_response',
:        'odor_pair', 'learning', 'performance', 'mouse', 'sample', 'test'],
:       dtype='object')

** PCA
*** utils

#+begin_src jupyter-python
from sklearn.model_selection import KFold, StratifiedKFold, RepeatedStratifiedKFold
from sklearn.decomposition import PCA

import numpy as np
import pandas as pd
import itertools
from tqdm import tqdm
from scipy.linalg import orthogonal_procrustes

# -----------------------------
# Condition levels / ordering
# -----------------------------
def get_levels(y: pd.DataFrame, factors):
    """
    Return a dict mapping each factor -> ordered levels, in a deterministic way.
    Numeric factors sorted numerically; non-numeric sorted lexicographically as strings.
    """
    if isinstance(factors, str):
        factors = [factors]

    levels = {}
    for f in factors:
        s = y[f]
        if np.issubdtype(s.dtype, np.number):
            levels[f] = np.sort(s.dropna().unique())
        else:
            levels[f] = np.sort(s.astype(str).dropna().unique())
    return levels


def _combo_mask(y: pd.DataFrame, factors, combo):
    """Boolean mask selecting rows matching a given factor-value combo."""
    m = np.ones(len(y), dtype=bool)
    for f, v in zip(factors, combo):
        s = y[f]
        if np.issubdtype(s.dtype, np.number):
            m &= (s.values == v)
        else:
            m &= (s.astype(str).values == str(v))
    return m


# -----------------------------
# Condition averaging / anchors
# -----------------------------
def cv_avg_cond(X, y: pd.DataFrame, factors, levels, drop_missing=False):
    """
    Condition-average X over all combinations of `factors` in the fixed order given by `levels`.

    X: (n_trials, n_neurons, n_time)
    returns:
      X_avg: (n_combo, n_neurons, n_time)
      combos: list of tuples of factor levels (in fixed order)
    """
    if isinstance(factors, str):
        factors = [factors]

    combos = list(itertools.product(*[levels[f] for f in factors]))
    X_avg = []

    for combo in combos:
        idx = _combo_mask(y, factors, combo)
        if not idx.any():
            if drop_missing:
                continue
            raise ValueError(f"Missing condition combo in this split: {dict(zip(factors, combo))}")
        X_avg.append(np.nanmean(X[idx], axis=0))

    return np.stack(X_avg, axis=0), combos


def anchors_from_Z(Z, y: pd.DataFrame, factors, levels, drop_missing=False):
    """
    Anchors = condition-averaged latent trajectories.

    Z: (n_trials, n_time, n_comp)
    returns:
      A: (n_combo*n_time, n_comp) in the fixed combo order defined by `levels`
    """
    if isinstance(factors, str):
        factors = [factors]

    combos = list(itertools.product(*[levels[f] for f in factors]))
    A = []

    for combo in combos:
        idx = _combo_mask(y, factors, combo)
        if not idx.any():
            if drop_missing:
                continue
            raise ValueError(f"Missing condition combo in anchors: {dict(zip(factors, combo))}")
        A.append(np.nanmean(Z[idx], axis=0))  # (n_time, n_comp)

    A = np.stack(A, axis=0)                  # (n_combo, n_time, n_comp)
    return A.reshape(-1, Z.shape[-1])


def _enforce_proper_rotation(R):
    """Optionally enforce det(R)=+1 to avoid reflections."""
    if np.linalg.det(R) < 0:
        R = R.copy()
        R[:, -1] *= -1
    return R


# -----------------------------
# CV PCA with anchor alignment
# -----------------------------
def cv_pca(
    X, y: pd.DataFrame, pca, folds, factors, epoch,
    group_col=None,
    show_pbar=True,
    clean_mask_fn=None,          # optional: function(y)->bool mask
    enforce_det_pos=False,       # set True to prevent reflections
    center_on="trialtime",       # "trialtime" or "condavg"
):
    """
    Per-mouse (or per-X) CV condition-average PCA + fold alignment using Procrustes on anchors.

    X: (n_trials, n_neurons, n_time)  [NO NaNs in neuron dims; OK if NaNs in time samples if you handle them]
    y: DataFrame aligned with trials
    factors: str or list of columns defining "condition" combinations used for averaging/anchors
    epoch: None or slice/indices used for fitting/alignment only

    Returns:
      Z_all: (n_clean_test_trials + n_pert_trials, n_time, n_comp) aligned latent trajectories
      y_all: DataFrame aligned with returned trials
      W_mean: (n_comp, n_neurons) mean of aligned fold loadings (interpret cautiously)
      evr_folds: array of explained variance ratio per fold
    """

    if isinstance(factors, str):
        factors = [factors]

    # ---- define "clean" subset used for fitting/CV
    if clean_mask_fn is None:
        m_clean = (y.laser == 0) & (y.performance == 1)
    else:
        m_clean = clean_mask_fn(y)

    Xc = X[m_clean]
    yc = y.loc[m_clean].reset_index(drop=True)

    # Fixed, deterministic level ordering computed ONCE from full clean data
    levels = get_levels(yc, factors)

    # -----------------------------
    # Fit PCA on ALL clean data (epoch) to project perturbed data
    # -----------------------------
    Xc_epoch = Xc if epoch is None else Xc[..., epoch]

    def flat_trial_time(A):
        return A.transpose(0, 2, 1).reshape(-1, A.shape[1])

    X_avg_epoch_all, _ = cv_avg_cond(Xc_epoch, yc, factors, levels, drop_missing=False)
    X_avg_flat_all = flat_trial_time(X_avg_epoch_all)

    if center_on == "trialtime":
        Xc_epoch_flat = flat_trial_time(Xc_epoch)
        X_mean_all = np.nanmean(Xc_epoch_flat, axis=0, keepdims=True)
    else:  # "condavg"
        X_mean_all = np.nanmean(X_avg_flat_all, axis=0, keepdims=True)

    X_cent_all = X_avg_flat_all - X_mean_all
    if np.isnan(X_cent_all).any():
        raise ValueError("NaNs detected in ALL-fit PCA matrix. Ensure you are not using NaN-padded neurons.")
    pca.fit(X_cent_all)

    W_all = pca.components_
    evr_all = pca.explained_variance_ratio_

    # anchors reference for all-fit PCA
    Xc_epoch_flat = flat_trial_time(Xc_epoch)
    Zc_epoch = pca.transform(Xc_epoch_flat - X_mean_all).reshape(Xc_epoch.shape[0], Xc_epoch.shape[-1], -1)

    A_ref = anchors_from_Z(Zc_epoch, yc, factors, levels, drop_missing=False)

    # ---- project perturbed
    m_pert = ~m_clean
    if m_pert.mean() != 0:
        Xp = X[m_pert]
        yp = y.loc[m_pert].reset_index(drop=True)

        # project perturbed FULL time
        Xp_full_flat = flat_trial_time(Xp)
        Zp_full = pca.transform(Xp_full_flat - X_mean_all).reshape(
            Xp.shape[0], Xp.shape[-1], -1
        )

        # estimate rotation using perturbed epoch anchors
        Xp_epoch = Xp if epoch is None else Xp[..., epoch]
        Xp_epoch_flat = flat_trial_time(Xp_epoch)
        Zp_epoch = pca.transform(Xp_epoch_flat - X_mean_all).reshape(
            Xp_epoch.shape[0], Xp_epoch.shape[-1], -1
        )
        A_p = anchors_from_Z(Zp_epoch, yp, factors, levels, drop_missing=True)

        # If you drop missing combos, also drop corresponding rows from A_ref
        # For simplicity, we recommend not dropping (i.e. ensure combos exist).
        if A_p.shape != A_ref.shape:
            raise ValueError(
                "Perturbed anchors shape != reference anchors shape. "
                "Ensure perturbed data contains all factor combinations or implement matched-row dropping."
            )

        R_p, _ = orthogonal_procrustes(A_p, A_ref)
        if enforce_det_pos:
            R_p = _enforce_proper_rotation(R_p)

        Zp_aligned = Zp_full @ R_p

    groups = None
    if group_col is not None:
        groups = yc[group_col].astype(str).values

    # your stratification (keep as-is, but ensure columns exist)
    strata = (
        yc["odor_pair"].astype(str)
        + "_" + yc["day"].astype(str)
        + "_" + yc["tasks"].astype(str)
    ).values

    Z_folds, y_folds = [], []
    w_folds, evr_folds = [], []

    it = folds.split(Xc, strata, groups=groups)
    if show_pbar:
        mouse_name = y.mouse.unique()[0] if "mouse" in y.columns else "X"
        it = tqdm(it, total=folds.get_n_splits(Xc, strata), desc=str(mouse_name))

    for fold, (train, test) in enumerate(it):
        X_train, y_train = Xc[train], yc.iloc[train].reset_index(drop=True)
        X_test,  y_test  = Xc[test],  yc.iloc[test].reset_index(drop=True)

        # epoch views
        X_train_epoch = X_train if epoch is None else X_train[..., epoch]
        X_test_full   = X_test

        # flatten helper
        def flat_trial_time(A):
            # (n_trials, n_neurons, n_time) -> (n_trials*n_time, n_neurons)
            return A.transpose(0, 2, 1).reshape(-1, A.shape[1])

        # ---- compute condition averages (TRAIN only) for fitting
        X_avg_epoch, _ = cv_avg_cond(X_train_epoch, y_train, factors, levels, drop_missing=False)
        X_avg_flat = flat_trial_time(X_avg_epoch)

        # ---- centering
        if center_on == "trialtime":
            X_train_epoch_flat = flat_trial_time(X_train_epoch)
            X_mean = np.nanmean(X_train_epoch_flat, axis=0, keepdims=True)
        elif center_on == "condavg":
            X_mean = np.nanmean(X_avg_flat, axis=0, keepdims=True)
        else:
            raise ValueError("center_on must be 'trialtime' or 'condavg'")

        # ---- fit PCA on centered TRAIN condition-average epoch
        X_cent_fit = X_avg_flat - X_mean
        if np.isnan(X_cent_fit).any():
            raise ValueError("NaNs detected in PCA fit matrix. Ensure you are not using NaN-padded neurons.")
        pca.fit(X_cent_fit)

        W_fold = pca.components_  # (n_comp, n_neurons)
        evr_folds.append(pca.explained_variance_ratio_)

        # ---- anchors from TRAIN single-trial epoch projections (for alignment)
        X_train_epoch_flat = flat_trial_time(X_train_epoch)
        Z_train_epoch = pca.transform(X_train_epoch_flat - X_mean).reshape(X_train_epoch.shape[0], X_train_epoch.shape[-1], -1)  # (n_train_trials, n_time_epoch, n_comp)

        A_train = anchors_from_Z(Z_train_epoch, y_train, factors, levels, drop_missing=False)

        # ---- reference + Procrustes
        if A_ref is None:
            A_ref = A_train
            R = np.eye(Z_train_epoch.shape[-1])
        else:
            # Find R such that A_train @ R ~= A_ref
            R, _ = orthogonal_procrustes(A_train, A_ref)
            if enforce_det_pos:
                R = _enforce_proper_rotation(R)

        # ---- project TEST full time then align
        X_test_full_flat = flat_trial_time(X_test_full)
        Z_test_full = pca.transform(X_test_full_flat - X_mean).reshape(X_test_full.shape[0], X_test_full.shape[-1], -1)  # (n_test_trials, n_time_full, n_comp)

        Z_test_aligned = Z_test_full @ R
        W_fold_aligned = R.T @ W_fold

        Z_folds.append(Z_test_aligned)
        y_folds.append(y_test)
        w_folds.append(W_fold_aligned)

    # ---- concat CV outputs (clean test trials across folds)
    Z_clean = np.concatenate(Z_folds, axis=0)
    y_clean = pd.concat(y_folds, axis=0, ignore_index=True)

    W_mean = np.mean(np.stack(w_folds, axis=0), axis=0)
    evr_folds = np.array(evr_folds, dtype=object)

    if m_pert.mean() != 0:
        Z_all = np.concatenate([Z_clean, Zp_aligned], axis=0)
        y_all = pd.concat([y_clean, yp], axis=0, ignore_index=True)

        return Z_all, y_all, W_mean, evr_folds

    return Z_clean, y_clean, W_all, evr_folds
#+end_src

#+RESULTS:

*** model

#+begin_src jupyter-python
options['learning'] = 'Naive'
options['epochs'] = ['POST_GNG']
epoch = options['bins_' + options['epochs'][0]]
#+end_src

#+RESULTS:

#+begin_src jupyter-python
# factors = ['sample', 'tasks', 'test']
factors = 'odor_pair'
n_splits = 5
n_repeats = 1
n_comp = 3

pca = PCA(n_components=n_comp, svd_solver='randomized')
# folds = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats)
folds = LeaveOneOut()
#+end_src

#+RESULTS:

#+begin_src jupyter-python
from scipy.linalg import orthogonal_procrustes
import numpy as np
import pandas as pd

X_mice, y_mice, w_mice, evr_mice = [], [], [], []

# --- run per-mouse CV PCA (no cross-mouse alignment yet)
for mouse in options['mice']:
    idx = (y_all.learning == options['learning']) & (y_all['mouse'] == mouse)

    X_pca = X_all[idx]
    y_pca = y_all.loc[idx].reset_index(drop=True)

    # drop padded neurons for this mouse
    valid_neurons = ~np.all(np.isnan(X_pca), axis=(0, 2))
    X_pca = X_pca[:, valid_neurons, :]

    Z_mouse, y_mouse, W_mouse, evr_mouse = cv_pca(X_pca, y_pca, pca, folds, factors, epoch, center_on="trialtime")

    X_mice.append(Z_mouse)     # (n_trials_mouse, T, n_comp)
    y_mice.append(y_mouse)     # DataFrame aligned with trials
    w_mice.append(W_mouse)     # (n_comp, n_neurons_mouse) averaged across folds
    evr_mice.append(evr_mouse)
#+end_src

#+RESULTS:
#+begin_example
JawsM01: 100% 216/216 [00:03<00:00, 54.29it/s]
JawsM06: 100% 183/183 [00:03<00:00, 57.54it/s]
JawsM12: 100% 182/182 [00:07<00:00, 24.87it/s]
JawsM15: 100% 195/195 [00:13<00:00, 14.39it/s]
JawsM18: 100% 235/235 [00:12<00:00, 18.44it/s]
ChRM04: 100% 234/234 [00:18<00:00, 12.67it/s]
ChRM23: 100% 198/198 [00:04<00:00, 47.33it/s]
ACCM03: 100% 372/372 [00:25<00:00, 14.51it/s]
ACCM04: 100% 332/332 [00:05<00:00, 57.83it/s]
#+end_example

#+begin_src jupyter-python
# --- define global levels from pooled data to fix combo ordering across mice
y_pool = pd.concat(y_mice, ignore_index=True)
levels = get_levels(y_pool, factors)

ref_idx = 3
A_ref = anchors_from_Z(X_mice[ref_idx], y_mice[ref_idx], factors, levels, drop_missing=False)

X_mice_aligned = []
R_mice = []
for Z_mouse, y_mouse in zip(X_mice, y_mice):
    A = anchors_from_Z(Z_mouse, y_mouse, factors, levels, drop_missing=False)

    # R s.t. A @ R ~= A_ref
    R, _ = orthogonal_procrustes(A, A_ref)

    # optional: prevent reflections
    # R = _enforce_proper_rotation(R)

    X_mice_aligned.append(Z_mouse @ R)
    R_mice.append(R)

X_aligned_all = np.concatenate(X_mice_aligned, axis=0)
y_aligned_all = pd.concat(y_mice, ignore_index=True)
#+end_src

#+RESULTS:

#+begin_src jupyter-python
print(X_aligned_all.shape, y_aligned_all.shape, w_mice[0].shape)
#+end_src

#+RESULTS:
: (5184, 84, 3) (5184, 18) (3, 184)

#+begin_src jupyter-python
w_stack = np.concatenate(w_mice, -1)
print(w_stack.shape)
#+end_src

#+RESULTS:
: (3, 3319)

#+begin_src jupyter-python
X_single = np.swapaxes(X_aligned_all, 1, 2)
y_single = y_aligned_all
w_single = w_stack * 100
print(X_single.shape, y_single.shape)
#+end_src

#+RESULTS:
: (5184, 3, 84) (5184, 18)

** Save/Load

#+begin_src jupyter-python
dum = options['epochs'][0] + '_' + options['learning'] + '_laser_%d' % options['laser']
print(dum)
#+end_src

#+RESULTS:
: POST_GNG_Naive_laser_0

#+begin_src jupyter-python
pkl_save(X_single, 'single_traj_' + dum, path="../data/pca/")
pkl_save(y_single, 'single_labels_' + dum, path="../data/pca/")
pkl_save(w_single, 'single_weights_' + dum, path="../data/pca/")
# pkl_save(evr_single, 'single_evr_' + dum, path="../data/pca/")
#+end_src

#+RESULTS:
: saving to ../data/pca//single_traj_POST_GNG_Naive_laser_0.pkl
: saving to ../data/pca//single_labels_POST_GNG_Naive_laser_0.pkl
: saving to ../data/pca//single_weights_POST_GNG_Naive_laser_0.pkl

 #+begin_src jupyter-python
X_single = pkl_load('single_traj_' + dum, path="../data/pca/")
y_single = pkl_load( 'single_labels_' + dum, path="../data/pca/")
w_single = pkl_load( 'single_weights_' + dum, path="../data/pca/")
evr_single = pkl_load( 'single_evr_' + dum, path="../data/pca/")
#+end_src

#+RESULTS:
: loading from ../data/pca//single_traj_POST_GNG_Expert_laser_0.pkl
: loading from ../data/pca//single_labels_POST_GNG_Expert_laser_0.pkl
: loading from ../data/pca//single_weights_POST_GNG_Expert_laser_0.pkl
: loading from ../data/pca//single_evr_POST_GNG_Expert_laser_0.pkl

** Trajectories

#+begin_src jupyter-python
n_comp = 3
laser = 0
i_mouse= -1

idx_correct =  (y_single.performance==1)
idx_mouse = True
if i_mouse !=-1:
    idx_mouse = (y_single.mouse==options['mice'][i_mouse])
#+end_src

#+RESULTS:

#+begin_src jupyter-python
fig, ax = plt.subplots(nrows=1, ncols=n_comp, figsize=(n_comp*width, height),)

color = ['#332288', '#88CCEE', '#117733', '#44AA99']

pair = ['AC', 'AD', 'BD', 'BC']
xtime = np.linspace(0, 14, 84)

for i in range(4):
    mask = (y_single.odor_pair==i) & (y_single.laser==laser) & idx_mouse & idx_correct
    X_sel = X_single[mask]

    X_avg = np.mean(X_sel, 0)    # Mean over trials/samples, shape: (n_comp, n_time)
    X_sem = np.std(X_sel, 0) / np.sqrt(X_sel.shape[0])  # SEM

    for k in range(n_comp):
        y_avg = X_avg[k]
        y_sem = X_sem[k]
        ax[k].plot(xtime, y_avg, color=color[i], label=pair[i])
        ax[k].fill_between(xtime, y_avg-y_sem, y_avg+y_sem, color=color[i], alpha=0.2)
        ax[k].axhline(0, ls='--', color='k')
        ax[k].set_xlabel('Time')
        ax[k].set_ylabel('PC %d' % (k+1))
        add_vlines(ax[k], if_dpa=1)

        ax[k].legend(fontsize=12, frameon=0, loc='best')
plt.savefig('./figures/single/pca_traj_%s.svg' % dum)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/pca/figure_35.png]]

#+begin_src jupyter-python
fig, ax = plt.subplots(nrows=1, ncols=n_comp, figsize=(n_comp*width, height), sharey=1)

color = ['#332288', '#88CCEE', '#117733', '#44AA99']

pair = y_single.tasks.unique()
xtime = np.linspace(0, 14, 84)

for i in range(len(pair)):
    mask = (y_single.tasks==y_single.tasks.unique()[i]) & (y_single.laser==laser) & idx_mouse & idx_correct
    X_sel = X_single[mask]

    X_avg = X_sel.mean(0)
    X_sem = X_sel.std(0) / np.sqrt(X_sel.shape[0])  # SEM

    for k in range(n_comp):
        y_avg = X_avg[k]
        y_sem = X_sem[k]
        ax[k].plot(xtime, y_avg, color=color[i], label=pair[i])
        ax[k].fill_between(xtime, y_avg-y_sem, y_avg+y_sem, color=color[i], alpha=0.2)
        ax[k].axhline(0, ls='--', color='k')
        ax[k].set_xlabel('Time')
        ax[k].set_ylabel('PC %d' % (k+1))
        add_vlines(ax[k], if_dpa=0)

        ax[k].legend(fontsize=12, frameon=0, loc='best')

plt.savefig('./figures/single/pca_traj_%s_tasks.svg' % dum)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/pca/figure_36.png]]

#+begin_src jupyter-python
fig, ax = plt.subplots(nrows=1, ncols=n_comp, figsize=(n_comp*width, height), sharey=1)

color = ["#377eb8", "#984ea3", "#4daf4a", "#ffae19"]

pair = ['A', 'B']
xtime = np.linspace(0, 14, 84)

for i in range(2):
    mask = (y_single.sample_odor==i) & (y_single.laser==laser) & idx_mouse & idx_correct
    X_sel = X_single[mask]          # Subselect rows
    X_avg = X_sel.mean(0)    # Mean over trials/samples, shape: (n_comp, n_time)
    X_sem = X_sel.std(0) / np.sqrt(X_sel.shape[0])  # SEM

    for k in range(n_comp):
        y_avg = X_avg[k]
        y_sem = X_sem[k]
        ax[k].plot(xtime, y_avg, color=color[i], label=pair[i])
        ax[k].fill_between(xtime, y_avg-y_sem, y_avg+y_sem, color=color[i], alpha=0.2)
        ax[k].axhline(0, ls='--', color='k')
        ax[k].set_xlabel('Time')
        ax[k].set_ylabel('PC %d' % (k+1))
        add_vlines(ax[k], if_dpa=1)
        ax[k].legend(fontsize=12, frameon=0, loc='best')

plt.savefig('./figures/single/pca_traj_%s_sample.svg' % dum)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/pca/figure_37.png]]

#+begin_src jupyter-python
fig, ax = plt.subplots(nrows=1, ncols=n_comp, figsize=(n_comp*width, height), sharey=1)

color = ["#377eb8", "#4daf4a"]

pair = ['nolick', 'lick']
xtime = np.linspace(0, 14, 84)

for i in range(2):
    mask = (y_single.choice==i) & (y_single.laser==laser) & idx_mouse & idx_correct
    X_sel = X_single[mask]          # Subselect rows
    X_avg = X_sel.mean(0)    # Mean over trials/samples, shape: (n_comp, n_time)
    X_sem = X_sel.std(0) / np.sqrt(X_sel.shape[0])  # SEM

    for k in range(n_comp):
        y_avg = X_avg[k]
        y_sem = X_sem[k]
        ax[k].plot(xtime, y_avg, color=color[i], label=pair[i])
        ax[k].fill_between(xtime, y_avg-y_sem, y_avg+y_sem, color=color[i], alpha=0.2)
        ax[k].axhline(0, ls='--', color='k')
        ax[k].set_xlabel('Time')
        ax[k].set_ylabel('PC %d' % (k+1))
        add_vlines(ax[k], if_dpa=1)
        ax[k].legend(fontsize=12, frameon=0, loc='best')

plt.savefig('./figures/single/pca_traj_%s_licks.svg' % dum)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/pca/figure_38.png]]

#+begin_src jupyter-python
fig, ax = plt.subplots(nrows=1, ncols=n_comp, figsize=(n_comp*width, height), sharey=1)

color = ["#377eb8", "#4daf4a"]

pair = ['C', 'D']
xtime = np.linspace(0, 14, 84)

for i in range(2):
    mask = (y_single.test_odor==i) & (y_single.laser==0) & idx_mouse & idx_correct
    X_sel = X_single[mask]
    X_avg = X_sel.mean(0)    # Mean over trials/samples, shape: (n_comp, n_time)
    X_sem = X_sel.std(0) / np.sqrt(X_sel.shape[0])  # SEM

    for k in range(n_comp):
        y_avg = X_avg[k]
        y_sem = X_sem[k]
        ax[k].plot(xtime, y_avg, color=color[i], label=pair[i])
        ax[k].fill_between(xtime, y_avg-y_sem, y_avg+y_sem, color=color[i], alpha=0.2)
        ax[k].axhline(0, ls='--', color='k')
        ax[k].set_xlabel('Time')
        ax[k].set_ylabel('PC %d' % (k+1))
        add_vlines(ax[k], if_dpa=1)
        ax[k].legend(fontsize=12, frameon=0, loc='best')

plt.savefig('./figures/single/pca_traj_%s_test.svg' % dum)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/pca/figure_39.png]]

#+begin_src jupyter-python
fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(3*width, height))

pair = ['AC', 'AD', 'BD', 'BC']

color = ['#332288', '#88CCEE', '#117733', '#44AA99']

for i in range(4):
    idx = (y_single.odor_pair==i) & (y_single.laser==laser) & idx_mouse & idx_correct

    X_avg = (X_single[idx].mean(0))[:, :66]

    ax[0].plot(X_avg[0], X_avg[1], color=color[i], label=pair[i])
    ax[0].set_xlabel('PC 1')
    ax[0].set_ylabel('PC 2')

    ax[1].plot(X_avg[0], X_avg[2], color=color[i], label=pair[i])
    ax[1].set_xlabel('PC 1')
    ax[1].set_ylabel('PC 3')

    ax[2].plot(X_avg[1], X_avg[2], color=color[i], label=pair[i])
    ax[2].set_xlabel('PC 2')
    ax[2].set_ylabel('PC 3')

for k in range(3):
    ax[k].legend(fontsize=12, frameon=0, loc='best')

plt.savefig('./figures/single/pca_traj_%s_2D.svg' % dum)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/pca/figure_40.png]]

#+begin_src jupyter-python

#+end_src

#+RESULTS:

** Embeddings
*** spatial filter

#+begin_src jupyter-python
z_lim =6
size = 0.05

import cmocean
cmap=cmocean.cm.phase

theta = np.arctan2(w_single[1], w_single[0]) * 180 / np.pi
idx = np.argsort(theta)

theta_norm = (theta+ 360) % (360)

counts, bins, patches = plt.hist(theta_norm, bins='auto', range=(0, 360), density=1)

bin_centers = 0.5*(bins[:-1] + bins[1:])
colors = [cmap(center/(360)) for center in bin_centers]

for patch, color in zip(patches, colors):
    patch.set_facecolor(color)

plt.xlabel('Neuron Loc (°)')
plt.ylabel('Density')
plt.savefig('./figures/single/pca_weights_%s_hist.svg' % dum)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/pca/figure_42.png]]

#+begin_src jupyter-python
from scipy.ndimage import gaussian_filter1d, gaussian_filter1d
fig, ax = plt.subplots(nrows=1, ncols=n_comp, figsize=(n_comp*width, height))

for k in range(n_comp):
    sc = ax[k].scatter(theta[idx], w_single[k][idx], alpha=0.5, c=theta_norm[idx], cmap=cmap, rasterized=1)
    ax[k].plot(theta[idx], gaussian_filter1d(w_single[k][idx], int(size*w_single.shape[1]), mode='wrap'), 'k')
    ax[k].axhline(0, ls='--', color='k')
    ax[k].set_ylabel('Weights PC %d' % (k+1))
    ax[k].set_xlabel('Neuron Loc (°)')
    ax[k].set_ylim([-z_lim, z_lim])

ax[-1].set_ylim([-z_lim/2, z_lim/2])
plt.colorbar(sc, ax=ax[-1], label='Angle (°)')
plt.savefig('./figures/single/pca_weights_%s.svg' % dum)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/pca/figure_43.png]]

#+begin_src jupyter-python
from scipy.ndimage import gaussian_filter1d, uniform_filter1d
fig, ax = plt.subplots(nrows=1, ncols=n_comp, figsize=(n_comp*width, width))

ax[0].scatter(w_single[0][idx], w_single[1][idx], c=theta_norm[idx], cmap=cmap, alpha=0.5, rasterized=1)
ax[0].plot(gaussian_filter1d(w_single[0][idx], int(size*w_single.shape[1]), mode='wrap'), gaussian_filter1d(w_single[1][idx], int(size*w_single.shape[1]), mode='wrap'), 'k')

ax[0].set_xlabel('PC 1')
ax[0].set_ylabel('PC 2')

ax[1].scatter(w_single[0][idx], w_single[2][idx], c=theta_norm[idx], cmap=cmap, alpha=0.5, rasterized=1)
ax[1].plot(gaussian_filter1d(w_single[0][idx], int(size*w_single.shape[1]), mode='wrap'), gaussian_filter1d(w_single[2][idx], int(size*w_single.shape[1]), mode='wrap'), 'k')
ax[1].set_xlabel('PC 1')
ax[1].set_ylabel('PC 3')

sc = ax[2].scatter(w_single[1][idx], w_single[2][idx], c=theta_norm[idx], cmap=cmap, alpha=0.5, rasterized=1)
ax[2].plot(gaussian_filter1d(w_single[1][idx], int(size*w_single.shape[1]), mode='wrap'), gaussian_filter1d(w_single[2][idx], int(size*w_single.shape[1]), mode='wrap'), 'k')
ax[2].set_xlabel('PC 2')
ax[2].set_ylabel('PC 3')

for k in range(3):
    ax[k].set_xlim(-z_lim, z_lim)
    ax[k].set_ylim(-z_lim, z_lim)

plt.colorbar(sc, ax=ax[-1], label='Angle (°)')
plt.savefig('./figures/single/pca_weights_%s_2D.svg' % dum)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/pca/figure_44.png]]

#+begin_src jupyter-python
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

ax.plot(gaussian_filter1d(w_single[0][idx], int(size*w_single.shape[1]), mode='wrap'),
           gaussian_filter1d(w_single[1][idx], int(size*w_single.shape[1]), mode='wrap'),
           gaussian_filter1d(w_single[2][idx], int(size*w_single.shape[1]), mode='wrap'),
           rasterized=1, color='k')


sc = ax.scatter(w_single[0][idx],
                w_single[1][idx],
                w_single[2][idx],
                c=theta_norm[idx], cmap=cmap,
                rasterized=1, alpha=0.5)

ax.tick_params(axis='both', which='major', labelsize=12)  # change both x and y (and z in 3D)
ax.tick_params(axis='z', which='major', labelsize=12)     # for the z-axis specifically

ax.set_xlabel('PC 1', fontsize=12)
ax.set_ylabel('PC 2', fontsize=12)
ax.set_zlabel('PC 3', fontsize=12)

ax.set_xlim([-z_lim, z_lim])
ax.set_ylim([-z_lim, z_lim])
ax.set_zlim([-z_lim/10, z_lim/10])

ax.grid(False)
plt.savefig('./figures/single/pca_weights_%s_3D.svg' % dum)

plt.show()
#+end_src

#+RESULTS:
[[file:./figures/pca/figure_50.png]]

*** theta space

#+begin_src jupyter-python
nbins = 32
theta_bins = np.linspace(0, 360, nbins+1)
theta_digitized = np.digitize(theta_norm, theta_bins) - 1

# For each bin, average w[0], w[1], and w[2]
w_binned = np.zeros((3, nbins))
for i in range(nbins):
    mask = theta_digitized == i
    for j in range(3):
        w_binned[j, i] = np.mean(w_single[j][mask]) if np.any(mask) else np.nan

w_smooth = gaussian_filter1d(w_binned, sigma=2, axis=1, mode='wrap')
#+end_src

#+RESULTS:

#+begin_src jupyter-python
import matplotlib.pyplot as plt
from numpy import deg2rad

bin_centers = 0.5 * (theta_bins[:-1] + theta_bins[1:])
theta_plot = deg2rad(bin_centers)

fig, axs = plt.subplots(nrows=1, ncols=n_comp, figsize=(n_comp*width, height))

for i, ax in enumerate(axs):
    ax.plot(theta_plot * 180 / np.pi, w_smooth[i], lw=2)
    ax.axhline(0, ls='--', color='k')
    ax.set_ylabel('Weights PC %d' % (i+1))
    ax.set_xlabel('Neuron Loc (°)')

axs[-1].axvline(45)
axs[-1].axvline(225)
plt.savefig('./figures/single/pca_weights_%s_bin.svg' % dum)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/pca/figure_51.png]]

#+begin_src jupyter-python
import matplotlib.pyplot as plt
from numpy import deg2rad

# Compute bin centers in degrees and radians
bin_centers = 0.5 * (theta_bins[:-1] + theta_bins[1:])
theta_plot = deg2rad(bin_centers)  # for polar plots

fig, axs = plt.subplots(nrows=1, ncols=n_comp, figsize=(n_comp*width, width), sharey=1)

# PC1 vs PC2
axs[0].plot(w_smooth[0], w_smooth[1], 'k-')
axs[0].set_xlabel('PC 1')
axs[0].set_ylabel('PC 2')

# PC1 vs PC3
axs[1].plot(w_smooth[0], w_smooth[2], 'k-')
axs[1].set_xlabel('PC 1')
axs[1].set_ylabel('PC 3')

# PC2 vs PC3
axs[2].plot(w_smooth[1], w_smooth[2], 'k-')
axs[2].set_xlabel('PC 2')
axs[2].set_ylabel('PC 3')

plt.tight_layout()
plt.savefig('./figures/single/pca_weights_%s_bin2D.svg' % dum)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/pca/figure_53.png]]

#+begin_src jupyter-python
import matplotlib.pyplot as plt
from numpy import deg2rad

bin_centers = 0.5 * (theta_bins[:-1] + theta_bins[1:])
theta_plot = deg2rad(bin_centers)

fig, axs = plt.subplots(1, 3, subplot_kw={'polar': True}, figsize=(n_comp*width, width))

for i, ax in enumerate(axs):
    ax.plot(theta_plot, w_smooth[i], lw=2)

plt.tight_layout()
plt.savefig('./figures/single/pca_weights_%s_bin_polar.svg' % dum)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/pca/figure_54.png]]

#+begin_src jupyter-python

#+end_src

#+RESULTS:

** Opto

#+begin_src jupyter-python
tasks = ['DPA'] # , 'DualGo', 'DualNoGo']
#+end_src

#+RESULTS:

#+begin_src jupyter-python
# laser_mice = ['JawsM01', 'JawsM06', 'JawsM12', 'JawsM15','JawsM18', 'ChRM04', 'ChRM23']
laser_mice = ['JawsM01', 'JawsM06', 'JawsM12', 'JawsM18', 'ChRM04', 'ChRM23']
# laser_mice = ['JawsM01', 'JawsM06', 'JawsM12', 'JawsM15','JawsM18']

traj_mouse = []
for i_mouse, mouse in enumerate(laser_mice):
    idx = (y_single.mouse==mouse) & (y_single.laser==0)
    X_idx = X_single[idx]
    y_idx = y_single[idx]

    traj_ = []
    for i in range(2):
        mask = (y_idx.sample_odor==i) & (y_idx.tasks.isin(tasks)) & (y_idx.performance==1) # & ((y_idx.tasks=='DPA') | (y_idx.odr_perf==1))
        traj_.append(X_idx[mask].mean(0))

    traj_mouse.append(traj_)

traj_mouse = np.array(traj_mouse)
print(traj_mouse.shape)
#+end_src

#+RESULTS:
: (6, 2, 3, 84)

#+begin_src jupyter-python
traj_opto = []
for i_mouse, mouse in enumerate(laser_mice):
    idx = (y_single.mouse==mouse) & (y_single.laser==1)
    X_idx = X_single[idx]
    y_idx = y_single[idx]

    traj_ = []
    for i in range(2):
        mask = (y_idx.sample_odor==i) & (y_idx.tasks.isin(tasks)) & (y_idx.performance==1) # & ((y_idx.tasks=='DPA') | (y_idx.odr_perf==1))
        traj_.append(X_idx[mask].mean(0))

    traj_opto.append(traj_)

traj_opto = np.array(traj_opto)
print(traj_opto.shape)
#+end_src

#+RESULTS:
: (6, 2, 3, 84)

#+begin_src jupyter-python
fp_mouse = np.nanmean(traj_mouse[..., options['bins_LD']], -1)
fp_opto = np.nanmean(traj_opto[..., options['bins_LD']], -1)

print(fp_mouse.shape)

pc1 = fp_mouse[..., 0]
pc2 = fp_mouse[..., 1]
# pc3 = fp_mouse[..., 2]

pc1_opto = fp_opto[..., 0]
pc2_opto = fp_opto[..., 1]
# pc3_opto = fp_opto[..., 2]
#+end_src

#+RESULTS:
: (6, 2, 3)

#+begin_src jupyter-python
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(2*height, height), sharey=1)

for i in range(pc1.shape[0]):
    ax[0].scatter(pc1[i], pc2[i], label=options['mice'][i])
    ax[1].scatter(pc1_opto[i], pc2_opto[i], label=options['mice'][i])

for k in range(2):
    ax[k].axvline(0, color='k')
    ax[k].axhline(0, color='k')

    ax[k].set_xlabel('PC1')
    ax[k].set_ylabel('PC2')
# plt.legend(fontsize=12, frameon=0)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/pca/figure_55.png]]

#+begin_src jupyter-python
perf_off = y_single[y_single['mouse'].isin(laser_mice) & (y_single.laser==0)].groupby(['mouse', 'sample_odor'])['performance'].mean().reset_index()

perf_on = y_single[y_single['mouse'].isin(laser_mice) & (y_single.laser==1)].groupby(['mouse', 'sample_odor'])['performance'].mean().reset_index()

delta_dpa = (perf_on['performance'] - perf_off['performance']).values
print(perf_off.shape, perf_on.shape)
print(delta_dpa)
#+end_src

#+RESULTS:
: (12, 3) (12, 3)
: [-0.01388889  0.00694444 -0.02777778 -0.06944444  0.04166667 -0.00694444
:   0.08333333 -0.02777778  0.0625      0.04166667 -0.02777778  0.04861111]

#+begin_src jupyter-python
perf_off = y_single[y_single['mouse'].isin(laser_mice) & (y_single.laser==0)].groupby(['mouse', 'sample_odor'])['odr_perf'].mean().reset_index()
perf_on = y_single[y_single['mouse'].isin(laser_mice) & (y_single.laser==1)].groupby(['mouse', 'sample_odor'])['odr_perf'].mean().reset_index()

delta_odr = (perf_on['odr_perf'] - perf_off['odr_perf']).values
print(perf_off.shape, perf_on.shape)
print(delta_odr)
#+end_src

#+RESULTS:
: (12, 3) (12, 3)
: [ 0.01041667  0.07291667 -0.02083333  0.          0.02083333  0.05208333
:   0.          0.          0.02083333  0.0625      0.          0.03125   ]

#+begin_src jupyter-python
dPC1 = (pc1_opto - pc1).reshape(-1)
dPC2 = (pc2_opto - pc2).reshape(-1)
print(dPC1.shape, dPC2.shape)
#+end_src

#+RESULTS:
: (12,) (12,)

#+begin_src jupyter-python
df = perf_off[['mouse', 'sample_odor']]
df['delta_dpa'] = delta_dpa
df['delta_odr'] = delta_odr

df['mouse'] = pd.Categorical(df['mouse'], categories=laser_mice, ordered=True)
df = df.sort_values('mouse')

df['delta_pc1'] = dPC1
df['delta_pc2'] = dPC2

# df = df[~df['mouse'].str.contains('ChR')]

print(df.head())
#+end_src

#+RESULTS:
:      mouse  sample_odor  delta_dpa  delta_odr  delta_pc1  delta_pc2
: 4  JawsM01          0.0   0.041667   0.020833  -1.252619   2.997242
: 5  JawsM01          1.0  -0.006944   0.052083  -0.850941   2.870058
: 6  JawsM06          0.0   0.083333   0.000000  -0.919534   1.011042
: 7  JawsM06          1.0  -0.027778   0.000000  -1.017295   0.892365
: 8  JawsM12          0.0   0.062500   0.020833  -0.259220   2.607384

#+begin_src jupyter-python
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from scipy.stats import pearsonr

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(2*width, height), sharey=1)

df_ = df.copy()

for i, delta_perf in enumerate(['delta_dpa', 'delta_odr']):
    sns.regplot(data=df_, x='delta_pc1', y=delta_perf, scatter=True,
                fit_reg=True, ci=95, ax=ax[i],
                scatter_kws={'s': 0, 'alpha': 0.7},
                line_kws={'color': 'k', 'lw': 2, 'ls':'--'})

    sns.scatterplot(data=df_, x='delta_pc1', y=delta_perf,
                    hue='mouse', style=None, s=80, alpha=0.8, ax=ax[i],
                    legend=None, rasterized=1)

    corr, p_value = pearsonr(df_['delta_pc1'].dropna(), df_[delta_perf].dropna())

    annotation = f"Pearson r = {corr:.2f}\np-value = {p_value:.3f}"
    ax[i].annotate(annotation, xy=(.65, 0.95), xycoords='axes fraction', fontsize=14,
                backgroundcolor='white', verticalalignment='top', horizontalalignment='left',
                bbox=dict(edgecolor=None, facecolor='white', boxstyle='round'))

    ax[i].set_xlabel("$\\Delta$ PC1")
    ax[i].set_ylabel("$\\Delta$ Performance")

ax[0].set_ylabel("$\\Delta$ Performance")
ax[1].set_ylabel("")
plt.savefig('./figures/single/pca_opto_%s_pc1.svg' % dum)

plt.show()
#+end_src

#+RESULTS:
[[file:./figures/pca/figure_60.png]]

#+begin_src jupyter-python
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from scipy.stats import pearsonr

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(2*width, height), sharey=1)

df_ = df.copy()

for i, delta_perf in enumerate(['delta_dpa', 'delta_odr']):
    sns.regplot(data=df_, x='delta_pc2', y=delta_perf, scatter=True,
                fit_reg=True, ci=95, ax=ax[i],
                scatter_kws={'s': 0, 'alpha': 0.7},
                line_kws={'color': 'k', 'lw': 2, 'ls':'--'})

    sns.scatterplot(data=df_, x='delta_pc2', y=delta_perf,
                    hue='mouse', style=None, s=80, alpha=0.8, ax=ax[i],
                    legend=None, rasterized=1)

    corr, p_value = pearsonr(df_['delta_pc2'].dropna(), df_[delta_perf].dropna())

    annotation = f"Pearson r = {corr:.2f}\np-value = {p_value:.3f}"
    ax[i].annotate(annotation, xy=(.65, 0.95), xycoords='axes fraction', fontsize=14,
                backgroundcolor='white', verticalalignment='top', horizontalalignment='left',
                bbox=dict(edgecolor=None, facecolor='white', boxstyle='round'))

    ax[i].set_xlabel("$\\Delta$ PC2")

ax[0].set_ylabel("$\\Delta$ Performance")
ax[1].set_ylabel("")

plt.savefig('./figures/single/pca_opto_%s_pc2.svg' % dum)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/pca/figure_61.png]]

#+begin_src jupyter-python

#+end_src

#+RESULTS:

** Binned Flow Fields
*** utils

#+begin_src jupyter-python
import numpy as np

def flow_field_from_trajectories(x, y, dt=1.0, bins=25, xrange=None, yrange=None,
                                 statistic="mean", min_count=1):
    """
    x, y: arrays (n_trials, n_time)
    dt: timestep
    bins: int or (nx, ny)
    xrange, yrange: (min, max); if None inferred from data
    statistic: "mean" (default). (You can extend to median easily.)
    Returns:
      xedges, yedges
      u, v: (nx, ny) average velocities in each spatial bin
      count: (nx, ny) number of samples per bin
    """
    x = np.asarray(x); y = np.asarray(y)
    n_trials, n_time = x.shape
    assert y.shape == x.shape

    # step velocities, shape (n_trials, n_time-1)
    u = (x[:, 1:] - x[:, :-1]) / dt
    v = (y[:, 1:] - y[:, :-1]) / dt

    # positions to bin (start of each step)
    xs = x[:, :-1]
    ys = y[:, :-1]

    if xrange is None:
        xrange = (xs.min(), xs.max())
    if yrange is None:
        yrange = (ys.min(), ys.max())

    if isinstance(bins, int):
        nx = ny = bins
    else:
        nx, ny = bins

    xedges = np.linspace(xrange[0], xrange[1], nx + 1)
    yedges = np.linspace(yrange[0], yrange[1], ny + 1)

    # flatten all steps across trials/time
    xsf = xs.ravel()
    ysf = ys.ravel()
    uf = u.ravel()
    vf = v.ravel()

    # bin index for each sample
    ix = np.searchsorted(xedges, xsf, side="right") - 1
    iy = np.searchsorted(yedges, ysf, side="right") - 1

    valid = (ix >= 0) & (ix < nx) & (iy >= 0) & (iy < ny)
    ix = ix[valid]; iy = iy[valid]
    uf = uf[valid]; vf = vf[valid]

    # accumulate sums and counts
    count = np.zeros((nx, ny), dtype=int)
    usum  = np.zeros((nx, ny), dtype=float)
    vsum  = np.zeros((nx, ny), dtype=float)

    np.add.at(count, (ix, iy), 1)
    np.add.at(usum,  (ix, iy), uf)
    np.add.at(vsum,  (ix, iy), vf)

    # mean velocity per bin
    ugrid = np.full((nx, ny), np.nan, dtype=float)
    vgrid = np.full((nx, ny), np.nan, dtype=float)
    mask = count >= min_count
    ugrid[mask] = usum[mask] / count[mask]
    vgrid[mask] = vsum[mask] / count[mask]

    return xedges, yedges, ugrid, vgrid, count


# Example usage:
# x, y = diffusion_2d(n_trials=200, n_time=2000, dt=0.01, D=0.5, seed=0)

#+end_src

#+RESULTS:

#+begin_src jupyter-python
import numpy as np

def flow_field_midpoint(x, y, dt=1.0, bins=25, xrange=None, yrange=None, min_count=1):
    """
    Mid-point binning: each velocity sample is assigned to the bin containing
    the segment midpoint ((x_t+x_{t+1})/2, (y_t+y_{t+1})/2).

    x, y: (n_trials, n_time)
    Returns: xedges, yedges, U, V, count with U,V,count shaped (nx, ny)
    """
    x = np.asarray(x); y = np.asarray(y)
    assert x.shape == y.shape
    n_trials, n_time = x.shape

    # step velocities (n_trials, n_time-1)
    u = (x[:, 1:] - x[:, :-1]) / dt
    v = (y[:, 1:] - y[:, :-1]) / dt

    # midpoints to bin (n_trials, n_time-1)
    xm = 0.5 * (x[:, 1:] + x[:, :-1])
    ym = 0.5 * (y[:, 1:] + y[:, :-1])

    if xrange is None:
        xrange = (xm.min(), xm.max())
    if yrange is None:
        yrange = (ym.min(), ym.max())

    if isinstance(bins, int):
        nx = ny = bins
    else:
        nx, ny = bins

    xedges = np.linspace(xrange[0], xrange[1], nx + 1)
    yedges = np.linspace(yrange[0], yrange[1], ny + 1)

    # flatten samples
    xf = xm.ravel()
    yf = ym.ravel()
    uf = u.ravel()
    vf = v.ravel()

    # bin indices
    ix = np.searchsorted(xedges, xf, side="right") - 1
    iy = np.searchsorted(yedges, yf, side="right") - 1

    valid = (ix >= 0) & (ix < nx) & (iy >= 0) & (iy < ny)
    ix = ix[valid]; iy = iy[valid]
    uf = uf[valid]; vf = vf[valid]

    # accumulate
    count = np.zeros((nx, ny), dtype=int)
    usum  = np.zeros((nx, ny), dtype=float)
    vsum  = np.zeros((nx, ny), dtype=float)

    np.add.at(count, (ix, iy), 1)
    np.add.at(usum,  (ix, iy), uf)
    np.add.at(vsum,  (ix, iy), vf)

    U = np.full((nx, ny), np.nan, dtype=float)
    V = np.full((nx, ny), np.nan, dtype=float)
    mask = count >= min_count
    U[mask] = usum[mask] / count[mask]
    V[mask] = vsum[mask] / count[mask]

    return xedges, yedges, U, V, count

#+end_src

#+RESULTS:

#+begin_src jupyter-python
import numpy as np

def radial_angular_speeds(x, y, dt=1.0):
    # step velocities at times t -> t+1
    vx = (x[:, 1:] - x[:, :-1]) / dt
    vy = (y[:, 1:] - y[:, :-1]) / dt
    xs = x[:, :-1]
    ys = y[:, :-1]

    r = np.sqrt(xs**2 + ys**2)
    theta = np.arctan2(ys, xs)  # [-pi, pi)

    # avoid r=0 issues
    eps = 1e-12
    r_safe = np.maximum(r, eps)

    vr = (xs*vx + ys*vy) / r_safe
    omega = (xs*vy - ys*vx) / (r_safe**2)

    return r, theta, vr, omega

def bin_mean_1d(xvals, yvals, edges, min_count=1):
    xvals = np.asarray(xvals).ravel()
    yvals = np.asarray(yvals).ravel()

    idx = np.searchsorted(edges, xvals, side="right") - 1
    nbin = len(edges) - 1
    valid = (idx >= 0) & (idx < nbin) & np.isfinite(yvals) & np.isfinite(xvals)
    idx = idx[valid]
    yvals = yvals[valid]

    count = np.zeros(nbin, dtype=int)
    ysum  = np.zeros(nbin, dtype=float)
    np.add.at(count, idx, 1)
    np.add.at(ysum,  idx, yvals)

    ymean = np.full(nbin, np.nan, float)
    m = count >= min_count
    ymean[m] = ysum[m] / count[m]
    centers = 0.5*(edges[:-1] + edges[1:])
    return centers, ymean, count
#+end_src

#+RESULTS:

#+begin_src jupyter-python
import numpy as np

def radial_angular_from_binned_uv(U, V, C, xedges, yedges, r_edges, th_edges,
                                 min_count_bin=1, min_count_1d=1):
    """
    Option (2): convert binned mean velocity field (U,V) into polar components at bin centers,
    then compute weighted 1D profiles vs radius r and angle theta using weights=C.

    Inputs:
      U,V,C: (nx, ny) arrays from flow_field_from_trajectories
      xedges,yedges: bin edges
      r_edges: 1D edges for radius bins
      th_edges: 1D edges for theta bins in [-pi, pi]
      min_count_bin: require C>=this to use a spatial bin at all
      min_count_1d: require total weight in a 1D bin >= this

    Returns:
      r_cent, vr_r, w_r
      th_cent, om_th, w_th
      plus (vr_grid, om_grid) for inspection
    """
    U = np.asarray(U); V = np.asarray(V); C = np.asarray(C)
    nx, ny = U.shape

    xc = 0.5 * (xedges[:-1] + xedges[1:])
    yc = 0.5 * (yedges[:-1] + yedges[1:])
    Xc, Yc = np.meshgrid(xc, yc, indexing="ij")  # (nx, ny)

    r = np.sqrt(Xc**2 + Yc**2)
    th = np.arctan2(Yc, Xc)
    eps = 1e-12
    r_safe = np.maximum(r, eps)

    # polar components derived from mean flow vector in each spatial bin
    vr_grid = np.abs(Xc*U + Yc*V) / r_safe
    om_grid = np.abs(Xc*V - Yc*U) / (r_safe**2)   # angular speed dtheta/dt

    # flatten
    rf = r.ravel()
    thf = th.ravel()
    vrf = vr_grid.ravel()
    omf = om_grid.ravel()
    wf = C.ravel().astype(float)

    # keep only bins with enough samples and finite values
    valid = (wf >= min_count_bin) & np.isfinite(vrf) & np.isfinite(omf) & np.isfinite(rf) & np.isfinite(thf)
    rf, thf, vrf, omf, wf = rf[valid], thf[valid], vrf[valid], omf[valid], wf[valid]

    def weighted_bin_mean(xvals, yvals, wvals, edges, min_w=1.0):
        idx = np.searchsorted(edges, xvals, side="right") - 1
        nbin = len(edges) - 1
        ok = (idx >= 0) & (idx < nbin) & np.isfinite(yvals) & np.isfinite(wvals)
        idx = idx[ok]; yvals = yvals[ok]; wvals = wvals[ok]

        wsum = np.zeros(nbin, float)
        ywsum = np.zeros(nbin, float)
        np.add.at(wsum, idx, wvals)
        np.add.at(ywsum, idx, wvals * yvals)

        ymean = np.full(nbin, np.nan, float)
        m = wsum >= min_w
        ymean[m] = ywsum[m] / wsum[m]
        centers = 0.5*(edges[:-1] + edges[1:])
        return centers, ymean, wsum

    r_cent, vr_r, w_r   = weighted_bin_mean(rf,  vrf, wf, r_edges,  min_w=min_count_1d)
    th_cent, om_th, w_th = weighted_bin_mean(thf, omf, wf, th_edges, min_w=min_count_1d)

    return r_cent, vr_r, w_r, th_cent, om_th, w_th, vr_grid, om_grid
#+end_src

#+RESULTS:

#+begin_src jupyter-python
import numpy as np
from scipy.ndimage import gaussian_filter1d

def gaussian_filter1d_nan(x, sigma, mode="nearest", truncate=4.0):
    x = np.asarray(x, float)
    m = np.isfinite(x).astype(float)          # 1 where valid, 0 where NaN
    x0 = np.where(np.isfinite(x), x, 0.0)

    xs = gaussian_filter1d(x0, sigma=sigma, mode=mode, truncate=truncate)
    ms = gaussian_filter1d(m,  sigma=sigma, mode=mode, truncate=truncate)

    out = xs / ms
    out[ms == 0] = np.nan
    return out
#+end_src

#+RESULTS:

#+begin_src jupyter-python
import numpy as np

def weighted_binned_mean(xvals, yvals, edges, weights=None,
                         min_count=1, min_weight_1d=1.0):
    """
    Bin yvals as a function of xvals using weighted mean.

    min_count: require at least this many contributing samples in each 1D bin
    min_weight_1d: require at least this much total weight in each 1D bin
    """
    xvals = np.asarray(xvals).ravel()
    yvals = np.asarray(yvals).ravel()

    if weights is None:
        weights = np.ones_like(yvals, float)
    else:
        weights = np.asarray(weights).ravel().astype(float)

    idx = np.searchsorted(edges, xvals, side="right") - 1
    nb = len(edges) - 1

    ok = (
        (idx >= 0) & (idx < nb) &
        np.isfinite(xvals) & np.isfinite(yvals) &
        np.isfinite(weights) & (weights > 0)
    )
    idx = idx[ok]
    yvals = yvals[ok]
    weights = weights[ok]

    wsum  = np.zeros(nb, float)
    ywsum = np.zeros(nb, float)
    cnt   = np.zeros(nb, int)

    np.add.at(wsum,  idx, weights)
    np.add.at(ywsum, idx, weights * yvals)
    np.add.at(cnt,   idx, 1)

    ymean = np.full(nb, np.nan)
    m = (cnt >= min_count) & (wsum >= min_weight_1d)
    ymean[m] = ywsum[m] / wsum[m]

    centers = 0.5 * (edges[:-1] + edges[1:])
    return centers, ymean, cnt, wsum
#+end_src

#+RESULTS:

#+begin_src jupyter-python
def binned_mean_1d(xvals, yvals, edges, min_count=1):
    xvals = np.asarray(xvals).ravel()
    yvals = np.asarray(yvals).ravel()
    idx = np.searchsorted(edges, xvals, side="right") - 1
    nb = len(edges) - 1
    ok = (idx >= 0) & (idx < nb) & np.isfinite(xvals) & np.isfinite(yvals)
    idx = idx[ok]; yvals = yvals[ok]

    cnt = np.zeros(nb, int)
    ysum = np.zeros(nb, float)
    np.add.at(cnt, idx, 1)
    np.add.at(ysum, idx, yvals)

    ymean = np.full(nb, np.nan)
    m = cnt >= min_count
    ymean[m] = ysum[m] / cnt[m]
    centers = 0.5*(edges[:-1] + edges[1:])
    return centers, ymean, cnt

def speed_vs_r_theta(x_coor, y_coor, dt=1.0,
                     r_edges=None, theta_edges=None,
                     min_count=50, use_midpoint=False, nbins=32):
    x = np.asarray(x_coor); y = np.asarray(y_coor)

    vx = (x[:, 1:] - x[:, :-1]) / dt
    vy = (y[:, 1:] - y[:, :-1]) / dt
    speed = np.sqrt(vx**2 + vy**2)  # (n_trials, n_time-1)

    if use_midpoint:
        xs = 0.5*(x[:, 1:] + x[:, :-1])
        ys = 0.5*(y[:, 1:] + y[:, :-1])
    else:
        xs = x[:, :-1]
        ys = y[:, :-1]

    r = np.sqrt(xs**2 + ys**2)
    theta = np.arctan2(ys, xs)

    eps = 1e-12
    r_safe = np.maximum(r, eps)
    omega = (xs*vy - ys*vx) / (r_safe**2)

    if r_edges is None:
        r_edges = np.linspace(0, np.nanmax(r), nbins)
    if theta_edges is None:
        theta_edges = np.linspace(-np.pi, np.pi, nbins)

    r_cent, speed_r, r_cnt = binned_mean_1d(r, np.abs(speed), r_edges, min_count=min_count)
    th_cent, speed_th, th_cnt = binned_mean_1d(theta, np.abs(speed), theta_edges, min_count=min_count)

    return (r_cent, speed_r, r_cnt), (th_cent, speed_th, th_cnt)
#+end_src

#+RESULTS:

#+begin_src jupyter-python
import numpy as np
import matplotlib.pyplot as plt

def vr_vth_omega_from_xy(x, y, dt=1.0, eps=1e-12):
    x = np.asarray(x); y = np.asarray(y)
    vx = (x[:, 1:] - x[:, :-1]) / dt
    vy = (y[:, 1:] - y[:, :-1]) / dt
    xs = x[:, :-1]; ys = y[:, :-1]

    r = np.sqrt(xs**2 + ys**2)
    th = np.arctan2(ys, xs)
    r_safe = np.maximum(r, eps)

    vr  = (xs*vx + ys*vy) / r_safe
    vth = (-ys*vx + xs*vy) / r_safe              # = (xs*vy - ys*vx)/r
    omega = (xs*vy - ys*vx) / (r_safe**2)        # = vth / r
    return r, th, vr, vth, omega

def binned_mean(xvals, yvals, edges, min_count=50):
    xvals = np.asarray(xvals).ravel()
    yvals = np.asarray(yvals).ravel()
    idx = np.searchsorted(edges, xvals, side="right") - 1
    nb = len(edges) - 1
    ok = (idx >= 0) & (idx < nb) & np.isfinite(xvals) & np.isfinite(yvals)
    idx = idx[ok]; yvals = yvals[ok]

    cnt = np.zeros(nb, int)
    ysum = np.zeros(nb, float)
    np.add.at(cnt, idx, 1)
    np.add.at(ysum, idx, yvals)

    ymean = np.full(nb, np.nan)
    m = cnt >= min_count
    ymean[m] = ysum[m] / cnt[m]
    centers = 0.5*(edges[:-1] + edges[1:])
    return centers, ymean, cnt
#+end_src

#+RESULTS:
: d7c9f692-6a78-4116-9f8b-f0cfb503c8f7

*** all mice

#+begin_src jupyter-python
from scipy.ndimage import gaussian_filter1d, uniform_filter1d
dt = 1
nbins = 32
min_count = 1  # choose based on how noisy you expect things to be
min_w=1
sigma_r, sigma_th= 1, 1
#+end_src

#+RESULTS:

**** cartesian speeds

#+begin_src jupyter-python
r_list, sp_r_list = [], []
th_list, sp_th_list = [], []

for i_mouse in range(len(options['mice'])):
    idx = (y_single.laser==0) & (y_single.mouse==options['mice'][i_mouse])
    X_delay = X_single[idx].copy()

    x_coor = X_delay[:, 0, options['bins_DELAY']]
    y_coor = X_delay[:, 1, options['bins_DELAY']]

    (r_c, sp_r, r_cnt), (th_c, sp_th, th_cnt) = speed_vs_r_theta(x_coor, y_coor, dt=dt, min_count=min_count, use_midpoint=True)

    r_c /= np.nanmax(r_c)

    m_sp_r = np.nanmean(sp_r)
    std_sp_r = np.nanstd(sp_r)

    sp_r = (sp_r - m_sp_r) / std_sp_r

    r_list.append(r_c)
    th_list.append(th_c)

    m_sp_th = np.nanmean(sp_th)
    std_sp_th = np.nanstd(sp_th)

    sp_th = (sp_th - m_sp_th) / std_sp_th

    sp_r_list.append(sp_r)
    sp_th_list.append(sp_th)
#+end_src

#+RESULTS:

#+begin_src jupyter-python
from scipy.stats import circmean
r_list = pad_list(r_list, axis=0, max_len=None)
sp_r_list = pad_list(sp_r_list, axis=0, max_len=None)
mean_sp_r = np.nanmean(uniform_filter1d(np.array(sp_r_list), sigma_r), 0)

th_list = pad_list(th_list, axis=0, max_len=None)
sp_th_list = pad_list(sp_th_list, axis=0, max_len=None)
mean_sp_th = np.nanmean(uniform_filter1d(np.array(sp_th_list), sigma_th), 0)
#+end_src

#+RESULTS:
: cfd4df66-dbc0-457c-ba5f-7c6509053efe

#+begin_src jupyter-python
from scipy.ndimage import gaussian_filter1d, uniform_filter1d
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(2*width, height),)

for i in range(len(options['mice'])):
    r_c = r_list[i]
    sp_r = sp_r_list[i]

    ax[0].plot(r_c, uniform_filter1d(sp_r, sigma_r), alpha=0.25)

    th_c = th_list[i]
    sp_th = sp_th_list[i]

    ax[1].plot(th_c * 180 / np.pi, uniform_filter1d(sp_th, sigma_th, mode='wrap'), alpha=0.25)

ax[0].plot(r_c, mean_sp_r, 'k')
ax[1].plot(th_c * 180 / np.pi, mean_sp_th, 'k')

ax[0].set_xlabel("r")
ax[0].set_ylabel(r'$\langle v\rangle(r)$')

ax[1].set_xlabel(r'$\theta$ (rad)')
ax[1].set_ylabel(r'$\langle v\rangle(\theta)$')
plt.show()
#+end_src

#+RESULTS:
: f343e1c3-b05b-48e3-bef3-22e64ee2a4ae

**** binned cartesian speeds

#+begin_src jupyter-python
r_list, sp_r_list = [], []
th_list, sp_th_list = [], []

for i_mouse in range(len(options['mice'])):
    idx = (y_single.laser==0) & (y_single.mouse==options['mice'][i_mouse])
    X_delay = X_single[idx].copy()

    x_coor = X_delay[:, 0, options['bins_DELAY']]
    y_coor = X_delay[:, 1, options['bins_DELAY']]

    xedges, yedges, U, V, C = flow_field_midpoint(x_coor, y_coor, dt=1, bins=nbins)

    speed = np.sqrt(U**2 + V**2)
    speed = np.where(C >= min_count, speed, np.nan)

    # bin centers -> R, TH
    xc = 0.5*(xedges[:-1] + xedges[1:])
    yc = 0.5*(yedges[:-1] + yedges[1:])
    Xc, Yc = np.meshgrid(xc, yc, indexing="ij")
    R  = np.sqrt(Xc**2 + Yc**2)
    TH = np.arctan2(Yc, Xc)

    # speed vs radius (weighted by C)
    r_edges = np.linspace(0, np.nanmax(R), nbins)
    r_c, sp_r, cnt_r, wsum_r = weighted_binned_mean(R, speed, r_edges, weights=C,min_count=min_count, min_weight_1d=min_w)

    # speed vs theta (weighted by C)
    th_edges = np.linspace(-np.pi, np.pi, nbins)
    th_c, sp_th, cnt_th, wsum_th = weighted_binned_mean(TH, speed, th_edges, weights=C, min_count=min_count, min_weight_1d=min_w)

    r_c /= np.nanmax(r_c)

    m_sp_r = np.nanmean(sp_r)
    std_sp_r = np.nanstd(sp_r)

    sp_r = (sp_r - m_sp_r) / std_sp_r

    r_list.append(r_c)
    th_list.append(th_c)

    m_sp_th = np.nanmean(sp_th)
    std_sp_th = np.nanstd(sp_th)

    sp_th = (sp_th - m_sp_th) / std_sp_th

    sp_r_list.append(sp_r)
    sp_th_list.append(sp_th)
#+end_src

#+RESULTS:
: 701ca3b2-fe77-42a6-8275-8bcb3e02ce4d

#+begin_src jupyter-python
from scipy.stats import circmean
r_list = pad_list(r_list, axis=0, max_len=None)
sp_r_list = pad_list(sp_r_list, axis=0, max_len=None)

mean_sp_r = np.nanmean(sp_r_list, 0)
std_sp_r = np.nanstd(sp_r_list, 0) / np.sqrt(len(options['mice']))


th_list = pad_list(th_list, axis=0, max_len=None)
sp_th_list = pad_list(sp_th_list, axis=0, max_len=None)
mean_sp_th = circmean(sp_th_list, -np.pi, np.pi, 0)
std_sp_th = np.nanstd(sp_th_list, 0) / np.sqrt(len(options['mice']))
#+end_src

#+RESULTS:
: a2925d6a-b860-476f-9e77-011b24a7e589

#+begin_src jupyter-python
sigma_r = 5
sigma_th = 5


fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(2*width, height),)

for i in range(len(options['mice'])):
    r_c = r_list[i]
    sp_r = sp_r_list[i]

    r_c /= np.nanmax(r_c)

    m_sp_r = np.nanmean(sp_r)
    std_sp_r = np.nanstd(sp_r)

    sp_r = (sp_r - m_sp_r) / std_sp_r

    ax[0].plot(r_c, uniform_filter1d(sp_r, sigma_r), alpha=0.25)

    th_c = th_list[i]
    sp_th = sp_th_list[i]

    m_sp_th = np.nanmean(sp_th)
    std_sp_th = np.nanstd(sp_th)

    sp_th = (sp_th - m_sp_th) / std_sp_th

    ax[1].plot(th_c * 180 / np.pi, uniform_filter1d(sp_th, sigma_th, mode='wrap'), alpha=0.25)

ax[0].plot(r_c, uniform_filter1d(mean_sp_r, sigma_r), 'k')
ax[1].plot(th_c * 180 / np.pi, uniform_filter1d(mean_sp_th, sigma_th), 'k')

ax[0].set_xlabel("r")
ax[0].set_ylabel(r'$\langle v_{bin} \rangle(r)$')

ax[1].set_xlabel(r'$\theta$ (rad)')
ax[1].set_ylabel(r'$\langle v_{bin}\rangle(\theta)$')
plt.show()
#+end_src

#+RESULTS:
: 06149a90-6eb3-4271-bf61-7201e028b949

#+begin_src jupyter-python


#+end_src

#+RESULTS:
: d05889a0-6d68-49ce-9844-c9270b01fa5d

**** polar speeds

#+begin_src jupyter-python
r_list = []
avr_r_list, avr_th_list = [], []
avth_r_list, avth_th_list = [], []
aw_r_list, aw_th_list = [], []

for i_mouse in range(len(options['mice'])):
    idx = (y_single.laser==0) & (y_single.mouse==options['mice'][i_mouse])
    X_delay = X_single[idx].copy()

    x_coor = X_delay[:, 0, options['bins_DELAY']]
    y_coor = X_delay[:, 1, options['bins_DELAY']]

    # --- compute components
    r, th, vr, vth, omega = vr_vth_omega_from_xy(x_coor, y_coor, dt=dt)

    # magnitudes
    avr  = np.abs(vr)
    avth = np.abs(vth)
    aw = omega

    # --- bin vs radius
    r_edges = np.linspace(0, np.nanmax(r), nbins)
    r_c, avr_r,  _ = binned_mean(r,  avr,  r_edges, min_count=min_count)
    _,   avth_r, _ = binned_mean(r,  avth, r_edges, min_count=min_count)
    _, aw_r, _ = binned_mean(r, aw, r_edges, min_count=min_count)

    th_edges = np.linspace(-np.pi, np.pi, nbins)
    th_c, avr_th,  _ = binned_mean(th, avr,  th_edges, min_count=min_count)
    _,    avth_th, _ = binned_mean(th, avth, th_edges, min_count=min_count)
    _, aw_th, _ = binned_mean(th, aw, th_edges, min_count=min_count)

    r_c /= np.nanmax(r_c)

    # m_avr_r = np.nanmean(avr_r)
    # std_avr_r = np.nanstd(avr_r)
    # avr_r = (avr_r - m_avr_r) / std_avr_r

    # m_avr_th = np.nanmean(avr_th)
    # std_avr_th = np.nanstd(avr_th)
    # avr_th = (avr_th - m_avr_th) / std_avr_th

    # m_avth_r = np.nanmean(avth_r)
    # std_avth_r = np.nanstd(avth_r)
    # avth_r = (avth_r - m_avth_r) / std_avth_r

    # m_avth_th = np.nanmean(avth_th)
    # std_avth_th = np.nanstd(avth_th)
    # avth_th = (avth_th - m_avth_th) / std_avth_th

    # m_aw_r = np.nanmean(aw_r)
    # std_aw_r = np.nanstd(aw_r)
    # aw_r = (aw_r - m_aw_r) / std_aw_r

    # m_aw_th = np.nanmean(aw_th)
    # std_aw_th = np.nanstd(aw_th)
    # aw_th = (aw_th - m_aw_th) / std_aw_th

    r_list.append(r_c)
    th_list.append(th_c)

    avr_r_list.append(avr_r)
    avr_th_list.append(avr_th)

    avth_r_list.append(avth_r)
    avth_th_list.append(avth_th)

    aw_r_list.append(aw_r)
    aw_th_list.append(aw_th)
#+end_src

#+RESULTS:
: 4f05f15d-ded9-469c-818a-51ac1eb0f3ec

#+begin_src jupyter-python
r_list = pad_list(r_list, axis=0, max_len=None)

avr_r_list = pad_list(avr_r_list, axis=0, max_len=None)
avth_r_list = pad_list(avth_r_list, axis=0, max_len=None)
aw_r_list = pad_list(aw_r_list, axis=0, max_len=None)

mean_avr_r = np.nanmean(uniform_filter1d(avr_r_list, sigma_r, axis=-1), 0)
mean_avth_r = np.nanmean(uniform_filter1d(avth_r_list, sigma_r, axis=-1), 0)
mean_aw_r = np.nanmean(uniform_filter1d(aw_r_list, sigma_r, axis=-1), 0)

mean_avr_r = uniform_filter1d(np.nanmean(avr_r_list, 0), sigma_r)
mean_avth_r = uniform_filter1d(np.nanmean(avth_r_list, 0), sigma_r)
mean_awr = uniform_filter1d(circmean(aw_r_list, low=-np.pi, high=np.pi, axis=0), sigma_r)

std_avr_r = uniform_filter1d(np.nanstd(avr_r_list, 0), sigma_r) / np.sqrt(len(options['mice']))
std_avth_r = uniform_filter1d(np.nanstd(avth_r_list, 0), sigma_r) / np.sqrt(len(options['mice']))
std_aw_r = uniform_filter1d(np.nanstd(aw_r_list, 0), sigma_r) / np.sqrt(len(options['mice']))

th_list = pad_list(th_list, axis=0, max_len=None)

avr_th_list = pad_list(avr_th_list, axis=0, max_len=None)
avth_th_list = pad_list(avth_th_list, axis=0, max_len=None)
aw_th_list = pad_list(aw_th_list, axis=0, max_len=None)

mean_avr_th = uniform_filter1d(np.mean(avr_th_list, 0), sigma_th, mode='wrap')
mean_avth_th = uniform_filter1d(np.mean(avth_th_list, axis=0), sigma_th, mode='wrap')
mean_aw_th = uniform_filter1d(circmean(aw_th_list, low=-np.pi, high=np.pi, axis=0), sigma_th, mode='wrap')

std_avr_th = uniform_filter1d(np.nanstd(avr_th_list, 0), sigma_th) / np.sqrt(len(options['mice']))
std_avth_th = uniform_filter1d(np.nanstd(avth_th_list, 0), sigma_th) / np.sqrt(len(options['mice']))
std_aw_th = uniform_filter1d(np.nanstd(aw_th_list, 0), sigma_th) / np.sqrt(len(options['mice']))
#+end_src

#+RESULTS:
: ec8df3e2-28c3-447b-a487-87551b095dea

#+begin_src jupyter-python
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(2*width, height),)

for i in range(len(options['mice'])):
    r_c = r_list[i]
    th_c = th_list[i]

    avr_r = avr_r_list[i]
    avr_th = avr_th_list[i]

    ax[0].plot(r_c, uniform_filter1d(avr_r, sigma_r), alpha=0.25)
    ax[1].plot(th_c * 180 / np.pi, uniform_filter1d(avr_th, sigma_th), alpha=0.2)

ax[0].plot(r_c, mean_avr_r, 'k')
ax[1].plot(th_c*180 / np.pi, mean_avr_th, 'k')

ax[0].fill_between(r_c, mean_avr_r-std_avr_r, mean_avr_r+std_avr_r, alpha=.2)
ax[1].fill_between(th_c*180/np.pi, mean_avr_th-std_avr_th, mean_avr_th+std_avr_th, alpha=.2)

ax[0].axhline(0, color='k', ls='--')
ax[0].set_xlabel("radius r")
ax[0].set_ylabel("$<v_r>$")

ax[1].axhline(0, color='k', ls='--')
ax[1].set_xlabel("angle θ (°)")
ax[1].set_ylabel("$<v_r>$")

# plt.legend()
plt.show()
#+end_src

#+RESULTS:
: 2ae097d8-3f1f-436c-81a9-e830e02bc28e

#+begin_src jupyter-python
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(2*width, height),)

for i in range(len(options['mice'])):
    r_c = r_list[i]
    th_c = th_list[i]

    avth_r = avth_r_list[i]
    avth_th = avth_th_list[i]

    ax[0].plot(r_c, uniform_filter1d(avth_r, sigma_r), alpha=0.25)
    ax[1].plot(th_c * 180 / np.pi, uniform_filter1d(avth_th, sigma_th), alpha=0.25)

ax[0].plot(r_c, mean_avth_r, 'k')
ax[1].plot(th_c*180 / np.pi, mean_avth_th, 'k')

ax[0].fill_between(r_c, mean_avth_r-std_avth_r, mean_avth_r+std_avth_r, alpha=.2)
ax[1].fill_between(th_c*180/np.pi, mean_avth_th-std_avth_th, mean_avth_th+std_avth_th, alpha=.2)

ax[0].axhline(0, color='k', ls='--')
ax[0].set_xlabel("radius r")
ax[0].set_ylabel("$<v_\\theta>$")

ax[1].axhline(0, color='k', ls='--')
ax[1].set_xlabel("angle θ (°)")
ax[1].set_ylabel("$<v_\\theta>$")

plt.show()
#+end_src

#+RESULTS:
: cbb1e108-b494-4042-9ef6-95160022852f

#+begin_src jupyter-python
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(2*width, height),)

for i in range(len(options['mice'])):
    r_c = r_list[i]
    th_c = th_list[i]

    aw_r = aw_r_list[i]
    aw_th = aw_th_list[i]

    ax[0].plot(r_c, uniform_filter1d(aw_r, sigma_r), alpha=0.2)
    ax[1].plot(th_c * 180 / np.pi, uniform_filter1d(aw_th, sigma_th), alpha=0.2)

ax[0].plot(r_c, mean_aw_r, 'k')
ax[1].plot(th_c * 180 / np.pi, mean_aw_th, 'k')

ax[0].fill_between(r_c, mean_aw_r-std_aw_r, mean_aw_r+std_aw_r, alpha=.2)
ax[1].fill_between(th_c*180/np.pi, mean_aw_th-std_aw_th, mean_aw_th+std_aw_th, alpha=.2)

ax[0].axhline(0, color='k', ls='--')
ax[0].set_xlabel("radius r")
ax[0].set_ylabel("$<\\omega>$")

ax[1].axhline(0, color='k', ls='--')
ax[1].set_xlabel("angle θ (°)")
ax[1].set_ylabel("$<\\omega>$")

plt.show()
#+end_src

#+RESULTS:
: 2d0ee18d-d6e6-4d98-adc3-f6ee20e8113d

#+begin_src jupyter-python

#+end_src

#+RESULTS:
: f7dd0eaf-9588-4688-a3ed-6b6ce2340669

*** data
**** data

 #+begin_src jupyter-python
n_comp = 3
laser = 0
i_mouse = 3

idx_mouse = True
if i_mouse !=-1:
    idx_mouse = (y_single.mouse==options['mice'][i_mouse])
#+end_src

#+RESULTS:
: 8e1b300f-a6d2-4ad9-855a-2cb7c8ad2393

#+begin_src jupyter-python
dt = 1
nbins = 16
min_count = 1
min_w = 1
sigma_r, sigma_th= 15, 15
#+end_src

#+RESULTS:
: e6547eca-f86b-4aa3-b073-9e95cd64d926

#+begin_src jupyter-python
from scipy.ndimage import gaussian_filter1d, uniform_filter1d

idx = (y_single.tasks=='DPA') & (y_single.laser==0) & (y_single.mouse==options['mice'][i_mouse])
# idx = (y_single.laser==0) & (y_single.mouse==options['mice'][i_mouse])

X_delay = X_single[idx].copy()

x_coor = X_delay[:, 0]
y_coor = X_delay[:, 1]

bins = np.concatenate( (options['bins_BL'], options['bins_DELAY']))

x_coor = X_delay[:, 0, bins]
y_coor = X_delay[:, 1, bins]

# x_coor = X_delay[:, 0, :options['bins_DELAY'][-1]]
# y_coor = X_delay[:, 1, :options['bins_DELAY'][-1]]

# x_coor = X_delay[:, 0, options['bins_DELAY']]
# y_coor = X_delay[:, 1, options['bins_DELAY']]

print(x_coor.shape)
#+end_src

#+RESULTS:
: 0d6206f3-6222-463d-a969-ea81fd8da30f

**** binned flows

#+begin_src jupyter-python
# xedges, yedges, U, V, C = flow_field_from_trajectories(x_coor, y_coor, dt=1, bins=32)
xedges, yedges, U, V, C = flow_field_midpoint(x_coor, y_coor, dt=dt, bins=nbins)
#+end_src

#+RESULTS:
: d7b50657-4aad-48cc-a9b1-3ac6372a6cd7

#+begin_src jupyter-python
from matplotlib import colors

speed = np.sqrt(U**2 + V**2)
speed = np.where(C >= min_count, speed, np.nan)

vmin = 0.0
vmax = np.nanpercentile(speed, 95)  # robust upper limit

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(2*width, height), sharey=1, sharex=1)

ax[0].plot(x_coor.T, y_coor.T, alpha=0.3)

pcm = ax[1].pcolormesh(xedges, yedges, speed.T, shading="auto", vmin=vmin, vmax=vmax)

cbar = fig.colorbar(pcm, ax=ax[1])
cbar.set_label("Speed")


# pcm = ax[2].pcolormesh(
#     xedges, yedges, speed.T, shading="auto",
#     norm=colors.LogNorm(vmin=np.nanmin(speed[speed>0]), vmax=vmax)
# )

# cbar = fig.colorbar(pcm, ax=ax[2])
# cbar.set_label("Speed (log)")

# optional: overlay flow direction
xc = 0.5*(xedges[:-1] + xedges[1:])
yc = 0.5*(yedges[:-1] + yedges[1:])
Xc, Yc = np.meshgrid(xc, yc, indexing="ij")
ax[1].quiver(Xc, Yc, U, V, color="w", angles="xy", scale_units="xy", scale=.2)
# ax[2].quiver(Xc, Yc, U, V, color="w", angles="xy", scale_units="xy", scale=.25)

# ax[0].set_xlim([-6, 6])
# ax[0].set_ylim([-4, 4])
ax[0].set_xlabel('PC1')
ax[0].set_ylabel('PC2')

plt.show()
#+end_src

#+RESULTS:
: 908cfc9b-1243-4b19-bb2c-fd3a150e5be6


#+begin_src jupyter-python
(r_c, sp_r, r_cnt), (th_c, sp_th, th_cnt) = speed_vs_r_theta(x_coor, y_coor, dt=dt, min_count=min_count, use_midpoint=True)
#+end_src

#+RESULTS:
: af000e74-196f-4f8b-afb7-69bc01251756

#+begin_src jupyter-python
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(2*width, height),)

ax[0].plot(r_c, sp_r)
ax[0].plot(r_c, uniform_filter1d(sp_r, sigma_r), color='k')

ax[0].set_xlabel("r")
ax[0].set_ylabel(r'$\langle \sqrt{v_x^2+v_y^2}\rangle(r)$')

ax[1].plot(th_c * 180 / np.pi, sp_th)
ax[1].plot(th_c * 180 / np.pi, uniform_filter1d(sp_th, sigma_th, mode='wrap'), color='k')

ax[1].set_xlabel(r'$\theta$ (rad)')
ax[1].set_ylabel(r'$\langle \sqrt{v_x^2+v_y^2}\rangle(\theta)$')
plt.show()
#+end_src

#+RESULTS:
: 939d646a-92ac-429b-b5a3-5907cffe5a4e


#+begin_src jupyter-python
import numpy as np

# per (x,y) bin:
speed = np.sqrt(U**2 + V**2)
speed = np.where(C >= min_count, speed, np.nan)  # optional mask low-occupancy bins

# bin centers -> R, TH
xc = 0.5*(xedges[:-1] + xedges[1:])
yc = 0.5*(yedges[:-1] + yedges[1:])
Xc, Yc = np.meshgrid(xc, yc, indexing="ij")
R  = np.sqrt(Xc**2 + Yc**2)
TH = np.arctan2(Yc, Xc)

# speed vs radius (weighted by C)
r_edges = np.linspace(0, np.nanmax(R), nbins)
r_c, speed_r, cnt_r, wsum_r = weighted_binned_mean(
    R, speed, r_edges, weights=C,
    min_count=min_count, min_weight_1d=min_w
)

# speed vs theta (weighted by C)
th_edges = np.linspace(-np.pi, np.pi, nbins)
th_c, speed_th, cnt_th, wsum_th = weighted_binned_mean(
    TH, speed, th_edges, weights=C,
    min_count=min_count, min_weight_1d=min_w
)
#+end_src

#+RESULTS:
: 767aeb28-802d-4a56-9376-3d1a4a7ab2fd

#+begin_src jupyter-python
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(2*width, height),)

ax[0].plot(r_c, speed_r)
ax[0].plot(r_c, uniform_filter1d(speed_r, sigma_r), color='k')

ax[0].set_xlabel("r")
ax[0].set_ylabel(r'$\langle |\mathbf{v}| \rangle(r)$')


ax[1].plot(th_c, speed_th)
ax[1].plot(th_c, uniform_filter1d(speed_th, sigma_th, mode='wrap'), color='k')

ax[1].set_xlabel(r'$\theta$ (rad)')
ax[1].set_ylabel(r'$\langle |\mathbf{v}| \rangle(\theta)$')
plt.show()
#+end_src

#+RESULTS:
: 3f9b5dab-5215-4421-93b2-3427a3032625

#+begin_src jupyter-python
import numpy as np

def polar_components_from_steps(x, y, dt=1.0, eps=1e-12):
    vx = (x[:,1:] - x[:,:-1]) / dt
    vy = (y[:,1:] - y[:,:-1]) / dt
    xs = x[:,:-1]; ys = y[:,:-1]
    r = np.sqrt(xs**2 + ys**2)
    th = np.arctan2(ys, xs)
    r_safe = np.maximum(r, eps)
    vr  = (xs*vx + ys*vy) / r_safe
    vth = (-ys*vx + xs*vy) / r_safe
    return r, th, vr, vth

def bin2d_mean(r, th, val, r_edges, th_edges, min_count=50):
    r = r.ravel(); th = th.ravel(); val = val.ravel()
    ir = np.searchsorted(r_edges, r, side="right") - 1
    it = np.searchsorted(th_edges, th, side="right") - 1
    nr = len(r_edges)-1; nt = len(th_edges)-1
    ok = (ir>=0)&(ir<nr)&(it>=0)&(it<nt)&np.isfinite(val)
    ir = ir[ok]; it = it[ok]; val = val[ok]

    cnt = np.zeros((nr, nt), int)
    s   = np.zeros((nr, nt), float)
    np.add.at(cnt, (ir, it), 1)
    np.add.at(s,   (ir, it), val)

    mean = np.full((nr, nt), np.nan)
    m = cnt >= min_count
    mean[m] = s[m] / cnt[m]
    return mean, cnt

def angular_anisotropy(mean_rt, eps=1e-12):
    # mean_rt: (nr, nt) array of mean quantity vs (r,theta)
    mu = np.nanmean(mean_rt, axis=1)          # mean over theta for each r
    sd = np.nanstd(mean_rt, axis=1)           # std over theta for each r
    A = sd / (np.abs(mu) + eps)               # relative angular modulation
    return A, mu, sd

# Example usage:
r, th, vr, vth = polar_components_from_steps(x_coor, y_coor, dt=dt)
r_edges  = np.linspace(0, np.nanmax(r), nbins)
th_edges = np.linspace(-np.pi, np.pi, nbins)
vr_rt, vr_cnt = bin2d_mean(r, th, np.abs(vr), r_edges, th_edges, min_count=min_w)
A_vr, vr_mu, vr_sd = angular_anisotropy(vr_rt)
vth_rt, vth_cnt = bin2d_mean(r, th, np.abs(vth), r_edges, th_edges, min_count=min_w)
#+end_src

#+RESULTS:
: 2c74a212-d9d4-44de-b99f-cac6e9f21f09

**** counts

 #+begin_src jupyter-python
import numpy as np
import matplotlib.pyplot as plt

r_cent  = 0.5*(r_edges[:-1] + r_edges[1:])
th_cent = 0.5*(th_edges[:-1] + th_edges[1:])

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(2*width, height),)

ax[0].pcolormesh(th_edges, r_edges, vr_rt, shading="auto")
ax[0].set_xlabel(r'$\theta$ (rad)')
ax[0].set_ylabel('r')

ax[1].pcolormesh(th_edges, r_edges, vth_rt, shading="auto")
ax[1].set_xlabel(r'$\theta$ (rad)')
ax[1].set_ylabel('r')

plt.show()
#+end_src

#+RESULTS:
: 8e817d16-de0e-41f9-8dc0-c45f0220d518

#+begin_src jupyter-python
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(2*width, height),)

ax[0].pcolormesh(th_edges, r_edges, vr_cnt, shading="auto")
ax[0].set_xlabel(r'$\theta$ (rad)')
ax[0].set_ylabel('r')

ax[1].pcolormesh(th_edges, r_edges, vth_cnt, shading="auto")
ax[1].set_xlabel(r'$\theta$ (rad)')
ax[1].set_ylabel('r')

plt.show()
#+end_src

#+RESULTS:
: 0dc59b4e-7461-47cc-adf8-f7246a77deb7

#+begin_src jupyter-python

#+end_src

#+RESULTS:
: 0a78d9fe-7f07-4bc3-898c-41340211731f

**** speeds

#+begin_src jupyter-python
# --- compute components
r, th, vr, vth, omega = vr_vth_omega_from_xy(x_coor, y_coor, dt=dt)

# magnitudes
avr  = np.abs(vr)
avth = np.abs(vth)
aw = np.abs(omega)

# --- bin vs radius
r_edges = np.linspace(0, np.nanmax(r), nbins)
r_c, avr_r,  _ = binned_mean(r,  avr,  r_edges, min_count=min_count)
_,   avth_r, _ = binned_mean(r,  avth, r_edges, min_count=min_count)
_, aw_r, _ = binned_mean(r, aw, r_edges, min_count=min_count)

th_edges = np.linspace(-np.pi, np.pi, nbins)  # ~5 degree bins
th_c, avr_th,  _ = binned_mean(th, avr,  th_edges, min_count=min_count)
_,    avth_th, _ = binned_mean(th, avth, th_edges, min_count=min_count)
_, aw_th, _ = binned_mean(th, aw, th_edges, min_count=min_count)
#+end_src

#+RESULTS:
: 525528ba-88f9-412b-b9ec-bcbcc809f8f8

#+begin_src jupyter-python
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(2*width, height),)

ax[0].plot(r_c, avr_r,  label=r'$\langle|v_r|\rangle(r)$')
ax[0].plot(r_c, avth_r, label=r'$\langle|v_\theta|\rangle(r)$')
ax[0].axhline(0, color='k', ls='--')
ax[0].set_xlabel("radius r")
ax[0].set_ylabel("$<v_r>, <v_\\theta>$")

ax[1].plot(th_c * 180 / np.pi, avr_th,  label=r'$\langle|v_r|\rangle(\theta)$')
ax[1].plot(th_c * 180 / np.pi, avth_th, label=r'$\langle|v_\theta|\rangle(\theta)$')
ax[1].axhline(0, color='k', ls='--')
ax[1].set_xlabel("angle θ (°)")
ax[1].set_ylabel("$<v_r>, <v_\\theta>$")

# plt.legend()
plt.show()
#+end_src

#+RESULTS:
: e13111a3-6219-49ab-8832-b5f79b0f4a8c

#+begin_src jupyter-python
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(2*width, height),)

ax[0].plot(r_c, aw_r,  label=r'$\langle|v_r|\rangle(r)$')
ax[0].axhline(0, color='k', ls='--')
ax[0].set_xlabel("radius r")
ax[0].set_ylabel("$<\\omega>$")


ax[1].plot(th_c* 180 / np.pi, aw_th,  label=r'$\langle|v_r|\rangle(\theta)$')
ax[1].plot(th_c* 180 / np.pi, uniform_filter1d(aw_th, sigma_th, mode='wrap'), color='k')
ax[1].axhline(0, color='k', ls='--')
ax[1].set_xlabel("angle θ (°)")
ax[1].set_ylabel("$<\\omega>$")

plt.show()
#+end_src

#+RESULTS:
: 486262e8-8178-4bd2-bc25-31010efc5e16

**** speeds from binned field

#+begin_src jupyter-python
import numpy as np

def binned_velocity_and_speed(
    x, y, dt,
    xedges, yedges,
    min_count=1,
    nan_empty=True,
):
    """
    Compute per-bin:
      U,V  = mean(vx), mean(vy)               (mean velocity components)
      S    = mean(speed) = mean(sqrt(vx^2+vy^2))  (mean speed; NOT sqrt(U^2+V^2))
      C    = counts per bin (# velocity samples falling in bin)

    x,y: arrays (n_trials, n_time)
    dt:  scalar timestep
    xedges,yedges: bin edges
    """
    x = np.asarray(x); y = np.asarray(y)
    assert x.shape == y.shape
    n_trials, n_time = x.shape
    if n_time < 2:
        raise ValueError("Need at least 2 timepoints per trial to compute velocity.")

    # per-sample velocities (n_trials, n_time-1)
    vx = np.diff(x, axis=1) / dt
    vy = np.diff(y, axis=1) / dt
    sp = np.sqrt(vx * vx + vy * vy)

    # position associated with each velocity sample: midpoint of segment
    xm = 0.5 * (x[:, 1:] + x[:, :-1])
    ym = 0.5 * (y[:, 1:] + y[:, :-1])

    # flatten all samples
    xm = xm.ravel()
    ym = ym.ravel()
    vx = vx.ravel()
    vy = vy.ravel()
    sp = sp.ravel()

    # keep finite
    ok = np.isfinite(xm) & np.isfinite(ym) & np.isfinite(vx) & np.isfinite(vy) & np.isfinite(sp)
    xm, ym, vx, vy, sp = xm[ok], ym[ok], vx[ok], vy[ok], sp[ok]

    nx = len(xedges) - 1
    ny = len(yedges) - 1

    # bin indices
    ix = np.searchsorted(xedges, xm, side="right") - 1
    iy = np.searchsorted(yedges, ym, side="right") - 1
    inside = (ix >= 0) & (ix < nx) & (iy >= 0) & (iy < ny)

    ix, iy = ix[inside], iy[inside]
    vx, vy, sp = vx[inside], vy[inside], sp[inside]

    # accumulate sums and counts
    C = np.zeros((nx, ny), dtype=np.int64)
    sum_vx = np.zeros((nx, ny), dtype=float)
    sum_vy = np.zeros((nx, ny), dtype=float)
    sum_sp = np.zeros((nx, ny), dtype=float)

    np.add.at(C, (ix, iy), 1)
    np.add.at(sum_vx, (ix, iy), vx)
    np.add.at(sum_vy, (ix, iy), vy)
    np.add.at(sum_sp, (ix, iy), sp)

    # means
    with np.errstate(invalid="ignore", divide="ignore"):
        U = sum_vx / C
        V = sum_vy / C
        S = sum_sp / C  # <-- mean speed per bin (the "fixed" part)

    if nan_empty:
        mask = C >= min_count
        U = np.where(mask, U, np.nan)
        V = np.where(mask, V, np.nan)
        S = np.where(mask, S, np.nan)

    return U, V, S, C


def polar_from_binned_field(xedges, yedges, U, V, C=None, min_count=1, eps=1e-12, mask_r0=None):
    """
    Convert mean velocity components (U,V) defined at bin centers to polar components.
    Optionally mask low-count bins and optionally mask a central disk (mask_r0).
    """
    U = np.asarray(U); V = np.asarray(V)
    nx, ny = U.shape

    xc = 0.5 * (xedges[:-1] + xedges[1:])
    yc = 0.5 * (yedges[:-1] + yedges[1:])
    Xc, Yc = np.meshgrid(xc, yc, indexing="ij")

    R = np.sqrt(Xc**2 + Yc**2)
    TH = np.arctan2(Yc, Xc)
    R_safe = np.maximum(R, eps)

    Vr  = (Xc*U + Yc*V) / R_safe
    Vth = (-Yc*U + Xc*V) / R_safe
    Omega = (Xc*V - Yc*U) / (R_safe**2)

    mask = np.isfinite(U) & np.isfinite(V)
    if C is not None:
        mask &= (C >= min_count)
    if mask_r0 is not None:
        mask &= (R >= mask_r0)

    Vr    = np.where(mask, Vr, np.nan)
    Vth   = np.where(mask, Vth, np.nan)
    Omega = np.where(mask, Omega, np.nan)

    return Xc, Yc, R, TH, Vr, Vth, Omega


# -------------------------
# Example usage:
# U,V are mean velocity components; S is mean speed (recommended for "speed map")
# -------------------------
# U, V, S, C = binned_velocity_and_speed(x, y, dt, xedges, yedges, min_count=10)
# Xc, Yc, R, TH, Vr, Vth, Omega = polar_from_binned_field(xedges, yedges, U, V, C=C, min_count=10, mask_r0=1e-6)
# speed_of_mean_flow = np.sqrt(U**2 + V**2)   # different quantity than S
#+end_src

#+RESULTS:
: 53de1495-9eb8-4d46-acfd-0990fe5abe51

#+begin_src jupyter-python
import numpy as np

def polar_from_binned_field(xedges, yedges, U, V, C=None, min_count=1, eps=1e-12):
    """
    xedges, yedges: bin edges (nx+1), (ny+1)
    U, V: mean velocity per bin, shape (nx, ny)
    C: counts per bin, shape (nx, ny) (optional but recommended)
    Returns:
      Xc, Yc, R, TH (nx, ny)
      Vr, Vth, Omega (nx, ny) with NaNs where invalid/low count
    """
    U = np.asarray(U); V = np.asarray(V)
    nx, ny = U.shape

    xc = 0.5 * (xedges[:-1] + xedges[1:])
    yc = 0.5 * (yedges[:-1] + yedges[1:])
    Xc, Yc = np.meshgrid(xc, yc, indexing="ij")  # (nx, ny)

    R = np.sqrt(Xc**2 + Yc**2)
    TH = np.arctan2(Yc, Xc)
    R_safe = np.maximum(R, eps)

    Vr  = (Xc*U + Yc*V) / R_safe
    Vth = (-Yc*U + Xc*V) / R_safe
    Omega = (Xc*V - Yc*U) / (R_safe**2)

    # mask out empty/low-sample bins
    if C is not None:
        mask = (C >= min_count) & np.isfinite(U) & np.isfinite(V)
        Vr    = np.where(mask, Vr, np.nan)
        Vth   = np.where(mask, Vth, np.nan)
        Omega = np.where(mask, Omega, np.nan)

    return Xc, Yc, R, TH, Vr, Vth, Omega
#+end_src

#+RESULTS:
: 6b6dc593-4053-44c6-b027-56d9304b0477

#+begin_src jupyter-python
min_w = 1
U, V, S, C = binned_velocity_and_speed(x_coor, y_coor, dt, xedges, yedges, min_count=1)
Xc, Yc, R, TH, Vr, Vth, Om = polar_from_binned_field(xedges, yedges, U, V, C=C, min_count=1)

# magnitudes per bin
aVr  = np.abs(Vr)
aVth = np.abs(Vth)
aOm  = np.abs(Om)

# vs radius (weighted by C)
r_edges = np.linspace(0, np.nanmax(R), nbins)
r_c, aVr_r,  _, _ = weighted_binned_mean(R,  aVr,  r_edges, weights=C, min_count=min_count, min_weight_1d=min_w)
_,   aVth_r, _, _ = weighted_binned_mean(R,  aVth, r_edges, weights=C, min_count=min_count, min_weight_1d=min_w)
_,   aOm_r,  _, _ = weighted_binned_mean(R,  aOm,  r_edges, weights=C, min_count=min_count, min_weight_1d=min_w)

th_edges = np.linspace(-np.pi, np.pi, nbins)
th_c, aVr_th,  _, _ = weighted_binned_mean(TH, aVr,  th_edges, weights=C, min_count=min_count, min_weight_1d=min_w)
_,    aVth_th, _, _ = weighted_binned_mean(TH, aVth, th_edges, weights=C, min_count=min_count, min_weight_1d=min_w)
_,    aOm_th,  _, _ = weighted_binned_mean(TH, aOm,  th_edges, weights=C, min_count=min_count, min_weight_1d=min_w)
#+end_src

#+RESULTS:
: 7ce54e53-c17e-4bf9-ad94-b2ecbc12da01

#+begin_src jupyter-python
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(2*width, height),)

ax[0].plot(r_c, aVr_r,  label=r'$\langle|v_r|\rangle(r)$')
ax[0].plot(r_c, aVth_r, label=r'$\langle|v_\theta|\rangle(r)$')
ax[0].axhline(0, color='k', ls='--')
ax[0].set_xlabel("radius r")
ax[0].set_ylabel("$<v_r>, <v_\\theta>$")


ax[1].plot(th_c * 180 / np.pi, aVr_th,  label=r'$\langle|v_r|\rangle(\theta)$')
ax[1].plot(th_c * 180 / np.pi, aVth_th, label=r'$\langle|v_\theta|\rangle(\theta)$')
ax[1].axhline(0, color='k', ls='--')
ax[1].set_xlabel("angle θ (°)")
ax[1].set_ylabel("$<v_r>, <v_\\theta>$")

plt.show()
#+end_src

#+RESULTS:
: d3d9e558-23f2-47fb-93cc-9736f1966aa9

#+begin_src jupyter-python
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(2*width, height),)

ax[0].plot(r_c, aOm_r,  label=r'$\langle|v_r|\rangle(r)$')
ax[0].axhline(0, color='k', ls='--')
ax[0].set_xlabel("radius r")
ax[0].set_ylabel("$<\\omega>$")

ax[1].plot(th_c * 180 / np.pi, aOm_th,  label=r'$\langle|v_r|\rangle(\theta)$')
ax[1].axhline(0, color='k', ls='--')
ax[1].set_xlabel("angle θ (°)")
ax[1].set_ylabel("$<\\omega>$")

plt.show()
#+end_src

#+RESULTS:
: 6432b10a-6637-4ee9-a429-1b7616829027

#+begin_src jupyter-python

#+end_src

#+RESULTS:
: 9693dace-8972-44bc-b265-a448a75b6c3d
