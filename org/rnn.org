#+TITLE: Data driven RNN
#+STARTUP: fold
#+PROPERTY: header-args:ipython :results both :exports both :async yes :session my_session :kernel torch

* Notebook Settings

#+begin_src ipython
  %load_ext autoreload
  %autoreload 2
  %reload_ext autoreload
  
  %run /home/leon/dual_task/dual_data/notebooks/setup.py
  %matplotlib inline
  %config InlineBackend.figure_format = 'png'
#+end_src

#+RESULTS:
: The autoreload extension is already loaded. To reload it, use:
:   %reload_ext autoreload
: Python exe
: /home/leon/mambaforge/envs/torch/bin/python

* Imports

#+begin_src ipython
  import torch
  import torch.nn as nn
  import torch.optim as optim
  from torch.utils.data import Dataset, TensorDataset, DataLoader
#+end_src

#+RESULTS:

* Utils
** Sliding Window

#+begin_src ipython
  class SlidingWindowDataset(Dataset):
      def __init__(self, data, sequence_length=100, stride=1):
          self.data = data
          self.sequence_length = sequence_length
          self.stride = stride
          # Calculate number of samples once to optimize __len__
          self.num_sessions, self.num_time_points, _ = self.data.size()
          self.num_samples_per_session = (self.num_time_points - self.sequence_length) // self.stride
          self.total_samples = self.num_samples_per_session * self.num_sessions

      def __len__(self):
          return self.total_samples

      def __getitem__(self, idx):
          # Determine which session this idx belongs to
          session_idx = idx // self.num_samples_per_session
          # Determine the start of the slice for this idx
          session_start = idx % self.num_samples_per_session
          time_idx = session_start * self.stride

          # Extract sequences using calculated indices
          input_sequence = self.data[session_idx, time_idx:time_idx + self.sequence_length]
          target_sequence = self.data[session_idx, time_idx + self.sequence_length]

          return input_sequence, target_sequence
#+end_src

#+RESULTS:

** Data Split

#+begin_src ipython
  def split_data(X, Y, train_perc=0.8, batch_size=32):

    # Split the dataset into training and validation sets
    train_size = int(train_perc * X.shape[0])

    X_train = X[:train_size]
    X_test = X[train_size:]
    
    # X_train, X_mean, X_std = standard_scaler(X_train, IF_RETURN=1)
    # X_test = (X_test - X_mean) / X_std

    Y_train = Y[:train_size]    
    Y_test = Y[train_size:]

    # Y_train, Y_mean, Y_std = standard_scaler(Y_train, IF_RETURN=1)
    # Y_test = (Y_test - Y_mean) / Y_std

    # Create data sets
    # train_dataset = TensorDataset(X_train_scaled, Y_train_scaled)
    # val_dataset = TensorDataset(X_test_scaled, Y_test_scaled)

    print(X_train.shape, Y_train.shape)
    train_dataset = TensorDataset(X_train, Y_train)

    print(X_test.shape, Y_test.shape)
    val_dataset = TensorDataset(X_test, Y_test)

    # Create data loaders
    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)

    # sequence_length = 14  # or any other sequence length you want
    # stride = 1  # or any other stride you want

    # sliding_window_dataset = SlidingWindowDataset(X, sequence_length, stride)
    # train_loader = torch.utils.data.DataLoader(sliding_window_dataset, batch_size=5, shuffle=True)
    # val_loader = torch.utils.data.DataLoader(sliding_window_dataset, batch_size=5, shuffle=True)

    return train_loader, val_loader
#+end_src

#+RESULTS:

** Optimization

#+begin_src ipython
  def train(dataloader, model, loss_fn, optimizer):
      size = len(dataloader.dataset)
      device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

      model.train()
      for batch, (X, y) in enumerate(dataloader):

          X, y = X.to(device), y.to(device)

          # Compute prediction error
          pred = model(X)
          loss = loss_fn(pred, y)

          # Backpropagation
          loss.backward()
          optimizer.step()
          optimizer.zero_grad()

      return loss
#+end_src


#+RESULTS:

#+begin_src ipython
  def test(dataloader, model, loss_fn):
      size = len(dataloader.dataset)
      num_batches = len(dataloader)

      device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

      # Validation loop.
      model.eval()
      val_loss = 0.0
      with torch.no_grad():
          for data, targets in dataloader:
              data, targets = data.to(device), targets.to(device)

              outputs = model(data)
              loss = loss_fn(outputs, targets)
              val_loss += loss.item() * data.size(0)
          val_loss /= size

      return val_loss
#+end_src

#+RESULTS:

#+begin_src ipython
  def run_optim(model, train_loader, val_loader, loss_fn, optimizer, num_epochs=100):

    # scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.1, verbose=True)
    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)

    # Training loop.
    for epoch in range(num_epochs):
        loss = train(train_loader, model, loss_fn, optimizer)
        val_loss = test(val_loader, model, loss_fn)
        scheduler.step(val_loss)

        if epoch % int(num_epochs  / 10) == 0:
            print(f'Epoch {epoch}/{num_epochs}, Training Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}')

#+end_src

#+RESULTS:


** Prediction


#+begin_src ipython
  def get_predictions(model, future_steps, device='cuda:1'):
      model.eval()  # Set the model to evaluation mode

      # Start with an initial seed sequence 
      input_size = model.input_size
      hidden_size = model.hidden_size

      seed_sequence = torch.randn(1, future_steps, input_size).to(device)  # Replace with your actual seed

      # Collect predictions
      predictions = []

      # Initialize the hidden state (optional, depends on your model architecture)
      hidden = torch.zeros(model.num_layers, 1, hidden_size).to(device)
      # hidden = torch.randn(model.num_layers, 1, hidden_size, device=device) * 0.01
      
      # Generate time series
      for _ in range(future_steps):
          # Forward pass
          with torch.no_grad():  # No need to track gradients
              # out, hidden = model.rnn(seed_sequence, hidden)
              out = model(hidden)
              next_step = out[:, -1, :]  # Output for the last time step

          predictions.append(next_step.cpu().numpy())

          # Use the predicted next step as the input for the next iteration
          next_step = next_step.unsqueeze(1)  # Add the sequence length dimension
          seed_sequence = torch.cat((seed_sequence[:, 1:, :], next_step), 1)  # Move the window

      # # Convert predictions to a numpy array for further analysis
      predicted_time_series = np.concatenate(predictions, axis=0)

      return predicted_time_series

#+end_src

#+RESULTS:

** Pipeline

#+begin_src ipython
  def standard_scaler(data, IF_RETURN=0):
      mean = data.mean(dim=0, keepdim=True)
      std = data.std(dim=0, keepdim=True)
      if IF_RETURN:
          return (data - mean) / std, mean, std
      else:
          return (data - mean) / std

#+end_src

#+RESULTS:

#+begin_src ipython

  from torch.utils.data import DataLoader
  from torchvision import transforms

  # Assuming 'MyDataset' is a Dataset object you've made for your data
  class MyPipeline:
      def __init__(self, model, preprocessing=None):
          self.model = model
          self.preprocessing = preprocessing

      def __call__(self, x):
          if self.preprocessing:
              x = self.preprocessing(x)
          return self.model(x)

  # Define the transformations (preprocessing)
  preprocessing = transforms.Compose([
      transforms.ToTensor(),
      # standard_scaler()
  ])

  # Create the pipeline
  model = MultivariateRNN()  # Replace with your actual model
  pipeline = MyPipeline(model, preprocessing)

  # Now you can use your pipeline to process and feed data into your model
  dataset = MyDataset()
  dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

  # Use the pipeline in your training loop
  for inputs, targets in dataloader:
      predictions = pipeline(inputs)
      loss = loss_func(predictions, targets)
      # ... rest of your training loop
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
#+begin_example
  ---------------------------------------------------------------------------
  NameError                                 Traceback (most recent call last)
  Cell In[10], line 22
       16 preprocessing = transforms.Compose([
       17     transforms.ToTensor(),
       18     # standard_scaler()
       19 ])
       21 # Create the pipeline
  ---> 22 model = MultivariateRNN()  # Replace with your actual model
       23 pipeline = MyPipeline(model, preprocessing)
       25 # Now you can use your pipeline to process and feed data into your model

  NameError: name 'MultivariateRNN' is not defined
#+end_example
:END:

** Synthetic Data

#+begin_src ipython
  def generate_multivariate_time_series(num_series, num_steps, num_features, device='cuda'):
      np.random.seed(42)  # For reproducibility

      # Generate random frequencies and phases for the sine waves
      frequencies = np.random.uniform(low=0.1, high=2.0, size=(num_features))
      phases = np.random.uniform(low=0, high=2*np.pi, size=(num_features))
      noise = np.random.uniform(low=0, high=1, size=(num_series))

      # Generate time steps for the sine waves
      time_steps = np.linspace(0, num_steps, num_steps)

      # Initialize the data array
      data = np.zeros((num_series, num_steps, num_features))

      # Populate the data array with sine waves
      for i in range(num_series):
          for j in range(num_steps):
              for k in range(num_features):
                  data[i, j, k] = np.sin(2 * np.pi * j / num_steps - phases[k]) + np.random.uniform()

      # Return as torch.FloatTensor
      return torch.FloatTensor(data).to(device)

#+end_src

#+RESULTS:

** Loss

#+begin_src ipython
  class CustomBCELoss(nn.Module):
      def __init__(self):
          super(CustomBCELoss, self).__init__()

      def forward(self, inputs, targets):
          inputs = torch.cat(inputs, dim=1)
          y_pred = self.linear(inputs[:, -1, :])

          proba = torch.sigmoid(y_pred).squeeze(-1)

          loss = F.binary_cross_entropy(proba, targets, reduction='none')
          
          return loss.mean()  # Or .sum(), or custom reduction as needed.
#+end_src

#+RESULTS:

* RNN models
** Vanilla

#+begin_src ipython
  # Define the RNN model
  class VanillaRNN(nn.Module):
      def __init__(self, input_size, hidden_size, num_layers, output_size, device):
          super(VanillaRNN, self).__init__()
          self.hidden_size = hidden_size
          self.num_layers = num_layers
          self.device=device
          # You can swap nn.RNN with nn.LSTM or nn.GRU depending on your requirements

          self.rnn = nn.RNN(input_size, hidden_size, num_layers,
                            batch_first=True, nonlinearity='relu', device=self.device)

          self.fc = nn.Linear(hidden_size, output_size, device=self.device)

          DT = 0.1
          TAU = 20

          self.DT_TAU = DT/TAU
          self.EXP_DT_TAU = np.exp(-DT/TAU)

      def forward(self, input):
          # Initial hidden state (can also initialize this outside and pass it as a parameter)
          rates = torch.zeros(self.num_layers, input.size(1), self.hidden_size, device=self.device)
          h = torch.zeros(self.num_layers, input.size(1), self.hidden_size, device=self.device)
          
          # Forward propagate the RNN
          h, _ = self.rnn(input, rates)
          rates = self.EXP_DT_TAU * rates + self.DT_TAU * h
          output = self.fc(rates)

          return output
#+end_src

#+RESULTS:

** Classifier

#+begin_src ipython
  class CustomCombinedLoss(nn.Module):
      def __init__(self, weight1=1.0, weight2=1.0):
          super(CustomCombinedLoss, self).__init__()
          self.weight1 = weight1
          self.weight2 = weight2
          # You could also include additional initializations for 
          # each distinct condition if necessary.

      def forward(self, inputs, targets):
          # Condition 1 (for example, Mean Squared Error)
          loss1 = torch.mean((inputs - targets[0]) ** 2)
          
          # Condition 2 (for example, Mean Absolute Error)
          loss2 = torch.mean(torch.abs(inputs - targets[1]))

          # Combine the two conditions
          loss = self.weight1 * loss1 + self.weight2 * loss2
          return loss
#+end_src

#+BEGIN_SRC ipython
  import torch
  import torch.nn as nn
  import torch.nn.functional as F

  class ClassifierRNN(nn.Module):
      def __init__(self, input_size, hidden_size, num_layers, output_size, device='cuda', dt=.01, noise=0.01, rank=2, alpha=0.0):
          super(ClassifierRNN, self).__init__()

          self.input_size = input_size
          self.hidden_size = hidden_size
          self.output_size = output_size

          self.num_layers = num_layers
          self.alpha = alpha
          # Weight matrices
          self.W_ih = nn.Parameter(torch.Tensor(hidden_size, input_size))
          self.W_hh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))
          # Bias terms
          self.b_ih = nn.Parameter(torch.Tensor(hidden_size))
          self.b_hh = nn.Parameter(torch.Tensor(hidden_size))
          # Low rank
          self.xi = nn.Parameter(torch.Tensor(hidden_size, rank))

          self.noise_std = torch.tensor(noise)

          self.linear = nn.Linear(hidden_size, 1)
          # Decay rate for the hidden state

          with torch.no_grad():
              self.decay_rate = torch.tensor(dt / 20)
              self.exp_rate = torch.exp(-self.decay_rate)

              self.decay_syn = torch.tensor(dt / 3)
              self.exp_syn = torch.exp(-self.decay_syn)

          # Initialize parameters
          self.reset_parameters()

      def reset_parameters(self):
          # Initialize weight and bias parameters using xavier initialization or another preferred method
          nn.init.xavier_uniform_(self.W_ih)
          nn.init.xavier_uniform_(self.W_hh)
          nn.init.zeros_(self.b_ih)
          nn.init.zeros_(self.b_hh)
          nn.init.normal_(self.xi)

      def forward(self, x):
          # x is of shape (batch_size, sequence_length, input_size)
          batch_size, seq_length, _ = x.size()

          rec_inputs = torch.zeros(batch_size, self.hidden_size, device=x.device)
          rates = torch.zeros(batch_size, self.hidden_size, device=x.device)
          noise = torch.randn(batch_size, seq_length, self.hidden_size, device=x.device)

          outputs = []
          for t in range(seq_length):

              ff_inputs = torch.mm(x[:, t], self.W_ih.t()) + self.b_ih

              full = torch.mm(rates, self.W_hh.t()) + self.b_hh
              lr = rates.matmul(self.xi).matmul(self.xi.t()) / self.hidden_size
              
              hidden = (1.0 - self.alpha) * full + self.alpha * lr
              rec_inputs = self.exp_syn * rec_inputs + self.decay_syn * hidden
              # rec_inputs = full + lr
              net_inputs = ff_inputs + rec_inputs + noise[:, t, :] * self.noise_std

              # Compute rates
              rates = nn.ReLU()(net_inputs)
              # rates = rates * self.exp_rate + self.decay_rate * nn.ReLU()(net_inputs)

              # Collect outputs
              outputs.append(rates.unsqueeze(1))

          # Concatenate outputs along the time dimension
          outputs = torch.cat(outputs, dim=1)
          y_pred = self.linear(outputs[:, -1, :])

          return torch.sigmoid(y_pred).squeeze(-1)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython
  # Example usage:
  input_size = 10
  hidden_size = 20
  num_layers = 1
  output_size = 10

  decay_rate = 0.1

  seq_length = 5
  batch_size = 3

  model = ClassifierRNN(input_size, hidden_size, num_layers, output_size, decay_rate)
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model.to(device)

  input_tensor = torch.randn(batch_size, seq_length, input_size)
  input_tensor = input_tensor.to(device)
  
  output = model(input_tensor)
  print(input_tensor.shape, output.shape)
#+END_SRC

#+RESULTS:
: torch.Size([3, 5, 10]) torch.Size([3])

** Rate

#+begin_src ipython
  class CustomCombinedLoss(nn.Module):
      def __init__(self, weight1=1.0, weight2=1.0):
          super(CustomCombinedLoss, self).__init__()
          self.weight1 = weight1
          self.weight2 = weight2
          # You could also include additional initializations for 
          # each distinct condition if necessary.

      def forward(self, inputs, targets):
          # Condition 1 (for example, Mean Squared Error)
          loss1 = torch.mean((inputs - targets[0]) ** 2)
          
          # Condition 2 (for example, Mean Absolute Error)
          loss2 = torch.mean(torch.abs(inputs - targets[1]))

          # Combine the two conditions
          loss = self.weight1 * loss1 + self.weight2 * loss2
          return loss
#+end_src

#+begin_src ipython
  covariance = torch.tensor([[1.0, self.LR_COV],
                             [self.LR_COV, 1.0],], dtype=self.FLOAT, device=self.device)


  multivariate_normal = MultivariateNormal(mean, covariance)
  self.ksi = multivariate_normal.sample((Nb,)).T

#+end_src

#+begin_src ipython
  ff_input = self.Ja0[0] * (1.0 + self.ksi[0] * self.I0[0] * self.M0)
#+end_src


#+BEGIN_SRC ipython
  import torch
  import torch.nn as nn
  import torch.nn.functional as F

  class ClassifierRNN(nn.Module):
      def __init__(self, input_size, hidden_size, num_layers, output_size, device='cuda', dt=.01, noise=0.01, rank=2, alpha=0.0):
          super(ClassifierRNN, self).__init__()

          self.input_size = input_size
          self.hidden_size = hidden_size
          self.output_size = output_size

          self.num_layers = num_layers
          self.alpha = alpha
          # Weight matrices
          self.W_ih = nn.Parameter(torch.Tensor(hidden_size, input_size))
          self.W_hh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))
          # Bias terms
          self.b_ih = nn.Parameter(torch.Tensor(hidden_size))
          self.b_hh = nn.Parameter(torch.Tensor(hidden_size))
          # Low rank
          self.xi = nn.Parameter(torch.Tensor(hidden_size, rank))

          self.noise_std = torch.tensor(noise)

          self.linear = nn.Linear(hidden_size, 1)
          # Decay rate for the hidden state

          with torch.no_grad():
              self.decay_rate = torch.tensor(dt / 20)
              self.exp_rate = torch.exp(-self.decay_rate)

              self.decay_syn = torch.tensor(dt / 3)
              self.exp_syn = torch.exp(-self.decay_syn)

          # Initialize parameters
          self.reset_parameters()

      def reset_parameters(self):
          # Initialize weight and bias parameters using xavier initialization or another preferred method
          nn.init.xavier_uniform_(self.W_ih)
          nn.init.xavier_uniform_(self.W_hh)
          nn.init.zeros_(self.b_ih)
          nn.init.zeros_(self.b_hh)
          nn.init.normal_(self.xi)

      def forward(self, x):
          # x is of shape (batch_size, sequence_length, input_size)
          batch_size, seq_length, _ = x.size()

          rec_inputs = torch.zeros(batch_size, self.hidden_size, device=x.device)
          rates = torch.zeros(batch_size, self.hidden_size, device=x.device)
          noise = torch.randn(batch_size, seq_length, self.hidden_size, device=x.device)

          outputs = []
          for t in range(seq_length):

              ff_inputs = torch.mm(x[:, t], self.W_ih.t()) + self.b_ih

              full = torch.mm(rates, self.W_hh.t()) + self.b_hh
              lr = rates.matmul(self.xi).matmul(self.xi.t()) / self.hidden_size
              
              hidden = (1.0 - self.alpha) * full + self.alpha * lr
              rec_inputs = self.exp_syn * rec_inputs + self.decay_syn * hidden
              # rec_inputs = full + lr
              net_inputs = ff_inputs + rec_inputs + noise[:, t, :] * self.noise_std

              # Compute rates
              rates = nn.ReLU()(net_inputs)
              # rates = rates * self.exp_rate + self.decay_rate * nn.ReLU()(net_inputs)

              # Collect outputs
              outputs.append(rates.unsqueeze(1))

          # Concatenate outputs along the time dimension
          outputs = torch.cat(outputs, dim=1)
          y_pred = self.linear(outputs[:, -1, :])

          return torch.sigmoid(y_pred).squeeze(-1)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython
  # Example usage:
  input_size = 10
  hidden_size = 20
  num_layers = 1
  output_size = 10

  decay_rate = 0.1

  seq_length = 5
  batch_size = 3

  model = ClassifierRNN(input_size, hidden_size, num_layers, output_size, decay_rate)
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model.to(device)

  input_tensor = torch.randn(batch_size, seq_length, input_size)
  input_tensor = input_tensor.to(device)
  
  output = model(input_tensor)
  print(input_tensor.shape, output.shape)
#+END_SRC

#+RESULTS:
: torch.Size([3, 5, 10]) torch.Size([3])

** Multivariate


#+BEGIN_SRC ipython
  import torch
  import torch.nn as nn
  import torch.nn.functional as F

  class MultivariateRNN(nn.Module):
      def __init__(self, input_size, hidden_size, num_layers, output_size, device='cuda', dt=.01, noise=0.01, rank=2, alpha=0.5):
          super(MultivariateRNN, self).__init__()

          self.input_size = input_size
          self.hidden_size = hidden_size
          self.output_size = output_size
          self.alpha = alpha
          self.num_layers = num_layers

          # Weight matrices
          self.W_ih = nn.Parameter(torch.Tensor(hidden_size, input_size))
          self.W_hh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))
          # Bias terms
          self.b_ih = nn.Parameter(torch.Tensor(hidden_size))
          self.b_hh = nn.Parameter(torch.Tensor(hidden_size))
          self.xi = nn.Parameter(torch.Tensor(hidden_size, rank))

          self.noise_std = torch.tensor(noise)

          self.fc = nn.Linear(hidden_size, output_size, bias=False)
          # Decay rate for the hidden state

          with torch.no_grad():          
              self.decay_rate = torch.tensor(dt / 20)
              self.exp_rate = torch.exp(-self.decay_rate)

              self.decay_syn = torch.tensor(dt / 3)
              self.exp_syn = torch.exp(-self.decay_syn)

          # Initialize parameters
          self.reset_parameters()

      def reset_parameters(self):
          # Initialize weight and bias parameters using xavier initialization or another preferred method
          nn.init.xavier_uniform_(self.W_ih)
          nn.init.xavier_uniform_(self.W_hh)
          nn.init.zeros_(self.b_ih)
          nn.init.zeros_(self.b_hh)
          nn.init.normal_(self.xi)

      def forward(self, x):
          # x is of shape (batch_size, sequence_length, input_size)
          batch_size, seq_length, _ = x.size()

          rec_inputs = torch.zeros(batch_size, self.W_hh.size(0), device=x.device)
          rates = torch.zeros(batch_size, self.W_hh.size(0), device=x.device)
          noise = torch.randn(batch_size, seq_length, self.W_hh.size(0), device=x.device)

          outputs = []
          for t in range(seq_length):
              
              ff_inputs = torch.mm(x[:, t], self.W_ih.t()) + self.b_ih
              hidden = torch.mm(rates, self.W_hh.t()) + self.b_hh

              full = torch.mm(rates, self.W_hh.t()) + self.b_hh
              lr = rates.matmul(self.xi).matmul(self.xi.t()) / self.hidden_size

              hidden = (1.0 - self.alpha) * full + self.alpha * lr

              rec_inputs = self.exp_syn * rec_inputs + self.decay_syn * hidden
              # rec_inputs = hidden

              net_inputs = ff_inputs + rec_inputs + noise[:, t, :] * self.noise_std

              # Compute rates
              rates = nn.ReLU()(net_inputs)
              # rates = rates * self.exp_rate + self.decay_rate * nn.ReLU()(net_inputs)

              # Collect outputs
              outputs.append(self.fc(rates.unsqueeze(1)))

          # Concatenate outputs along the time dimension
          outputs = torch.cat(outputs, dim=1)

          return outputs
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython
  # Example usage:
  input_size = 10
  hidden_size = 20
  num_layers = 1
  output_size = 10

  decay_rate = 0.1

  seq_length = 5
  batch_size = 3
  
  model = MultivariateRNN(input_size, hidden_size, num_layers, output_size, decay_rate)
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model.to(device)

  input_tensor = torch.randn(batch_size, seq_length, input_size)
  input_tensor = input_tensor.to(device)

  output = model(input_tensor)
  print(input_tensor.shape, output.shape)
#+END_SRC

#+RESULTS:
: torch.Size([3, 5, 10]) torch.Size([3, 5, 10])

* Train on synthetic data
*** Create synthetic data

#+begin_src ipython
  num_series = 32  # Number of time series samples to generate
  num_steps = 84  # Number of time steps in each time series
  num_features = 100  # Number of features (signals) in each time series
  
  # Generate synthetic data
  synthetic_data = generate_multivariate_time_series(num_series, num_steps, num_features)

  # Split the data into inputs (X) and targets (Y), e.g., use previous timesteps to predict the next timestep
  X = synthetic_data[:, :-1, :]  # Using all but the last timestep as input
  Y = synthetic_data[:, 1:, :]   # Using all but the first timestep as target (shifted by one)

  print("Input shape:", X.shape)
  print("Target shape:", Y.shape)

#+end_src

#+RESULTS:
: Input shape: torch.Size([32, 83, 100])
: Target shape: torch.Size([32, 83, 100])

#+begin_src ipython
  plt.plot(np.arange(0, num_steps, 180), np.sin(num_steps))
  plt.plot(X.cpu().numpy()[0,:,2], alpha=1)
  plt.plot(X.cpu().numpy()[3,:,0], alpha=1, color='r')
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/3b18165ce9cd91277e97f3d4353d14ebbde9a524.png]]

*** Train model

#+begin_src ipython

  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

  hidden_size = 1000
  num_layers = 1
  model = MultivariateRNN(input_size=num_features, hidden_size=hidden_size,
                          num_layers=num_layers, output_size=num_features, device=device)

  batch_size = 8
  train_loader, val_loader = split_data(X, Y, train_perc=0.8, batch_size=batch_size)

  learning_rate = 0.001
  criterion = nn.MSELoss()
  optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
  
  num_epochs = 100
  run_optim(model, train_loader, val_loader, criterion, optimizer, num_epochs)  
#+end_src

#+RESULTS:
#+begin_example
  torch.Size([25, 83, 100]) torch.Size([25, 83, 100])
  torch.Size([7, 83, 100]) torch.Size([7, 83, 100])
  Epoch 0/100, Training Loss: 0.2809, Validation Loss: 0.1823
  Epoch 10/100, Training Loss: 0.0889, Validation Loss: 0.0885
  Epoch 20/100, Training Loss: 0.0857, Validation Loss: 0.0875
  Epoch 30/100, Training Loss: 0.0847, Validation Loss: 0.0875
  Epoch 40/100, Training Loss: 0.0800, Validation Loss: 0.0875
  Epoch 00043: reducing learning rate of group 0 to 1.0000e-04.
  Epoch 50/100, Training Loss: 0.0814, Validation Loss: 0.0872
  Epoch 00058: reducing learning rate of group 0 to 1.0000e-05.
  Epoch 60/100, Training Loss: 0.0818, Validation Loss: 0.0872
  Epoch 70/100, Training Loss: 0.0817, Validation Loss: 0.0872
  Epoch 80/100, Training Loss: 0.0820, Validation Loss: 0.0872
  Epoch 00082: reducing learning rate of group 0 to 1.0000e-06.
  Epoch 90/100, Training Loss: 0.0825, Validation Loss: 0.0872
  Epoch 00093: reducing learning rate of group 0 to 1.0000e-07.
#+end_example

#+RESULTS:

*** See data

#+begin_src ipython
  from sklearn.metrics import mean_squared_error

  model.eval()  # Set the model to evaluation mode

  # This function feeds inputs through the model and computes the predictions
  def get_predictions(data_loader):
      predictions = []
      ground_truth = []
      with torch.no_grad():  # Disable gradient computation for evaluation
          for inputs, targets in data_loader:
              inputs, targets = inputs.to(device), targets.to(device)
              outputs = model(inputs)
              predictions.append(outputs.cpu()) # If using cuda, need to move data to cpu
              ground_truth.append(targets.cpu())

      # Concatenate all batches
      predictions = torch.cat(predictions, dim=0)
      ground_truth = torch.cat(ground_truth, dim=0)

      return predictions, ground_truth

  # Call the function using your data loader
  predictions, ground_truth = get_predictions(val_loader)

  print(ground_truth.numpy().shape, predictions.numpy().shape)
  # Calculate the loss or performance metric
  # For example, we can use the Mean Squared Error
  # error = mean_squared_error(ground_truth.numpy(), predictions.numpy())
  # print(f"Mean Squared Error: {error}")
#+end_src

#+RESULTS:
: (7, 83, 100) (7, 83, 100)

#+begin_src ipython
  import matplotlib.pyplot as plt

  # Convert tensors to numpy arrays for plotting
  predictions_np = predictions.numpy()
  ground_truth_np = ground_truth.numpy()
  
  # Plot the predictions on top of the ground truth
  plt.figure()
  pal = sns.color_palette("tab10")

  # Example for plotting the first feature dimension
  for i in range(2):
     plt.plot(ground_truth_np[0, :, i], label='Ground Truth', marker='.', color=pal[i])
     plt.plot(predictions_np[0, :, i], label='Model Prediction', marker='x', color=pal[i])

  plt.title("Model Prediction vs Ground Truth", fontsize=14)
  plt.xlabel("Time steps")
  plt.ylabel("Value")
  # plt.legend(fontsize=12)
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/d1a83e29b24b28332a20c96cc5c0b3cb4954b379.png]]

* Train on Experimental Data
** Imports

#+begin_src ipython
  import sys
  sys.path.insert(0, '../')
  
  from src.common.get_data import get_X_y_days, get_X_y_S1_S2
  from src.common.options import set_options
#+end_src

#+RESULTS:

** Parameters

#+begin_src ipython
  mice = ['ChRM04','JawsM15', 'JawsM18', 'ACCM03', 'ACCM04']
  tasks = ['DPA', 'DualGo', 'DualNoGo']
  days = ['first', 'last']

  kwargs = dict()
  kwargs = {'prescreen': None, 'pval': 0.05, 'trials': '', 'balance': 'under',
            'method': 'bootstrap', 'bolasso_pval':0.05, 'bolasso_penalty': 'l2',
            'bootstrap': True, 'n_boots': 1000,
            'preprocess': True, 'scaler_BL': None, 'avg_noise':True, 'unit_var_BL':False,
            'clf':'log_loss', 'scaler': None, 'tol':0.001, 'penalty':'l2',
            'out_fold': 'stratified', 'n_out': 5,
            'in_fold': 'stratified', 'n_in': 5,
            'random_state': None, 'n_repeats': 10,
            'n_lambda': 20, 'T_WINDOW': 0.5,
            }

  kwargs['mouse'] = 'JawsM15'
#+end_src

#+RESULTS:

** Load Data

#+begin_src ipython
  options = set_options(**kwargs)
  options['reload'] = True
  options['data_type'] = 'raw'
  options['DCVL'] = 1
#+end_src

#+RESULTS:

#+begin_src ipython  
  # X_days, y_days = get_X_y_days(**options)
  # X_data, y_data = get_X_y_S1_S2(X_days, y_days, **options)

  # y_data = y_data[:, np.newaxis]
  # print(X_data.shape, y_data.shape)
#+end_src

#+RESULTS:

#+begin_src ipython
  import pickle as pkl

  filename = "../data/" + kwargs['mouse'] + "/X_dcvl.pkl"
  # pkl.dump(X_data, open(filename + ".pkl", "wb"))

  filename = "../data/" + kwargs['mouse'] + "/y_dcvl_.pkl"
  # pkl.dump(y_data, open(filename + ".pkl", "wb"))
#+end_src

#+RESULTS:

#+begin_src ipython
  import pickle as pkl
  filename = "../data/" + kwargs['mouse'] + "/X_dcvl.pkl"
  X_data = pkl.load(open(filename + ".pkl", "rb"))

  filename = "../data/" + kwargs['mouse'] + "/y_dcvl_.pkl"
  y_data = pkl.load(open(filename + ".pkl", "rb"))
#+end_src

#+RESULTS:

#+begin_src ipython
  import numpy as np
  from scipy.ndimage import convolve1d
  
  def moving_average_multidim(data, window_size, axis=-1):
      """
      Apply a 1D moving average across a specified axis of a multi-dimensional array.

      :param data: multi-dimensional array of data
      :param window_size: size of the moving window 
      :param axis: axis along which to apply the moving average
      :return: smoothed data with the same shape as input data
      """
      # Create a moving average filter window
      window = np.ones(window_size) / window_size
      # Apply 1D convolution along the specified axis
      smoothed_data = convolve1d(data, weights=window, axis=axis, mode='reflect')
      return smoothed_data

#+end_src

#+RESULTS:

#+begin_src ipython
  from src.decode.bump import circcvl
  # smoothed_data = circcvl(X_data, windowSize=2, axis=-1)
  print(X_data.shape)
  window_size = 6
  # from scipy.ndimage import gaussian_filter1d
  # smoothed_data = gaussian_filter1d(X_data, axis=-1, sigma=2)
  # smoothed_data = moving_average_multidim(X_data[..., :52], window_size, axis=-1)
  smoothed_data = moving_average_multidim(X_data, window_size, axis=-1)
#+end_src

#+RESULTS:
: (96, 693, 84)

#+begin_src ipython
  time = np.linspace(0, 14, 84)
  for i in range(10):
      i = np.random.randint(100)
      plt.plot(time, smoothed_data[0, i,:], alpha=.5)

  plt.ylabel('Rate (Hz)')
  plt.xlabel('Time (s)')
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/7f4539ef4bed2a80edcf910fae40892f08148ad8.png]]

** Training

#+begin_src ipython
  # y = np.roll(X_data, -1)
  # y = y[..., :-1]

  Y = smoothed_data[..., 1:]
  X = smoothed_data[..., :-1]
  
  X = np.swapaxes(X, 1, -1)
  Y = np.swapaxes(Y, 1, -1)

  print(X.shape, Y.shape)
#+end_src

#+RESULTS:
: (96, 83, 693) (96, 83, 693)

#+begin_src ipython
  X = torch.tensor(X, dtype=torch.float32, device=device)
  Y = torch.tensor(Y, dtype=torch.float32, device=device)
  print(X.shape, Y.shape)
#+end_src

#+RESULTS:
: torch.Size([96, 83, 693]) torch.Size([96, 83, 693])

#+RESULTS:

#+begin_src ipython
  y_data[y_data==-1] = 0
  Y = torch.tensor(y_data, dtype=torch.float32, device=device)
  print(Y.shape)
#+end_src

#+RESULTS:
: torch.Size([96])

#+begin_src ipython
  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

  hidden_size = 1024
  num_layers = 1
  num_features = 693
  
  batch_size = 8
  train_loader, val_loader = split_data(X, Y, train_perc=0.8, batch_size=batch_size)
#+end_src

#+RESULTS:
: torch.Size([76, 83, 693]) torch.Size([76, 83, 693])
: torch.Size([20, 83, 693]) torch.Size([20, 83, 693])

#+begin_src ipython
  learning_rate = 0.001
  criterion = nn.MSELoss()
  # criterion = nn.BCELoss() # Binary Cross-Entropy Loss

  optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.001)

  num_epochs = 100
  
  model = MultivariateRNN(input_size=num_features, hidden_size=hidden_size,
                        num_layers=num_layers, output_size=num_features, device=device, dt=.01, noise=0, alpha=1)

  run_optim(model, train_loader, val_loader, criterion, optimizer, num_epochs)
#+end_src

#+RESULTS:
#+begin_example
  Epoch 0/100, Training Loss: 0.0018, Validation Loss: 0.0022
  Epoch 10/100, Training Loss: 0.0026, Validation Loss: 0.0022
  Epoch 00012: reducing learning rate of group 0 to 1.0000e-04.
  Epoch 20/100, Training Loss: 0.0021, Validation Loss: 0.0022
  Epoch 00023: reducing learning rate of group 0 to 1.0000e-05.
  Epoch 30/100, Training Loss: 0.0022, Validation Loss: 0.0022
  Epoch 00034: reducing learning rate of group 0 to 1.0000e-06.
  Epoch 40/100, Training Loss: 0.0019, Validation Loss: 0.0022
  Epoch 00045: reducing learning rate of group 0 to 1.0000e-07.
  Epoch 50/100, Training Loss: 0.0025, Validation Loss: 0.0022
  Epoch 00056: reducing learning rate of group 0 to 1.0000e-08.
  Epoch 60/100, Training Loss: 0.0021, Validation Loss: 0.0022
  Epoch 70/100, Training Loss: 0.0019, Validation Loss: 0.0022
  Epoch 80/100, Training Loss: 0.0017, Validation Loss: 0.0022
  Epoch 90/100, Training Loss: 0.0019, Validation Loss: 0.0022
#+end_example

* Reverse Engineering
** Generate series

#+begin_src ipython
  from sklearn.metrics import mean_squared_error

  model.eval()  # Set the model to evaluation mode
  
  # This function feeds inputs through the model and computes the predictions
  def get_predictions(data_loader):
      predictions = []
      ground_truth = []
      with torch.no_grad():  # Disable gradient computation for evaluation
          for inputs, targets in data_loader:
              inputs, targets = inputs.to(device), targets.to(device)
              outputs = model(inputs)
              predictions.append(outputs.cpu())  # If using cuda, need to move data to cpu
              ground_truth.append(targets.cpu())

      # Concatenate all batches
      predictions = torch.cat(predictions, dim=0)
      ground_truth = torch.cat(ground_truth, dim=0)

      return predictions, ground_truth

  # Call the function using your data loader
  predictions, ground_truth = get_predictions(val_loader)

  print(ground_truth.numpy().shape, predictions.numpy().shape)
  # Calculate the loss or performance metric
  # For example, we can use the Mean Squared Error
  # error = mean_squared_error(ground_truth.numpy(), predictions.numpy())
  # print(f"Mean Squared Error: {error}")
#+end_src

#+RESULTS:
: (20, 83, 693) (20, 83, 693)

#+begin_src ipython
  import matplotlib.pyplot as plt
  
  # Assuming predictions and ground_truth are for a single batch or example:
  # predictions: tensor of shape (batch_size, sequence_length, output_size)
  # ground_truth: tensor of shape (batch_size, sequence_length, output_size)

  # Convert tensors to numpy arrays for plotting
  predictions_np = predictions.numpy()
  ground_truth_np = ground_truth.numpy()

  # Plot the predictions on top of the ground truth
  plt.figure()
  pal = sns.color_palette("tab10")
  time = np.linspace(0, 14, 84)[:-1]
  # Example for plotting the first feature dimension
  for i in range(3):
     j = np.random.randint(693)
     plt.plot(time, ground_truth_np[0, :, j], '-', label='Ground Truth', color=pal[i], alpha=.2)
     plt.plot(time, predictions_np[0, :, j], label='Model Prediction', marker='o', color=pal[i], alpha=1, lw=0)

  # You can loop through more feature dimensions if needed
  # for i in range(output_size):
  #     plt.plot(ground_truth_np[0, :, i], label=f'Ground Truth Feature {i}', marker='.')
  #     plt.plot(predictions_np[0, :, i], label=f'Prediction Feature {i}', marker='x')

  plt.title("Model Prediction vs Ground Truth")
  plt.xlabel("Time steps")
  plt.ylabel("Value")
  # plt.legend(fontsize=12)
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/c300ecd214cc68557e80464695d21404a2d435d8.png]]

** Connectivity

#+begin_src ipython
  # weights = model.rnn.weight_hh_l0.data.cpu().numpy()  # Get the recurring weights of the RNN
  weights = model.W_hh.cpu().detach().numpy()
  print(weights.shape)
  # Perform singular value decomposition<
  U, S, Vt = np.linalg.svd(weights, full_matrices=False)

  u1, u2, u3 = U[:, 0], U[:, 1], U[:, 2]  # First two left singular vectors
  v1, v2, v3 = Vt[0, :], Vt[1, :], Vt[2, :]  # First two right singular vectors
#+end_src

#+RESULTS:
: (1024, 1024)

#+begin_src ipython
  ksi1 = S[0] * u1 * v1
  ksi2 = S[1] * u2 * v2
  ksi3 = S[2] * u3 * v3
  print(ksi1.shape)
#+end_src

#+RESULTS:
: (1024,)

#+begin_src ipython
  plt.imshow(weights, cmap='jet')
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/11578db95a1a6b58bc8dec464e5a0f44c398e91e.png]]

#+begin_src ipython
print(S[:10])
#+end_src

#+RESULTS:
: [1.9814498 1.9744787 1.9646422 1.9554437 1.9534712 1.9489214 1.9417928
:  1.9313139 1.9252403 1.9179983]

#+begin_src ipython
  theta = np.arctan2(ksi2, ksi1)
  index = theta.argsort()
  print(index.shape)
#+end_src

#+RESULTS:
: (1024,)

#+begin_src ipython
  plt.hist(theta*180/np.pi, bins='auto', density=True)
  plt.ylabel('Density')
  plt.xlabel('$\\theta$ (Â°)')
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/099a65908ef7704d6358c246a22486d3109d6a14.png]]

#+begin_src ipython
  plt.scatter(ksi1, ksi2)
  plt.xlabel('$\\xi_{1}$')
  plt.ylabel('$\\xi_{2}$')
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/d01f5d2783046201922e1db5107ff2e26aa456cc.png]]

#+begin_src ipython
  Jij = weights[index][index]
  print(Jij.shape)
#+end_src

#+RESULTS:
: (1024, 1024)
#+RESULTS:

#+begin_src ipython
  plt.imshow(Jij, cmap='jet')
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/b0df0938d097ca3d5cc016971789444184450c2a.png]]

#+begin_src ipython
  # Plot the singular values
  plt.figure(figsize=(10, 5))
  plt.plot(S)
  plt.yscale('log')  # Log scale can be helpful to see the drop-off more clearly
  plt.title('Singular Values of the Weight Matrix')
  plt.ylabel('Singular values (log scale)')
  plt.xlabel('Index')
  plt.grid(True)
  plt.show()

  # To see the cumulative energy, plot the cumulative sum of squares of singular values
  cumulative_energy = np.cumsum(S*2) / np.sum(S*2)
  plt.figure(figsize=(10, 5))
  plt.plot(cumulative_energy)
  plt.title('Cumulative Sum of Squares of Singular Values')
  plt.ylabel('Cumulative energy')
  plt.xlabel('Index')
  plt.grid(True)
  plt.show()

#+end_src

#+RESULTS:
:RESULTS:
[[file:./.ob-jupyter/30201ecad8903d28918b95016afd87dd50c33b31.png]]
[[file:./.ob-jupyter/6c3e9d4171dfadfd37066d45bee6edb45d108828.png]]
:END:
