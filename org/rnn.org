#+TITLE: Data driven RNN
#+STARTUP: fold
#+PROPERTY: header-args:ipython :results both :exports both :async yes :session my_session :kernel torch

* Notebook Settings

#+begin_src ipython
  %load_ext autoreload
  %autoreload 2
  %reload_ext autoreload

  %run /home/leon/dual_task/dual_data/notebooks/setup.py
  %matplotlib inline
  %config InlineBackend.figure_format = 'png'
#+end_src

#+RESULTS:
: The autoreload extension is already loaded. To reload it, use:
:   %reload_ext autoreload
: Python exe
: /home/leon/mambaforge/envs/torch/bin/python

* Imports

#+begin_src ipython
  DEVICE = 'cuda'
  import torch
  import torch.nn as nn
  import torch.optim as optim
  from torch.utils.data import Dataset, TensorDataset, DataLoader
#+end_src

#+RESULTS:

* Utils
** Sliding Window

#+begin_src ipython
  class SlidingWindowDataset(Dataset):
      def __init__(self, data, sequence_length=100, stride=1):
          self.data = data
          self.sequence_length = sequence_length
          self.stride = stride
          # Calculate number of samples once to optimize __len__
          self.num_sessions, self.num_time_points, _ = self.data.size()
          self.num_samples_per_session = (self.num_time_points - self.sequence_length) // self.stride
          self.total_samples = self.num_samples_per_session * self.num_sessions

      def __len__(self):
          return self.total_samples

      def __getitem__(self, idx):
          # Determine which session this idx belongs to
          session_idx = idx // self.num_samples_per_session
          # Determine the start of the slice for this idx
          session_start = idx % self.num_samples_per_session
          time_idx = session_start * self.stride

          # Extract sequences using calculated indices
          input_sequence = self.data[session_idx, time_idx:time_idx + self.sequence_length]
          target_sequence = self.data[session_idx, time_idx + self.sequence_length]

          return input_sequence, target_sequence
#+end_src

#+RESULTS:

** Data Split

#+begin_src ipython
  def training_step(dataloader, model, loss_fn, optimizer, penalty=None, lbd=1, clip_grad=0, zero_grad=0, l1_ratio=0.95):
      device = torch.device(DEVICE if torch.cuda.is_available() else "cpu")

      model.train()
      for batch, (X, y) in enumerate(dataloader):
          X, y = X.to(device), y.to(device)

          if zero_grad > 0:
              try:
                  model.U[:, zero_grad-1] = 0
                  model.V[:, zero_grad-1] = 0
              except:
                  pass

          y_pred = model(X)
          loss = loss_fn(y_pred, y)

          if penalty is not None:
              reg_loss = 0
              for param in model.parameters():
                  if penalty=='l1':
                      reg_loss += torch.sum(torch.abs(param))
                  elif penalty=='l2':
                      reg_loss += torch.sum(torch.square(param))
                  else:
                      reg_loss += l1_ratio * torch.sum(torch.abs(param)) + (1.0-l1_ratio) * torch.sum(torch.square(param))

                  loss = loss + lbd * reg_loss

          # Backpropagation
          loss.backward()

          if zero_grad > 0:
              try:
                  model.U.grad[:, zero_grad-1] = 0
                  model.V.grad[:, zero_grad-1] = 0
              except:
                  pass

          # Clip gradients
          if clip_grad:
              torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)
              #torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)

          optimizer.step()
          optimizer.zero_grad()

      return loss
#+end_src

#+RESULTS:

#+begin_src ipython
    def split_data(X, Y, train_perc=0.8, batch_size=8, n_labels=2):

       sample_size = int(train_perc * (X.shape[0] // n_labels))
       all_indices = np.arange(X.shape[0] // n_labels)

       train_indices = []
       test_indices = []
       for i in range(n_labels):
          all_indices = np.arange(i * X.shape[0] // n_labels, (i+1) * X.shape[0] // n_labels)
          idx = np.random.choice(all_indices, size=sample_size, replace=False)

          train_indices.append(idx)
          test_indices.append(np.setdiff1d(all_indices, idx))

       X_train = X[train_indices]
       X_test = X[test_indices]

       Y_train = Y[train_indices]
       Y_test = Y[test_indices]

       print(X_train.shape, Y_train.shape)
       train_dataset = TensorDataset(X_train, Y_train)

       print(X_test.shape, Y_test.shape)
       val_dataset = TensorDataset(X_test, Y_test)

       # Create data loaders
       train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
       val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)

       # sequence_length = 14  # or any other sequence length you want
       # stride = 1  # or any other stride you want

       # sliding_window_dataset = SlidingWindowDataset(X, sequence_length, stride)
       # train_loader = torch.utils.data.DataLoader(sliding_window_dataset, batch_size=5, shuffle=True)
       # val_loader = torch.utils.data.DataLoader(sliding_window_dataset, batch_size=5, shuffle=True)

       return train_loader, val_loader
#+end_src

#+RESULTS:

#+begin_src ipython
  def validation_step(dataloader, model, loss_fn):
      size = len(dataloader.dataset)
      num_batches = len(dataloader)

      device = torch.device(DEVICE if torch.cuda.is_available() else "cpu")

      # Validation loop.
      model.eval()
      val_loss = 0.0

      with torch.no_grad():
          for X, y in dataloader:
              X, y = X.to(device), y.to(device)

              y_pred = model(X)
              loss = loss_fn(y_pred, y)

              val_loss += loss.item() * X.size(0)

          val_loss /= size
          # acc = metric.compute()
          # print(f"Accuracy: {acc}")
          # metric.reset()
      return val_loss
#+end_src

#+RESULTS:

** Optimization

#+begin_src ipython
  def train(dataloader, model, loss_fn, optimizer, penalty=None, zero_grad=0):
      size = len(dataloader.dataset)
      device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

      model.train()
      for batch, (X, y) in enumerate(dataloader):

          X, y = X.to(device), y.to(device)

          # Compute prediction error
          pred = model(X)
          loss = loss_fn(pred, y)

          if penalty is not None:
              reg_loss = 0
              for param in model.parameters():
                  if penalty=='l1':
                      reg_loss += torch.sum(torch.abs(param))
                  else:
                      reg_loss += torch.sum(torch.square(param))

                  loss = loss + lbd * reg_loss

          # Backpropagation
          loss.backward()
          optimizer.step()
          optimizer.zero_grad()

      return loss
#+end_src


#+RESULTS:

#+begin_src ipython
  def test(dataloader, model, loss_fn):
      size = len(dataloader.dataset)
      num_batches = len(dataloader)

      device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

      # Validation loop.
      model.eval()
      val_loss = 0.0
      with torch.no_grad():
          for data, targets in dataloader:
              data, targets = data.to(device), targets.to(device)

              outputs = model(data)
              loss = loss_fn(outputs, targets)
              val_loss += loss.item() * data.size(0)
          val_loss /= size

      return val_loss
#+end_src

#+RESULTS:

#+begin_src ipython
    def run_optim(model, train_loader, val_loader, loss_fn, optimizer, num_epochs=100, zero_grad=0, penalty=None, lbd=0, thresh=0.005, l1_ratio=0.95):

      # scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)
      scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)
      # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.1, verbose=True)
      # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)

      device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
      model.to(device)

      # Training loop.
      for epoch in range(num_epochs):
          loss = training_step(train_loader, model, loss_fn, optimizer, penalty, lbd, zero_grad=zero_grad, l1_ratio=l1_ratio)
          val_loss = validation_step(val_loader, model, loss_fn)
          scheduler.step(val_loss)

          if epoch % int(num_epochs  / 10) == 0:
              print(f'Epoch {epoch}/{num_epochs}, Training Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}')

          if val_loss < thresh and loss < thresh:
              print(f'Stopping training as loss has fallen below the threshold: {loss}, {val_loss}')
              break

          if val_loss > 300:
              print(f'Stopping training as loss is too high: {val_loss}')
              break

          if torch.isnan(loss):
              print(f'Stopping training as loss is NaN.')
              break
#+end_src

#+RESULTS:

** Prediction


#+begin_src ipython
  def get_predictions(model, future_steps, device='cuda:1'):
      model.eval()  # Set the model to evaluation mode

      # Start with an initial seed sequence
      input_size = model.input_size
      hidden_size = model.hidden_size

      seed_sequence = torch.randn(1, future_steps, input_size).to(device)  # Replace with your actual seed

      # Collect predictions
      predictions = []

      # Initialize the hidden state (optional, depends on your model architecture)
      hidden = torch.zeros(model.num_layers, 1, hidden_size).to(device)
      # hidden = torch.randn(model.num_layers, 1, hidden_size, device=device) * 0.01

      # Generate time series
      for _ in range(future_steps):
          # Forward pass
          with torch.no_grad():  # No need to track gradients
              # out, hidden = model.rnn(seed_sequence, hidden)
              out = model(hidden)
              next_step = out[:, -1, :]  # Output for the last time step

          predictions.append(next_step.cpu().numpy())

          # Use the predicted next step as the input for the next iteration
          next_step = next_step.unsqueeze(1)  # Add the sequence length dimension
          seed_sequence = torch.cat((seed_sequence[:, 1:, :], next_step), 1)  # Move the window

      # # Convert predictions to a numpy array for further analysis
      predicted_time_series = np.concatenate(predictions, axis=0)

      return predicted_time_series

#+end_src

#+RESULTS:

** Pipeline

#+begin_src ipython
  def standard_scaler(data, IF_RETURN=0):
      mean = data.mean(dim=0, keepdim=True)
      std = data.std(dim=0, keepdim=True)
      if IF_RETURN:
          return (data - mean) / std, mean, std
      else:
          return (data - mean) / std

#+end_src

#+RESULTS:

#+begin_src ipython

  from torch.utils.data import DataLoader
  from torchvision import transforms

  # Assuming 'MyDataset' is a Dataset object you've made for your data
  class MyPipeline:
      def __init__(self, model, preprocessing=None):
          self.model = model
          self.preprocessing = preprocessing

      def __call__(self, x):
          if self.preprocessing:
              x = self.preprocessing(x)
          return self.model(x)

  # Define the transformations (preprocessing)
  preprocessing = transforms.Compose([
      transforms.ToTensor(),
      # standard_scaler()
  ])

  # Create the pipeline
  model = MultivariateRNN()  # Replace with your actual model
  pipeline = MyPipeline(model, preprocessing)

  # Now you can use your pipeline to process and feed data into your model
  dataset = MyDataset()
  dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

  # Use the pipeline in your training loop
  for inputs, targets in dataloader:
      predictions = pipeline(inputs)
      loss = loss_func(predictions, targets)
      # ... rest of your training loop
#+end_src

#+RESULTS:
:RESULTS:
: /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
:   warn(
# [goto error]
#+begin_example
  ---------------------------------------------------------------------------
  NameError                                 Traceback (most recent call last)
  Cell In[12], line 22
       16 preprocessing = transforms.Compose([
       17     transforms.ToTensor(),
       18     # standard_scaler()
       19 ])
       21 # Create the pipeline
  ---> 22 model = MultivariateRNN()  # Replace with your actual model
       23 pipeline = MyPipeline(model, preprocessing)
       25 # Now you can use your pipeline to process and feed data into your model

  NameError: name 'MultivariateRNN' is not defined
#+end_example
:END:

** Synthetic Data

#+begin_src ipython
  def generate_multivariate_time_series(num_series, num_steps, num_features, device='cuda'):
      np.random.seed(42)  # For reproducibility

      # Generate random frequencies and phases for the sine waves
      frequencies = np.random.uniform(low=0.1, high=2.0, size=(num_features))
      phases = np.random.uniform(low=0, high=2*np.pi, size=(num_features))
      noise = np.random.uniform(low=0, high=1, size=(num_series))

      # Generate time steps for the sine waves
      time_steps = np.linspace(0, num_steps, num_steps)

      # Initialize the data array
      data = np.zeros((num_series, num_steps, num_features))

      # Populate the data array with sine waves
      for i in range(num_series):
          for j in range(num_steps):
              for k in range(num_features):
                  data[i, j, k] = np.sin(2 * np.pi * j / num_steps - phases[k]) + np.random.uniform() * .1

      # Return as torch.FloatTensor
      return torch.FloatTensor(data).to(device)

#+end_src

#+RESULTS:

** Loss

#+begin_src ipython
  class CustomBCELoss(nn.Module):
      def __init__(self):
          super(CustomBCELoss, self).__init__()

      def forward(self, inputs, targets):
          inputs = torch.cat(inputs, dim=1)
          y_pred = self.linear(inputs[:, -1, :])

          proba = torch.sigmoid(y_pred).squeeze(-1)

          loss = F.binary_cross_entropy(proba, targets, reduction='none')

          return loss.mean()  # Or .sum(), or custom reduction as needed.
#+end_src

#+RESULTS:

* RNN models
** Vanilla

#+begin_src ipython
  # Define the RNN model
  class VanillaRNN(nn.Module):
      def __init__(self, input_size, hidden_size, num_layers, output_size, device):
          super(VanillaRNN, self).__init__()
          self.hidden_size = hidden_size
          self.num_layers = num_layers
          self.device=device
          # You can swap nn.RNN with nn.LSTM or nn.GRU depending on your requirements

          self.rnn = nn.RNN(input_size, hidden_size, num_layers,
                            batch_first=True, nonlinearity='relu', device=self.device)

          self.fc = nn.Linear(hidden_size, output_size, device=self.device)

          DT = 0.1
          TAU = 20

          self.DT_TAU = DT/TAU
          self.EXP_DT_TAU = np.exp(-DT/TAU)

      def forward(self, input):
          # Initial hidden state (can also initialize this outside and pass it as a parameter)
          rates = torch.zeros(self.num_layers, input.size(1), self.hidden_size, device=self.device)
          h = torch.zeros(self.num_layers, input.size(1), self.hidden_size, device=self.device)

          # Forward propagate the RNN
          h, _ = self.rnn(input, rates)
          rates = self.EXP_DT_TAU * rates + self.DT_TAU * h
          output = self.fc(rates)

          return output
#+end_src

#+RESULTS:

** Classifier

#+begin_src ipython
  class CustomCombinedLoss(nn.Module):
      def __init__(self, weight1=1.0, weight2=1.0):
          super(CustomCombinedLoss, self).__init__()
          self.weight1 = weight1
          self.weight2 = weight2
          # You could also include additional initializations for
          # each distinct condition if necessary.

      def forward(self, inputs, targets):
          # Condition 1 (for example, Mean Squared Error)
          loss1 = torch.mean((inputs - targets[0]) ** 2)

          # Condition 2 (for example, Mean Absolute Error)
          loss2 = torch.mean(torch.abs(inputs - targets[1]))

          # Combine the two conditions
          loss = self.weight1 * loss1 + self.weight2 * loss2
          return loss
#+end_src

#+RESULTS:

#+BEGIN_SRC ipython
  import torch
  import torch.nn as nn
  import torch.nn.functional as F

  class ClassifierRNN(nn.Module):
      def __init__(self, input_size, hidden_size, num_layers, output_size, device='cuda', dt=.01, noise=0.01, rank=2, alpha=0.0):
          super(ClassifierRNN, self).__init__()

          self.input_size = input_size
          self.hidden_size = hidden_size
          self.output_size = output_size

          self.num_layers = num_layers
          self.alpha = alpha
          # Weight matrices
          self.W_ih = nn.Parameter(torch.Tensor(hidden_size, input_size))
          self.W_hh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))
          # Bias terms
          self.b_ih = nn.Parameter(torch.Tensor(hidden_size))
          self.b_hh = nn.Parameter(torch.Tensor(hidden_size))
          # Low rank
          self.xi = nn.Parameter(torch.Tensor(hidden_size, rank))

          self.noise_std = torch.tensor(noise)

          self.linear = nn.Linear(hidden_size, 1)
          # Decay rate for the hidden state

          with torch.no_grad():
              self.decay_rate = torch.tensor(dt / 20)
              self.exp_rate = torch.exp(-self.decay_rate)

              self.decay_syn = torch.tensor(dt / 3)
              self.exp_syn = torch.exp(-self.decay_syn)

          # Initialize parameters
          self.reset_parameters()

      def reset_parameters(self):
          # Initialize weight and bias parameters using xavier initialization or another preferred method
          nn.init.xavier_uniform_(self.W_ih)
          nn.init.xavier_uniform_(self.W_hh)
          nn.init.zeros_(self.b_ih)
          nn.init.zeros_(self.b_hh)
          nn.init.normal_(self.xi)

      def forward(self, x):
          # x is of shape (batch_size, sequence_length, input_size)
          batch_size, seq_length, _ = x.size()

          rec_inputs = torch.zeros(batch_size, self.hidden_size, device=x.device)
          rates = torch.zeros(batch_size, self.hidden_size, device=x.device)
          noise = torch.randn(batch_size, seq_length, self.hidden_size, device=x.device)

          outputs = []
          for t in range(seq_length):

              ff_inputs = torch.mm(x[:, t], self.W_ih.t()) + self.b_ih

              full = torch.mm(rates, self.W_hh.t()) + self.b_hh
              lr = rates.matmul(self.xi).matmul(self.xi.t()) / self.hidden_size

              hidden = (1.0 - self.alpha) * full + self.alpha * lr
              rec_inputs = self.exp_syn * rec_inputs + self.decay_syn * hidden
              # rec_inputs = full + lr
              net_inputs = ff_inputs + rec_inputs + noise[:, t, :] * self.noise_std

              # Compute rates
              rates = nn.ReLU()(net_inputs)
              # rates = rates * self.exp_rate + self.decay_rate * nn.ReLU()(net_inputs)

              # Collect outputs
              outputs.append(rates.unsqueeze(1))

          # Concatenate outputs along the time dimension
          outputs = torch.cat(outputs, dim=1)
          y_pred = self.linear(outputs[:, -1, :])

          return torch.sigmoid(y_pred).squeeze(-1)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython
  # Example usage:
  input_size = 10
  hidden_size = 20
  num_layers = 1
  output_size = 10

  decay_rate = 0.1

  seq_length = 5
  batch_size = 3

  model = ClassifierRNN(input_size, hidden_size, num_layers, output_size, decay_rate)
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model.to(device)

  input_tensor = torch.randn(batch_size, seq_length, input_size)
  input_tensor = input_tensor.to(device)

  output = model(input_tensor)
  print(input_tensor.shape, output.shape)
#+END_SRC

#+RESULTS:
: torch.Size([3, 5, 10]) torch.Size([3])

** Rate

#+begin_src ipython
  class CustomCombinedLoss(nn.Module):
      def __init__(self, weight1=1.0, weight2=1.0):
          super(CustomCombinedLoss, self).__init__()
          self.weight1 = weight1
          self.weight2 = weight2
          # You could also include additional initializations for
          # each distinct condition if necessary.

      def forward(self, inputs, targets):
          # Condition 1 (for example, Mean Squared Error)
          loss1 = torch.mean((inputs - targets[0]) ** 2)

          # Condition 2 (for example, Mean Absolute Error)
          loss2 = torch.mean(torch.abs(inputs - targets[1]))

          # Combine the two conditions
          loss = self.weight1 * loss1 + self.weight2 * loss2
          return loss
#+end_src

#+RESULTS:

#+begin_src ipython
  covariance = torch.tensor([[1.0, self.LR_COV],
                             [self.LR_COV, 1.0],], dtype=self.FLOAT, device=self.device)


  multivariate_normal = MultivariateNormal(mean, covariance)
  self.ksi = multivariate_normal.sample((Nb,)).T

#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
: ---------------------------------------------------------------------------
: NameError                                 Traceback (most recent call last)
: Cell In[20], line 1
: ----> 1 covariance = torch.tensor([[1.0, self.LR_COV],
:       2                            [self.LR_COV, 1.0],], dtype=self.FLOAT, device=self.device)
:       5 multivariate_normal = MultivariateNormal(mean, covariance)
:       6 self.ksi = multivariate_normal.sample((Nb,)).T
:
: NameError: name 'self' is not defined
:END:

#+begin_src ipython
  ff_input = self.Ja0[0] * (1.0 + self.ksi[0] * self.I0[0] * self.M0)
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
: ---------------------------------------------------------------------------
: NameError                                 Traceback (most recent call last)
: Cell In[21], line 1
: ----> 1 ff_input = self.Ja0[0] * (1.0 + self.ksi[0] * self.I0[0] * self.M0)
:
: NameError: name 'self' is not defined
:END:


#+BEGIN_SRC ipython
  import torch
  import torch.nn as nn
  import torch.nn.functional as F

  class ClassifierRNN(nn.Module):
      def __init__(self, input_size, hidden_size, num_layers, output_size, device='cuda', dt=.01, noise=0.01, rank=2, alpha=0.0):
          super(ClassifierRNN, self).__init__()

          self.input_size = input_size
          self.hidden_size = hidden_size
          self.output_size = output_size

          self.num_layers = num_layers
          self.alpha = alpha
          # Weight matrices
          self.W_ih = nn.Parameter(torch.Tensor(hidden_size, input_size))
          self.W_hh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))
          # Bias terms
          self.b_ih = nn.Parameter(torch.Tensor(hidden_size))
          self.b_hh = nn.Parameter(torch.Tensor(hidden_size))
          # Low rank
          self.xi = nn.Parameter(torch.Tensor(hidden_size, rank))

          self.noise_std = torch.tensor(noise)

          self.linear = nn.Linear(hidden_size, 1)
          # Decay rate for the hidden state

          with torch.no_grad():
              self.decay_rate = torch.tensor(dt / 20)
              self.exp_rate = torch.exp(-self.decay_rate)

              self.decay_syn = torch.tensor(dt / 3)
              self.exp_syn = torch.exp(-self.decay_syn)

          # Initialize parameters
          self.reset_parameters()

      def reset_parameters(self):
          # Initialize weight and bias parameters using xavier initialization or another preferred method
          nn.init.xavier_uniform_(self.W_ih)
          nn.init.xavier_uniform_(self.W_hh)
          nn.init.zeros_(self.b_ih)
          nn.init.zeros_(self.b_hh)
          nn.init.normal_(self.xi)

      def forward(self, x):
          # x is of shape (batch_size, sequence_length, input_size)
          batch_size, seq_length, _ = x.size()

          rec_inputs = torch.zeros(batch_size, self.hidden_size, device=x.device)
          rates = torch.zeros(batch_size, self.hidden_size, device=x.device)
          noise = torch.randn(batch_size, seq_length, self.hidden_size, device=x.device)

          outputs = []
          for t in range(seq_length):

              ff_inputs = torch.mm(x[:, t], self.W_ih.t()) + self.b_ih

              full = torch.mm(rates, self.W_hh.t()) + self.b_hh
              lr = rates.matmul(self.xi).matmul(self.xi.t()) / self.hidden_size

              hidden = (1.0 - self.alpha) * full + self.alpha * lr
              rec_inputs = self.exp_syn * rec_inputs + self.decay_syn * hidden
              # rec_inputs = full + lr
              net_inputs = ff_inputs + rec_inputs + noise[:, t, :] * self.noise_std

              # Compute rates
              rates = nn.ReLU()(net_inputs)
              # rates = rates * self.exp_rate + self.decay_rate * nn.ReLU()(net_inputs)

              # Collect outputs
              outputs.append(rates.unsqueeze(1))

          # Concatenate outputs along the time dimension
          outputs = torch.cat(outputs, dim=1)
          y_pred = self.linear(outputs[:, -1, :])

          return torch.sigmoid(y_pred).squeeze(-1)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython
  # Example usage:
  input_size = 10
  hidden_size = 20
  num_layers = 1
  output_size = 10

  decay_rate = 0.1

  seq_length = 5
  batch_size = 3

  model = ClassifierRNN(input_size, hidden_size, num_layers, output_size, decay_rate)
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model.to(device)

  input_tensor = torch.randn(batch_size, seq_length, input_size)
  input_tensor = input_tensor.to(device)

  output = model(input_tensor)
  print(input_tensor.shape, output.shape)
#+END_SRC

#+RESULTS:
: torch.Size([3, 5, 10]) torch.Size([3])

** Multivariate

#+BEGIN_SRC ipython
  import torch
  import torch.nn as nn
  import torch.nn.functional as F

  class MultivariateRNN(nn.Module):
      def __init__(self, input_size, hidden_size, num_layers, output_size, device='cuda', dt=.01, noise=0.01, rank=2, alpha=0.5):
          super(MultivariateRNN, self).__init__()

          self.input_size = input_size
          self.hidden_size = hidden_size
          self.output_size = output_size
          self.alpha = alpha
          self.num_layers = num_layers

          # Weight matrices
          self.W_ih = nn.Parameter(torch.Tensor(hidden_size, input_size))
          self.W_hh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))
          # Bias terms
          self.b_ih = nn.Parameter(torch.Tensor(hidden_size))
          self.b_hh = nn.Parameter(torch.Tensor(hidden_size))
          self.xi = nn.Parameter(torch.Tensor(hidden_size, rank))

          self.noise_std = torch.tensor(noise)

          self.fc = nn.Linear(hidden_size, output_size, bias=False)
          # Decay rate for the hidden state

          with torch.no_grad():
              self.decay_rate = torch.tensor(dt / 20)
              self.exp_rate = torch.exp(-self.decay_rate)

              self.decay_syn = torch.tensor(dt / 3)
              self.exp_syn = torch.exp(-self.decay_syn)

          # Initialize parameters
          self.reset_parameters()

      def reset_parameters(self):
          # Initialize weight and bias parameters using xavier initialization or another preferred method
          nn.init.xavier_uniform_(self.W_ih)
          nn.init.xavier_uniform_(self.W_hh)
          nn.init.zeros_(self.b_ih)
          nn.init.zeros_(self.b_hh)
          nn.init.normal_(self.xi)

      def forward(self, x):
          # x is of shape (batch_size, sequence_length, input_size)
          batch_size, seq_length, _ = x.size()

          rec_inputs = torch.zeros(batch_size, self.W_hh.size(0), device=x.device)
          rates = torch.zeros(batch_size, self.W_hh.size(0), device=x.device)
          noise = torch.randn(batch_size, seq_length, self.W_hh.size(0), device=x.device)

          outputs = []
          for t in range(seq_length):

              ff_inputs = torch.mm(x[:, t], self.W_ih.t()) + self.b_ih
              hidden = torch.mm(rates, self.W_hh.t()) + self.b_hh

              full = torch.mm(rates, self.W_hh.t()) + self.b_hh
              lr = rates.matmul(self.xi).matmul(self.xi.t()) / self.hidden_size

              hidden = (1.0 - self.alpha) * full + self.alpha * lr

              rec_inputs = self.exp_syn * rec_inputs + self.decay_syn * hidden
              # rec_inputs = hidden

              net_inputs = ff_inputs + rec_inputs + noise[:, t, :] * self.noise_std

              # Compute rates
              rates = nn.ReLU()(net_inputs)
              # rates = rates * self.exp_rate + self.decay_rate * nn.ReLU()(net_inputs)

              # Collect outputs
              outputs.append(self.fc(rates.unsqueeze(1)))

          # Concatenate outputs along the time dimension
          outputs = torch.cat(outputs, dim=1)

          return outputs
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython
  # Example usage:
  input_size = 10
  hidden_size = 20
  num_layers = 1
  output_size = 10

  decay_rate = 0.1

  seq_length = 5
  batch_size = 3

  model = MultivariateRNN(input_size, hidden_size, num_layers, output_size, decay_rate)
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model.to(device)

  input_tensor = torch.randn(batch_size, seq_length, input_size)
  input_tensor = input_tensor.to(device)

  output = model(input_tensor)
  print(input_tensor.shape, output.shape)
#+END_SRC

#+RESULTS:
: torch.Size([3, 5, 10]) torch.Size([3, 5, 10])

** Low rank

#+begin_src ipython
  class LRRNN(nn.Module):
      def __init__(self, N_NEURON, N_BATCH, RANK=2, DT=0.05, TAU=1, NONLINEAR='sig', DEVICE='cuda', DROP=0.5):
          super(LRRNN, self).__init__()

          self.N_BATCH = N_BATCH
          self.N_NEURON = N_NEURON
          self.RANK = RANK
          self.DEVICE = DEVICE
          self.DT = DT
          self.TAU = TAU
          self.EXP_DT_TAU = torch.exp(-torch.tensor(self.DT / self.TAU))
          self.DT_TAU = self.DT / self.TAU

          self.dropout = nn.Dropout(DROP)

          if NONLINEAR == 'relu':
              self.Activation = nn.ReLU()
          else:
              self.Activation = nn.Tanh()

          self.U = nn.Parameter(
              torch.randn((self.N_NEURON, int(self.RANK)), device=self.DEVICE) * 0.001
          )

          # self.V = nn.Parameter(
          #     torch.randn((self.N_NEURON, int(self.RANK)), device=self.DEVICE) * 0.001
          # )

      def update_dynamics(self, rates, ff_input, rec_input, lr):
          noise = torch.randn_like(rates) * 0.01

          # update hidden state
          hidden = rates @ lr

          rec_input = rec_input * self.EXP_DT_TAU + hidden * self.DT_TAU + noise

          # compute net input
          net_input = ff_input + rec_input

          # update rates
          # non_linear = self.Activation(net_input)
          # rates = rates * self.EXP_DT_TAU + non_linear * self.DT_TAU + noise
          rates = self.Activation(net_input)

          return rates, rec_input

      def forward(self, ff_input):

          # initialize state
          rates = torch.zeros(ff_input.size(0), self.N_NEURON, device=self.DEVICE)
          rec_input = torch.zeros(ff_input.size(0), self.N_NEURON, device=self.DEVICE)
          lr = self.U @ self.U.T

          # print('ff_input', ff_input.shape, 'rates', rates.shape, 'lr', lr.shape)

          rates_sequence = []
          for step in range(ff_input.size(1)):
              rates, rec_input = self.update_dynamics(self.dropout(rates), ff_input[:, step], rec_input, lr)
              rates_sequence.append(rates.unsqueeze(1))

          rates_sequence = torch.cat(rates_sequence, dim=1)

          return rates_sequence
#+end_src

#+RESULTS:

#+begin_src ipython

#+end_src

#+RESULTS:

* Train on synthetic data
*** Create synthetic data

#+begin_src ipython
  num_series = 128  # Number of time series samples to generate
  num_steps = 84  # Number of time steps in each time series
  num_features = 100  # Number of features (signals) in each time series

  # Generate synthetic data
  synthetic_data = generate_multivariate_time_series(num_series, num_steps, num_features)

  # Split the data into inputs (X) and targets (Y), e.g., use previous timesteps to predict the next timestep
  X = synthetic_data[:, :-1, :]  # Using all but the last timestep as input
  Y = synthetic_data[:, 1:, :]   # Using all but the first timestep as target (shifted by one)

  print("Input shape:", X.shape)
  print("Target shape:", Y.shape)

#+end_src

#+RESULTS:
: Input shape: torch.Size([128, 83, 100])
: Target shape: torch.Size([128, 83, 100])

#+begin_src ipython
  plt.plot(np.arange(0, num_steps, 180), np.sin(num_steps))
  plt.plot(X.cpu().numpy()[0,:,2], alpha=1)
  plt.plot(X.cpu().numpy()[3,:,0], alpha=1, color='r')
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/4ec29a7710e2ba5a441ddf5b12b4b74c011fc6f7.png]]

*** Train model

#+begin_src ipython
  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

  hidden_size = 1000
  num_layers = 1
  # model = LRRNN(input_size=num_features, hidden_size=hidden_size,
  #               num_layers=num_layers, output_size=num_features, device=device)

  batch_size = 32
  model = LRRNN(N_NEURON=num_features, N_BATCH=batch_size, DEVICE=device, RANK=4)

  train_loader, val_loader = split_data(X, Y, train_perc=0.8, batch_size=batch_size)

  num_epochs = 100
  learning_rate = 0.01
  criterion = nn.MSELoss()
  optimizer = optim.Adam(model.parameters(), lr=learning_rate)
  model = LRRNN(N_NEURON=num_features, N_BATCH=batch_size, DEVICE=device, RANK=4)

  num_epochs = 100
  run_optim(model, train_loader, val_loader, criterion, optimizer, num_epochs)
#+end_src

#+RESULTS:
#+begin_example
  torch.Size([102, 83, 100]) torch.Size([102, 83, 100])
  torch.Size([26, 83, 100]) torch.Size([26, 83, 100])
  Epoch 0/100, Training Loss: 0.5030, Validation Loss: 0.5031
  Epoch 10/100, Training Loss: 0.4986, Validation Loss: 0.4985
  Epoch 20/100, Training Loss: 0.4900, Validation Loss: 0.4900
  Epoch 30/100, Training Loss: 0.4856, Validation Loss: 0.4856
  Epoch 40/100, Training Loss: 0.4847, Validation Loss: 0.4846
  Epoch 50/100, Training Loss: 0.4838, Validation Loss: 0.4841
  Epoch 60/100, Training Loss: 0.4837, Validation Loss: 0.4838
  Epoch 70/100, Training Loss: 0.4838, Validation Loss: 0.4837
  Epoch 80/100, Training Loss: 0.4835, Validation Loss: 0.4835
  Epoch 90/100, Training Loss: 0.4833, Validation Loss: 0.4834
#+end_example

*** See data

#+begin_src ipython
  from sklearn.metrics import mean_squared_error

  model.eval()  # Set the model to evaluation mode

  # This function feeds inputs through the model and computes the predictions
  def get_predictions(data_loader):
      predictions = []
      ground_truth = []
      with torch.no_grad():  # Disable gradient computation for evaluation
          for inputs, targets in data_loader:
              inputs, targets = inputs.to(device), targets.to(device)
              outputs = model(inputs)
              predictions.append(outputs.cpu()) # If using cuda, need to move data to cpu
              ground_truth.append(targets.cpu())

      # Concatenate all batches
      predictions = torch.cat(predictions, dim=0)
      ground_truth = torch.cat(ground_truth, dim=0)

      return predictions, ground_truth

  # Call the function using your data loader
  predictions, ground_truth = get_predictions(val_loader)

  print(ground_truth.numpy().shape, predictions.numpy().shape)
  # Calculate the loss or performance metric
  # For example, we can use the Mean Squared Error
  # error = mean_squared_error(ground_truth.numpy(), predictions.numpy())
  # print(f"Mean Squared Error: {error}")
#+end_src

#+RESULTS:
: (26, 83, 100) (26, 83, 100)


#+begin_src ipython
  import matplotlib.pyplot as plt

  # Convert tensors to numpy arrays for plotting
  predictions_np = predictions.numpy()
  ground_truth_np = ground_truth.numpy()

  # Plot the predictions on top of the ground truth
  plt.figure()
  pal = sns.color_palette("tab10")

  # Example for plotting the first feature dimension
  for i in range(2):
     # plt.plot(ground_truth_np[0, :, i], label='Ground Truth', marker='.', color=pal[i])
     plt.plot(predictions_np[1, :, i], label='Model Prediction', marker='x', color=pal[i])

  plt.title("Model Prediction vs Ground Truth", fontsize=14)
  plt.xlabel("Time steps")
  plt.ylabel("Value")
  # plt.legend(fontsize=12)
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/2c71401b65b1e954193f28dbfb7a94098950f67c.png]]

* Train on Experimental Data
** Imports

#+begin_src ipython
  import sys
  sys.path.insert(0, '../')

  from src.common.get_data import get_X_y_days, get_X_y_S1_S2
  from src.common.options import set_options
#+end_src

#+RESULTS:

** Parameters

#+begin_src ipython
  mice = ['ChRM04','JawsM15', 'JawsM18', 'ACCM03', 'ACCM04']
  tasks = ['DPA', 'DualGo', 'DualNoGo']
  days = ['first', 'last']

  kwargs = dict()
  kwargs = {'trials': '',
            'preprocess': True, 'scaler_BL': 'standard', 'avg_noise':True, 'unit_var_BL':True}

  kwargs['mouse'] = 'JawsM15'
#+end_src

#+RESULTS:

** Load Data

#+begin_src ipython
  options = set_options(**kwargs)
  options['reload'] = False
  options['data_type'] = 'dF'
  options['DCVL'] = 0
#+end_src

#+RESULTS:

#+begin_src ipython
  X_days, y_days = get_X_y_days(**options)
  options['day'] = 'last'
  options['task'] = 'DPA'
  X_data, y_data = get_X_y_S1_S2(X_days, y_days, **options)

  # y_data = y_data[:, np.newaxis]
  print(X_data.shape, y_data.shape)
#+end_src

#+RESULTS:
: Loading files from /home/leon/dual_task/dual_data/data/JawsM15
: ##########################################
: PREPROCESSING: SCALER standard AVG MEAN False AVG NOISE True UNIT VAR True
: ##########################################
: DATA: FEATURES sample TASK DPA TRIALS  DAYS last LASER 0
: multiple days 0 3 0
: (96, 693, 84) (96,)

#+begin_src ipython
  # import pickle as pkl

  # filename = "../data/" + kwargs['mouse'] + "/X_dcvl.pkl"
  # # pkl.dump(X_data, open(filename + ".pkl", "wb"))

  # filename = "../data/" + kwargs['mouse'] + "/y_dcvl_.pkl"
  # # pkl.dump(y_data, open(filename + ".pkl", "wb"))
#+end_src

#+RESULTS:

#+begin_src ipython
  # import pickle as pkl
  # filename = "../data/" + kwargs['mouse'] + "/X_dcvl.pkl"
  # X_data = pkl.load(open(filename + ".pkl", "rb"))

  # filename = "../data/" + kwargs['mouse'] + "/y_dcvl_.pkl"
  # y_data = pkl.load(open(filename + ".pkl", "rb"))
#+end_src

#+RESULTS:

#+begin_src ipython
  import numpy as np
  from scipy.ndimage import convolve1d

  def moving_average_multidim(data, window_size, axis=-1):
      """
      Apply a 1D moving average across a specified axis of a multi-dimensional array.

      :param data: multi-dimensional array of data
      :param window_size: size of the moving window
      :param axis: axis along which to apply the moving average
      :return: smoothed data with the same shape as input data
      """
      # Create a moving average filter window
      window = np.ones(window_size) / window_size
      # Apply 1D convolution along the specified axis
      smoothed_data = convolve1d(data, weights=window, axis=axis, mode='reflect')
      return smoothed_data

#+end_src

#+RESULTS:

#+begin_src ipython
  from src.decode.bump import circcvl
  # smoothed_data = circcvl(X_data, windowSize=2, axis=-1)
  print(X_data.shape)
  window_size = 6
  # from scipy.ndimage import gaussian_filter1d
  # smoothed_data = gaussian_filter1d(X_data, axis=-1, sigma=2)
  # smoothed_data = moving_average_multidim(X_data[..., :52], window_size, axis=-1)
  smoothed_data = moving_average_multidim(X_data, window_size, axis=-1) / 10
#+end_src

#+RESULTS:

#+begin_src ipython
  time = np.linspace(0, 14, 84)
  for i in range(10):
      i = np.random.randint(100)
      plt.plot(time, smoothed_data[-1, i,:], alpha=.5)

  plt.ylabel('Rate (Hz)')
  plt.xlabel('Time (s)')
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/02565f4436274ea86b847fb20f637924ea0acbba.png]]

** Training

#+begin_src ipython
  # y = np.roll(X_data, -1)
  # y = y[..., :-1]

  Y = smoothed_data[..., 1:]
  X = smoothed_data[..., :-1]

  X = np.swapaxes(X, 1, -1)
  Y = np.swapaxes(Y, 1, -1)

  print(X.shape, Y.shape)
#+end_src

#+RESULTS:
: (96, 83, 693) (96, 83, 693)

#+begin_src ipython
  X = torch.tensor(X, dtype=torch.float32, device=device)
  Y = torch.tensor(Y, dtype=torch.float32, device=device)
  print(X.shape, Y.shape)
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
#+begin_example
  ---------------------------------------------------------------------------
  RuntimeError                              Traceback (most recent call last)
  Cell In[417], line 1
  ----> 1 X = torch.tensor(X, dtype=torch.float32, device=device)
        2 Y = torch.tensor(Y, dtype=torch.float32, device=device)
        3 print(X.shape, Y.shape)

  RuntimeError: CUDA error: device-side assert triggered
  CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
  For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
#+end_example
:END:

#+RESULTS:

#+begin_src ipython
  # y_data[y_data==-1] = 0
  # Y = torch.tensor(y_data, dtype=torch.float32, device=device)
  # print(Y.shape)
#+end_src

#+RESULTS:

#+begin_src ipython
  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

  hidden_size = 1024
  num_layers = 1
  num_features = X.shape[-1]

  batch_size = 16
  train_loader, val_loader = split_data(X, Y, train_perc=0.8, batch_size=batch_size)
#+end_src

#+RESULTS:
#+begin_src ipython
  learning_rate = 0.01
  criterion = nn.MSELoss()
  # criterion = nn.L1Loss()
  criterion = nn.SmoothL1Loss()
  # criterion = nn.BCELoss() # Binary Cross-Entropy Loss

  optimizer = optim.Adam(model.parameters(), lr=learning_rate)
  # optimizer = optim.AdamW(model.parameters(), lr=learning_rate)

  num_epochs = 100

  # model = MultivariateRNN(input_size=num_features, hidden_size=hidden_size,
  #                       num_layers=num_layers, output_size=num_features, device=device, dt=.01, noise=0, alpha=1)
  model = LRRNN(N_NEURON=num_features, N_BATCH=batch_size, DEVICE=device, RANK=2)
#+end_src

#+RESULTS:

#+begin_src ipython
  run_optim(model, train_loader, val_loader, criterion, optimizer, num_epochs, zero_grad=2, penalty='elastic', lbd=.001)
#+end_src

#+RESULTS:

* Reverse Engineering
** Generate series

#+begin_src ipython
  from sklearn.metrics import mean_squared_error

  model.eval()  # Set the model to evaluation mode

  # This function feeds inputs through the model and computes the predictions
  def get_predictions(data_loader):
      predictions = []
      ground_truth = []
      with torch.no_grad():  # Disable gradient computation for evaluation
          for inputs, targets in data_loader:
              inputs, targets = inputs.to(device), targets.to(device)
              outputs = model(inputs)
              predictions.append(outputs.cpu())  # If using cuda, need to move data to cpu
              ground_truth.append(targets.cpu())

      # Concatenate all batches
      predictions = torch.cat(predictions, dim=0)
      ground_truth = torch.cat(ground_truth, dim=0)

      return predictions, ground_truth

  # Call the function using your data loader
  predictions, ground_truth = get_predictions(val_loader)

  print(ground_truth.numpy().shape, predictions.numpy().shape)
  # Calculate the loss or performance metric
  # For example, we can use the Mean Squared Error
  # error = mean_squared_error(ground_truth.numpy(), predictions.numpy())
  # print(f"Mean Squared Error: {error}")
#+end_src

#+RESULTS:
: (20, 83, 693) (20, 83, 693)

#+begin_src ipython
  import matplotlib.pyplot as plt

  # Assuming predictions and ground_truth are for a single batch or example:
  # predictions: tensor of shape (batch_size, sequence_length, output_size)
  # ground_truth: tensor of shape (batch_size, sequence_length, output_size)

  # Convert tensors to numpy arrays for plotting
  predictions_np = predictions.numpy()
  ground_truth_np = ground_truth.numpy()

  # Plot the predictions on top of the ground truth
  plt.figure()
  pal = sns.color_palette("tab10")
  time = np.linspace(0, 14, 84)[:-1]
  # Example for plotting the first feature dimension
  for i in range(3):
     j = np.random.randint(693)
     plt.plot(time, ground_truth_np[0, :, j], 'x', label='Ground Truth', color=pal[i], alpha=.2)
     plt.plot(time, predictions_np[0, :, j], '-', label='Model Prediction', color=pal[i], alpha=1)

  # You can loop through more feature dimensions if needed
  # for i in range(output_size):
     # plt.plot(ground_truth_np[0, :, i], label=f'Ground Truth Feature {i}', marker='.')
     # plt.plot(predictions_np[0, :, i], label=f'Prediction Feature {i}', marker='x')

  plt.title("Model Prediction vs Ground Truth")
  plt.xlabel("Time steps")
  plt.ylabel("Value")
  # plt.legend(fontsize=12)
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/6e7ba587925611ded6083969cb515334df8444fb.png]]

#+begin_src ipython

#+end_src

#+RESULTS:

** Low rank

#+begin_src ipython
  print(model.U.shape, model.U.shape)
  UdotV = model.U.T @ model.U
  print(UdotV.detach().cpu().numpy())
#+end_src

#+RESULTS:
: torch.Size([693, 2]) torch.Size([693, 2])
: [[6.489632e-04 9.939262e-06]
:  [9.939262e-06 6.078492e-04]]

#+begin_src ipython
  def angle_AB(A, B):
      A_norm = A / (np.linalg.norm(A) + 1e-5)
      B_norm = B / (np.linalg.norm(B) + 1e-5)

      return int(np.arccos(A_norm @ B_norm) * 180 / np.pi)
#+end_src

#+RESULTS:

#+begin_src ipython
  print(angle_AB(model.U[:, 0].detach().cpu().numpy(), model.U[:, 1].detach().cpu().numpy()))
  # print(angle_AB(model.U[:, 1].detach().cpu().numpy(), model.V[:, 1].detach().cpu().numpy()))
#+end_src

#+RESULTS:
: 89

#+begin_src ipython
  options['task'] = 'DPA'
  options['features'] = 'sample'
  print(options['day'])

  X_data, y_data = get_X_y_S1_S2(X_days, y_days, **options)
  X_data = np.swapaxes(X_data, 1, -1)

  print('X', X_data.shape, 'U', model.U.shape)
  fig, ax = plt.subplots(1, 2, figsize= [1.5 * width, height])

  U_proj = -(X_data @ model.U.detach().cpu().numpy()) * 100
  # V_proj = -(X_data @ model.V.detach().cpu().numpy()) * 100
  print('proj', U_proj.shape)

  idx = np.where(y_data==1)[0]
  # print('idx', idx.shape)

  ax[0].plot(U_proj[idx].mean(0)[..., 0], label='A')
  ax[1].plot(U_proj[idx].mean(0)[..., 1], label='A')

  idx = np.where(y_data==-1)[0]
  # print('idx', idx.shape)

  ax[0].plot(U_proj[idx].mean(0)[..., 0], label='B')
  ax[1].plot(U_proj[idx].mean(0)[..., 1], label='B')

  ax[0].set_ylabel('Axis U')
  ax[1].set_ylabel('Axis V')
  plt.legend(fontsize=10)
  plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: last
: DATA: FEATURES sample TASK DPA TRIALS  DAYS last LASER 0
: multiple days 0 3 0
: X (96, 84, 693) U torch.Size([693, 2])
: proj (96, 84, 2)
[[file:./.ob-jupyter/77384d0f7ce8c0e67a484be0aca2d9f1fc45e4a0.png]]
:END:

#+begin_src ipython

#+end_src

#+RESULTS:

* Connectivity

#+begin_src ipython
  # weights = model.rnn.weight_hh_l0.data.cpu().numpy()  # Get the recurring weights of the RNN
  weights = model.W_hh.cpu().detach().numpy()
  print(weights.shape)
  # Perform singular value decomposition<
  U, S, Vt = np.linalg.svd(weights, full_matrices=False)

  u1, u2, u3 = U[:, 0], U[:, 1], U[:, 2]  # First two left singular vectors
  v1, v2, v3 = Vt[0, :], Vt[1, :], Vt[2, :]  # First two right singular vectors
#+end_src

#+RESULTS:
: (1024, 1024)

#+begin_src ipython
  ksi1 = S[0] * u1 * v1
  ksi2 = S[1] * u2 * v2
  ksi3 = S[2] * u3 * v3
  print(ksi1.shape)
#+end_src

#+RESULTS:
: (1024,)

#+begin_src ipython
  plt.imshow(weights, cmap='jet')
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/11578db95a1a6b58bc8dec464e5a0f44c398e91e.png]]

#+begin_src ipython
print(S[:10])
#+end_src

#+RESULTS:
: [1.9814498 1.9744787 1.9646422 1.9554437 1.9534712 1.9489214 1.9417928
:  1.9313139 1.9252403 1.9179983]

#+begin_src ipython
  theta = np.arctan2(ksi2, ksi1)
  index = theta.argsort()
  print(index.shape)
#+end_src

#+RESULTS:
: (1024,)

#+begin_src ipython
  plt.hist(theta*180/np.pi, bins='auto', density=True)
  plt.ylabel('Density')
  plt.xlabel('$\\theta$ (Â°)')
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/099a65908ef7704d6358c246a22486d3109d6a14.png]]

#+begin_src ipython
  plt.scatter(ksi1, ksi2)
  plt.xlabel('$\\xi_{1}$')
  plt.ylabel('$\\xi_{2}$')
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/d01f5d2783046201922e1db5107ff2e26aa456cc.png]]

#+begin_src ipython
  Jij = weights[index][index]
  print(Jij.shape)
#+end_src

#+RESULTS:
: (1024, 1024)
#+RESULTS:

#+begin_src ipython
  plt.imshow(Jij, cmap='jet')
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/b0df0938d097ca3d5cc016971789444184450c2a.png]]

#+begin_src ipython
  # Plot the singular values
  plt.figure(figsize=(10, 5))
  plt.plot(S)
  plt.yscale('log')  # Log scale can be helpful to see the drop-off more clearly
  plt.title('Singular Values of the Weight Matrix')
  plt.ylabel('Singular values (log scale)')
  plt.xlabel('Index')
  plt.grid(True)
  plt.show()

  # To see the cumulative energy, plot the cumulative sum of squares of singular values
  cumulative_energy = np.cumsum(S*2) / np.sum(S*2)
  plt.figure(figsize=(10, 5))
  plt.plot(cumulative_energy)
  plt.title('Cumulative Sum of Squares of Singular Values')
  plt.ylabel('Cumulative energy')
  plt.xlabel('Index')
  plt.grid(True)
  plt.show()

#+end_src

#+RESULTS:
:RESULTS:
[[file:./.ob-jupyter/30201ecad8903d28918b95016afd87dd50c33b31.png]]
[[file:./.ob-jupyter/6c3e9d4171dfadfd37066d45bee6edb45d108828.png]]
:END:
