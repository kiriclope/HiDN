#+TITLE: Data driven RNN
#+STARTUP: fold
#+PROPERTY: header-args:ipython :results both :exports both :async yes :session my_session :kernel torch

* Notebook Settings

#+begin_src ipython
  %load_ext autoreload
  %autoreload 2
  %reload_ext autoreload

  %run /home/leon/dual_task/dual_data/notebooks/setup.py
  %matplotlib inline
  %config InlineBackend.figure_format = 'png'
#+end_src

#+RESULTS:
: The autoreload extension is already loaded. To reload it, use:
:   %reload_ext autoreload
: Python exe
: /home/leon/mambaforge/envs/torch/bin/python

* Imports

#+begin_src ipython
  DEVICE = 'cuda'
  import torch
  import torch.nn as nn
  import torch.optim as optim
  from torch.utils.data import Dataset, TensorDataset, DataLoader
#+end_src

#+RESULTS:

* Utils
** Sliding Window

#+begin_src ipython
  class SlidingWindowDataset(Dataset):
      def __init__(self, data, sequence_length=100, stride=1):
          self.data = data
          self.sequence_length = sequence_length
          self.stride = stride
          # Calculate number of samples once to optimize __len__
          self.num_sessions, self.num_time_points, _ = self.data.size()
          self.num_samples_per_session = (self.num_time_points - self.sequence_length) // self.stride
          self.total_samples = self.num_samples_per_session * self.num_sessions

      def __len__(self):
          return self.total_samples

      def __getitem__(self, idx):
          # Determine which session this idx belongs to
          session_idx = idx // self.num_samples_per_session
          # Determine the start of the slice for this idx
          session_start = idx % self.num_samples_per_session
          time_idx = session_start * self.stride

          # Extract sequences using calculated indices
          input_sequence = self.data[session_idx, time_idx:time_idx + self.sequence_length]
          target_sequence = self.data[session_idx, time_idx + self.sequence_length]

          return input_sequence, target_sequence
#+end_src

#+RESULTS:

** Data Split


#+RESULTS:

#+begin_src ipython
    def split_data(X, Y, train_perc=0.8, batch_size=8, n_labels=2):

       sample_size = int(train_perc * (X.shape[0] // n_labels))
       all_indices = np.arange(X.shape[0] // n_labels)

       train_indices = []
       test_indices = []
       for i in range(n_labels):
          all_indices = np.arange(i * X.shape[0] // n_labels, (i+1) * X.shape[0] // n_labels)
          idx = np.random.choice(all_indices, size=sample_size, replace=False)

          train_indices.append(idx)
          test_indices.append(np.setdiff1d(all_indices, idx))

       X_train = X[train_indices]
       X_test = X[test_indices]

       Y_train = Y[train_indices]
       Y_test = Y[test_indices]

       print(X_train.shape, Y_train.shape)
       train_dataset = TensorDataset(X_train, Y_train)

       print(X_test.shape, Y_test.shape)
       val_dataset = TensorDataset(X_test, Y_test)

       # Create data loaders
       train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
       val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)

       # sequence_length = 14  # or any other sequence length you want
       # stride = 1  # or any other stride you want

       # sliding_window_dataset = SlidingWindowDataset(X, sequence_length, stride)
       # train_loader = torch.utils.data.DataLoader(sliding_window_dataset, batch_size=5, shuffle=True)
       # val_loader = torch.utils.data.DataLoader(sliding_window_dataset, batch_size=5, shuffle=True)

       return train_loader, val_loader
#+end_src

#+RESULTS:

#+begin_src ipython
import numpy as np
from torch.utils.data import TensorDataset, DataLoader
import torch

def split_data(X, Y, train_perc=0.8, batch_size=8, n_labels=2):

    sample_size = int(train_perc * (X.shape[0] // n_labels))
    train_indices = []
    test_indices = []

    for i in range(n_labels):
        start_idx = i * (X.shape[0] // n_labels)
        end_idx = (i + 1) * (X.shape[0] // n_labels)
        all_indices = np.arange(start_idx, end_idx)
        idx = np.random.choice(all_indices, size=sample_size, replace=False)
        train_indices.extend(idx)
        test_indices.extend(np.setdiff1d(all_indices, idx))

    train_indices = np.array(train_indices)
    test_indices = np.array(test_indices)

    X_train, X_test = X[train_indices], X[test_indices]
    Y_train, Y_test = Y[train_indices], Y[test_indices]

    print("X_train shape:", X_train.shape, "Y_train shape:", Y_train.shape)
    train_dataset = TensorDataset(X_train, Y_train)

    print("X_test shape:", X_test.shape, "Y_test shape:", Y_test.shape)
    val_dataset = TensorDataset(X_test, Y_test)

    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)

    return train_loader, val_loader
#+end_src

#+RESULTS:

#+begin_src ipython
  def training_step(dataloader, model, loss_fn, optimizer, penalty=None, lbd=1, clip_grad=0, l1_ratio=0.95):
      device = torch.device(DEVICE if torch.cuda.is_available() else "cpu")

      model.train()
      for batch, (X, y) in enumerate(dataloader):
          X, y = X.to(device), y.to(device)

          y_pred = model(X)
          loss = loss_fn(y_pred, y)

          if penalty is not None:
              reg_loss = 0
              for param in model.parameters():
                  if penalty=='l1':
                      reg_loss += torch.sum(torch.abs(param))
                  elif penalty=='l2':
                      reg_loss += torch.sum(torch.square(param))
                  else:
                      reg_loss += l1_ratio * torch.sum(torch.abs(param)) + (1.0-l1_ratio) * torch.sum(torch.square(param))

                  loss = loss + lbd * reg_loss

          # Backpropagation
          loss.backward()

          # Clip gradients
          if clip_grad:
              torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)
              #torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)

          optimizer.step()
          optimizer.zero_grad()

      return loss
#+end_src

#+RESULTS:

#+begin_src ipython
  def validation_step(dataloader, model, loss_fn):
      size = len(dataloader.dataset)
      num_batches = len(dataloader)

      device = torch.device(DEVICE if torch.cuda.is_available() else "cpu")

      # Validation loop.
      model.eval()
      val_loss = 0.0

      with torch.no_grad():
          for X, y in dataloader:
              X, y = X.to(device), y.to(device)

              y_pred = model(X)
              loss = loss_fn(y_pred, y)

              val_loss += loss.item() * X.size(0)

          val_loss /= size
          # acc = metric.compute()
          # print(f"Accuracy: {acc}")
          # metric.reset()
      return val_loss
#+end_src

#+RESULTS:

** Optimization

#+begin_src ipython
    def run_optim(model, train_loader, val_loader, loss_fn, optimizer, num_epochs=100, penalty=None, lbd=0, thresh=0.005, l1_ratio=0.95):

      scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)
      # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)
      # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.1, verbose=True)
      # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)

      device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
      model.to(device)

      # Training loop.
      for epoch in range(num_epochs):
          loss = training_step(train_loader, model, loss_fn, optimizer, penalty, lbd, l1_ratio=l1_ratio)
          val_loss = validation_step(val_loader, model, loss_fn)

          scheduler.step()

          if epoch % int(num_epochs  / 10) == 0:
              print(f'Epoch {epoch}/{num_epochs}, Training Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}')

          if val_loss < thresh and loss < thresh:
              print(f'Stopping training as loss has fallen below the threshold: {loss}, {val_loss}')
              break

          if val_loss > 300:
              print(f'Stopping training as loss is too high: {val_loss}')
              break

          if torch.isnan(loss):
              print(f'Stopping training as loss is NaN.')
              break
#+end_src

#+RESULTS:

** Prediction


#+begin_src ipython
  def get_predictions(model, future_steps, device='cuda:1'):
      model.eval()  # Set the model to evaluation mode

      # Start with an initial seed sequence
      input_size = model.input_size
      hidden_size = model.hidden_size

      seed_sequence = torch.randn(1, future_steps, input_size).to(device)  # Replace with your actual seed

      # Collect predictions
      predictions = []

      # Initialize the hidden state (optional, depends on your model architecture)
      hidden = torch.zeros(model.num_layers, 1, hidden_size).to(device)
      # hidden = torch.randn(model.num_layers, 1, hidden_size, device=device) * 0.01

      # Generate time series
      for _ in range(future_steps):
          # Forward pass
          with torch.no_grad():  # No need to track gradients
              # out, hidden = model.rnn(seed_sequence, hidden)
              out = model(hidden)
              next_step = out[:, -1, :]  # Output for the last time step

          predictions.append(next_step.cpu().numpy())

          # Use the predicted next step as the input for the next iteration
          next_step = next_step.unsqueeze(1)  # Add the sequence length dimension
          seed_sequence = torch.cat((seed_sequence[:, 1:, :], next_step), 1)  # Move the window

      # # Convert predictions to a numpy array for further analysis
      predicted_time_series = np.concatenate(predictions, axis=0)

      return predicted_time_series

#+end_src

#+RESULTS:

** Pipeline

#+begin_src ipython
  def standard_scaler(data, IF_RETURN=0):
      mean = data.mean(dim=0, keepdim=True)
      std = data.std(dim=0, keepdim=True)
      if IF_RETURN:
          return (data - mean) / std, mean, std
      else:
          return (data - mean) / std

#+end_src

#+RESULTS:

#+begin_src ipython

  from torch.utils.data import DataLoader
  from torchvision import transforms

  # Assuming 'MyDataset' is a Dataset object you've made for your data
  class MyPipeline:
      def __init__(self, model, preprocessing=None):
          self.model = model
          self.preprocessing = preprocessing

      def __call__(self, x):
          if self.preprocessing:
              x = self.preprocessing(x)
          return self.model(x)

  # Define the transformations (preprocessing)
  preprocessing = transforms.Compose([
      transforms.ToTensor(),
      # standard_scaler()
  ])

  # Create the pipeline
  model = MultivariateRNN()  # Replace with your actual model
  pipeline = MyPipeline(model, preprocessing)

  # Now you can use your pipeline to process and feed data into your model
  dataset = MyDataset()
  dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

  # Use the pipeline in your training loop
  for inputs, targets in dataloader:
      predictions = pipeline(inputs)
      loss = loss_func(predictions, targets)
      # ... rest of your training loop
#+end_src

#+RESULTS:
:RESULTS:
: /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
:   warn(
# [goto error]
#+begin_example
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
Cell In[11], line 22
     16 preprocessing = transforms.Compose([
     17     transforms.ToTensor(),
     18     # standard_scaler()
     19 ])
     21 # Create the pipeline
---> 22 model = MultivariateRNN()  # Replace with your actual model
     23 pipeline = MyPipeline(model, preprocessing)
     25 # Now you can use your pipeline to process and feed data into your model

NameError: name 'MultivariateRNN' is not defined
#+end_example
:END:

** Synthetic Data

#+begin_src ipython
  def generate_multivariate_time_series(num_series, num_steps, num_features, device='cuda'):
      np.random.seed(42)  # For reproducibility

      # Generate random frequencies and phases for the sine waves
      frequencies = np.random.uniform(low=0.1, high=2.0, size=(num_features))
      phases = np.random.uniform(low=0, high=2*np.pi, size=(num_features))
      noise = np.random.uniform(low=0, high=1, size=(num_series))

      # Generate time steps for the sine waves
      time_steps = np.linspace(0, num_steps, num_steps)

      # Initialize the data array
      data = np.zeros((num_series, num_steps, num_features))

      # Populate the data array with sine waves
      for i in range(num_series):
          for j in range(num_steps):
              for k in range(num_features):
                  data[i, j, k] = np.sin(2 * np.pi * j / num_steps - phases[k]) + np.random.uniform() * .1

      # Return as torch.FloatTensor
      return torch.FloatTensor(data).to(device)

#+end_src

#+RESULTS:

** Loss

#+begin_src ipython
  class CustomBCELoss(nn.Module):
      def __init__(self):
          super(CustomBCELoss, self).__init__()

      def forward(self, inputs, targets):
          inputs = torch.cat(inputs, dim=1)
          y_pred = self.linear(inputs[:, -1, :])

          proba = torch.sigmoid(y_pred).squeeze(-1)

          loss = F.binary_cross_entropy(proba, targets, reduction='none')

          return loss.mean()  # Or .sum(), or custom reduction as needed.
#+end_src

#+RESULTS:

* RNN models
** Vanilla

#+begin_src ipython
  # Define the RNN model
  class VanillaRNN(nn.Module):
      def __init__(self, input_size, hidden_size, num_layers, output_size, device):
          super(VanillaRNN, self).__init__()
          self.hidden_size = hidden_size
          self.num_layers = num_layers
          self.device=device
          # You can swap nn.RNN with nn.LSTM or nn.GRU depending on your requirements

          self.rnn = nn.RNN(input_size, hidden_size, num_layers,
                            batch_first=True, nonlinearity='relu', device=self.device)

          self.fc = nn.Linear(hidden_size, output_size, device=self.device)

          DT = 0.1
          TAU = 20

          self.DT_TAU = DT / TAU
          self.EXP_DT_TAU = np.exp(-DT/TAU)

      def forward(self, input):
          # Initial hidden state (can also initialize this outside and pass it as a parameter)
          rates = torch.zeros(self.num_layers, input.size(1), self.hidden_size, device=self.device)
          h = torch.zeros(self.num_layers, input.size(1), self.hidden_size, device=self.device)

          # Forward propagate the RNN
          h, _ = self.rnn(input, rates)
          rates = self.EXP_DT_TAU * rates + self.DT_TAU * h
          output = self.fc(rates)

          return output
#+end_src

#+RESULTS:

** Classifier

#+begin_src ipython
  class CustomCombinedLoss(nn.Module):
      def __init__(self, weight1=1.0, weight2=1.0):
          super(CustomCombinedLoss, self).__init__()
          self.weight1 = weight1
          self.weight2 = weight2
          # You could also include additional initializations for
          # each distinct condition if necessary.

      def forward(self, inputs, targets):
          # Condition 1 (for example, Mean Squared Error)
          loss1 = torch.mean((inputs - targets[0]) ** 2)

          # Condition 2 (for example, Mean Absolute Error)
          loss2 = torch.mean(torch.abs(inputs - targets[1]))

          # Combine the two conditions
          loss = self.weight1 * loss1 + self.weight2 * loss2
          return loss
#+end_src

#+RESULTS:

#+BEGIN_SRC ipython
  import torch
  import torch.nn as nn
  import torch.nn.functional as F

  class ClassifierRNN(nn.Module):
      def __init__(self, input_size, hidden_size, num_layers, output_size, device='cuda', dt=.01, noise=0.01, rank=2, alpha=0.0):
          super(ClassifierRNN, self).__init__()

          self.input_size = input_size
          self.hidden_size = hidden_size
          self.output_size = output_size

          self.num_layers = num_layers
          self.alpha = alpha
          # Weight matrices
          self.W_ih = nn.Parameter(torch.Tensor(hidden_size, input_size))
          self.W_hh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))
          # Bias terms
          self.b_ih = nn.Parameter(torch.Tensor(hidden_size))
          self.b_hh = nn.Parameter(torch.Tensor(hidden_size))
          # Low rank
          self.xi = nn.Parameter(torch.Tensor(hidden_size, rank))

          self.noise_std = torch.tensor(noise)

          self.linear = nn.Linear(hidden_size, 1)
          # Decay rate for the hidden state

          with torch.no_grad():
              self.decay_rate = torch.tensor(dt / 20)
              self.exp_rate = torch.exp(-self.decay_rate)

              self.decay_syn = torch.tensor(dt / 3)
              self.exp_syn = torch.exp(-self.decay_syn)

          # Initialize parameters
          self.reset_parameters()

      def reset_parameters(self):
          # Initialize weight and bias parameters using xavier initialization or another preferred method
          nn.init.xavier_uniform_(self.W_ih)
          nn.init.xavier_uniform_(self.W_hh)
          nn.init.zeros_(self.b_ih)
          nn.init.zeros_(self.b_hh)
          nn.init.normal_(self.xi)

      def forward(self, x):
          # x is of shape (batch_size, sequence_length, input_size)
          batch_size, seq_length, _ = x.size()

          rec_inputs = torch.zeros(batch_size, self.hidden_size, device=x.device)
          rates = torch.zeros(batch_size, self.hidden_size, device=x.device)
          noise = torch.randn(batch_size, seq_length, self.hidden_size, device=x.device)

          outputs = []
          for t in range(seq_length):

              ff_inputs = torch.mm(x[:, t], self.W_ih.t()) + self.b_ih

              full = torch.mm(rates, self.W_hh.t()) + self.b_hh
              lr = rates.matmul(self.xi).matmul(self.xi.t()) / self.hidden_size

              hidden = (1.0 - self.alpha) * full + self.alpha * lr
              rec_inputs = self.exp_syn * rec_inputs + self.decay_syn * hidden
              # rec_inputs = full + lr
              net_inputs = ff_inputs + rec_inputs + noise[:, t, :] * self.noise_std

              # Compute rates
              rates = nn.ReLU()(net_inputs)
              # rates = rates * self.exp_rate + self.decay_rate * nn.ReLU()(net_inputs)

              # Collect outputs
              outputs.append(rates.unsqueeze(1))

          # Concatenate outputs along the time dimension
          outputs = torch.cat(outputs, dim=1)
          y_pred = self.linear(outputs[:, -1, :])

          return torch.sigmoid(y_pred).squeeze(-1)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython
  # Example usage:
  input_size = 10
  hidden_size = 20
  num_layers = 1
  output_size = 10

  decay_rate = 0.1

  seq_length = 5
  batch_size = 3

  model = ClassifierRNN(input_size, hidden_size, num_layers, output_size, decay_rate)
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model.to(device)

  input_tensor = torch.randn(batch_size, seq_length, input_size)
  input_tensor = input_tensor.to(device)

  output = model(input_tensor)
  print(input_tensor.shape, output.shape)
#+END_SRC

#+RESULTS:
: torch.Size([3, 5, 10]) torch.Size([3])

** Rate

#+begin_src ipython
  class CustomCombinedLoss(nn.Module):
      def __init__(self, weight1=1.0, weight2=1.0):
          super(CustomCombinedLoss, self).__init__()
          self.weight1 = weight1
          self.weight2 = weight2
          # You could also include additional initializations for
          # each distinct condition if necessary.

      def forward(self, inputs, targets):
          # Condition 1 (for example, Mean Squared Error)
          loss1 = torch.mean((inputs - targets[0]) ** 2)

          # Condition 2 (for example, Mean Absolute Error)
          loss2 = torch.mean(torch.abs(inputs - targets[1]))

          # Combine the two conditions
          loss = self.weight1 * loss1 + self.weight2 * loss2
          return loss
#+end_src

#+RESULTS:

#+begin_src ipython
  covariance = torch.tensor([[1.0, self.LR_COV],
                             [self.LR_COV, 1.0],], dtype=self.FLOAT, device=self.device)


  multivariate_normal = MultivariateNormal(mean, covariance)
  self.ksi = multivariate_normal.sample((Nb,)).T

#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
: ---------------------------------------------------------------------------
: NameError                                 Traceback (most recent call last)
: Cell In[19], line 1
: ----> 1 covariance = torch.tensor([[1.0, self.LR_COV],
:       2                            [self.LR_COV, 1.0],], dtype=self.FLOAT, device=self.device)
:       5 multivariate_normal = MultivariateNormal(mean, covariance)
:       6 self.ksi = multivariate_normal.sample((Nb,)).T
:
: NameError: name 'self' is not defined
:END:

#+begin_src ipython
  ff_input = self.Ja0[0] * (1.0 + self.ksi[0] * self.I0[0] * self.M0)
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
: ---------------------------------------------------------------------------
: NameError                                 Traceback (most recent call last)
: Cell In[20], line 1
: ----> 1 ff_input = self.Ja0[0] * (1.0 + self.ksi[0] * self.I0[0] * self.M0)
:
: NameError: name 'self' is not defined
:END:


#+BEGIN_SRC ipython
  import torch
  import torch.nn as nn
  import torch.nn.functional as F

  class ClassifierRNN(nn.Module):
      def __init__(self, input_size, hidden_size, num_layers, output_size, device='cuda', dt=.01, noise=0.01, rank=2, alpha=0.0):
          super(ClassifierRNN, self).__init__()

          self.input_size = input_size
          self.hidden_size = hidden_size
          self.output_size = output_size

          self.num_layers = num_layers
          self.alpha = alpha
          # Weight matrices
          self.W_ih = nn.Parameter(torch.Tensor(hidden_size, input_size))
          self.W_hh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))
          # Bias terms
          self.b_ih = nn.Parameter(torch.Tensor(hidden_size))
          self.b_hh = nn.Parameter(torch.Tensor(hidden_size))
          # Low rank
          self.xi = nn.Parameter(torch.Tensor(hidden_size, rank))

          self.noise_std = torch.tensor(noise)

          self.linear = nn.Linear(hidden_size, 1)
          # Decay rate for the hidden state

          with torch.no_grad():
              self.decay_rate = torch.tensor(dt / 20)
              self.exp_rate = torch.exp(-self.decay_rate)

              self.decay_syn = torch.tensor(dt / 3)
              self.exp_syn = torch.exp(-self.decay_syn)

          # Initialize parameters
          self.reset_parameters()

      def reset_parameters(self):
          # Initialize weight and bias parameters using xavier initialization or another preferred method
          nn.init.xavier_uniform_(self.W_ih)
          nn.init.xavier_uniform_(self.W_hh)
          nn.init.zeros_(self.b_ih)
          nn.init.zeros_(self.b_hh)
          nn.init.normal_(self.xi)

      def forward(self, x):
          # x is of shape (batch_size, sequence_length, input_size)
          batch_size, seq_length, _ = x.size()

          rec_inputs = torch.zeros(batch_size, self.hidden_size, device=x.device)
          rates = torch.zeros(batch_size, self.hidden_size, device=x.device)
          noise = torch.randn(batch_size, seq_length, self.hidden_size, device=x.device)

          outputs = []
          for t in range(seq_length):

              ff_inputs = torch.mm(x[:, t], self.W_ih.t()) + self.b_ih

              full = torch.mm(rates, self.W_hh.t()) + self.b_hh
              lr = rates.matmul(self.xi).matmul(self.xi.t()) / self.hidden_size

              hidden = (1.0 - self.alpha) * full + self.alpha * lr
              rec_inputs = self.exp_syn * rec_inputs + self.decay_syn * hidden
              # rec_inputs = full + lr
              net_inputs = ff_inputs + rec_inputs + noise[:, t, :] * self.noise_std

              # Compute rates
              rates = nn.ReLU()(net_inputs)
              # rates = rates * self.exp_rate + self.decay_rate * nn.ReLU()(net_inputs)

              # Collect outputs
              outputs.append(rates.unsqueeze(1))

          # Concatenate outputs along the time dimension
          outputs = torch.cat(outputs, dim=1)
          y_pred = self.linear(outputs[:, -1, :])

          return torch.sigmoid(y_pred).squeeze(-1)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython
  # Example usage:
  input_size = 10
  hidden_size = 20
  num_layers = 1
  output_size = 10

  decay_rate = 0.1

  seq_length = 5
  batch_size = 3

  model = ClassifierRNN(input_size, hidden_size, num_layers, output_size, decay_rate)
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model.to(device)

  input_tensor = torch.randn(batch_size, seq_length, input_size)
  input_tensor = input_tensor.to(device)

  output = model(input_tensor)
  print(input_tensor.shape, output.shape)
#+END_SRC

#+RESULTS:
: torch.Size([3, 5, 10]) torch.Size([3])

** Multivariate

#+BEGIN_SRC ipython
  import torch
  import torch.nn as nn
  import torch.nn.functional as F

  class MultivariateRNN(nn.Module):
      def __init__(self, input_size, hidden_size, num_layers, output_size, device='cuda', dt=.01, noise=0.01, rank=2, alpha=0.5):
          super(MultivariateRNN, self).__init__()

          self.input_size = input_size
          self.hidden_size = hidden_size
          self.output_size = output_size
          self.alpha = alpha
          self.num_layers = num_layers

          # Weight matrices
          self.W_ih = nn.Parameter(torch.Tensor(hidden_size, input_size))
          self.W_hh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))
          # Bias terms
          self.b_ih = nn.Parameter(torch.Tensor(hidden_size))
          self.b_hh = nn.Parameter(torch.Tensor(hidden_size))
          self.xi = nn.Parameter(torch.Tensor(hidden_size, rank))

          self.noise_std = torch.tensor(noise)

          self.fc = nn.Linear(hidden_size, output_size, bias=False)
          # Decay rate for the hidden state

          with torch.no_grad():
              self.decay_rate = torch.tensor(dt / 20)
              self.exp_rate = torch.exp(-self.decay_rate)

              self.decay_syn = torch.tensor(dt / 3)
              self.exp_syn = torch.exp(-self.decay_syn)

          # Initialize parameters
          self.reset_parameters()

      def reset_parameters(self):
          # Initialize weight and bias parameters using xavier initialization or another preferred method
          nn.init.xavier_uniform_(self.W_ih)
          nn.init.xavier_uniform_(self.W_hh)
          nn.init.zeros_(self.b_ih)
          nn.init.zeros_(self.b_hh)
          nn.init.normal_(self.xi)

      def forward(self, x):
          # x is of shape (batch_size, sequence_length, input_size)
          batch_size, seq_length, _ = x.size()

          rec_inputs = torch.zeros(batch_size, self.W_hh.size(0), device=x.device)
          rates = torch.zeros(batch_size, self.W_hh.size(0), device=x.device)
          noise = torch.randn(batch_size, seq_length, self.W_hh.size(0), device=x.device)

          outputs = []
          for t in range(seq_length):

              ff_inputs = torch.mm(x[:, t], self.W_ih.t()) + self.b_ih
              hidden = torch.mm(rates, self.W_hh.t()) + self.b_hh

              full = torch.mm(rates, self.W_hh.t()) + self.b_hh
              lr = rates.matmul(self.xi).matmul(self.xi.t()) / self.hidden_size

              hidden = (1.0 - self.alpha) * full + self.alpha * lr

              rec_inputs = self.exp_syn * rec_inputs + self.decay_syn * hidden
              # rec_inputs = hidden

              net_inputs = ff_inputs + rec_inputs + noise[:, t, :] * self.noise_std

              # Compute rates
              rates = nn.ReLU()(net_inputs)
              # rates = rates * self.exp_rate + self.decay_rate * nn.ReLU()(net_inputs)

              # Collect outputs
              outputs.append(self.fc(rates.unsqueeze(1)))

          # Concatenate outputs along the time dimension
          outputs = torch.cat(outputs, dim=1)

          return outputs
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython
  # Example usage:
  input_size = 10
  hidden_size = 20
  num_layers = 1
  output_size = 10

  decay_rate = 0.1

  seq_length = 5
  batch_size = 3

  model = MultivariateRNN(input_size, hidden_size, num_layers, output_size, decay_rate)
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model.to(device)

  input_tensor = torch.randn(batch_size, seq_length, input_size)
  input_tensor = input_tensor.to(device)

  output = model(input_tensor)
  print(input_tensor.shape, output.shape)
#+END_SRC

#+RESULTS:
: torch.Size([3, 5, 10]) torch.Size([3, 5, 10])

** Low rank

#+begin_src ipython
  class LRRNN(nn.Module):
      def __init__(self, N_NEURON, N_BATCH, RANK=2, DT=0.05, TAU=1, SIGMA=0.001, NONLINEAR='sig', DEVICE='cuda', DROP=0.5):
          super(LRRNN, self).__init__()

          self.N_BATCH = N_BATCH
          self.DEVICE = DEVICE

          self.N_NEURON = N_NEURON
          self.RANK = RANK

          self.DT = DT
          self.TAU = TAU
          self.GAIN = nn.Parameter(torch.tensor(1.0).to(DEVICE))

          self.SIGMA = nn.Parameter(torch.tensor(SIGMA).to(DEVICE))
          self.EXP_DT_TAU = torch.exp(-torch.tensor(self.DT / self.TAU).to(DEVICE))
          self.DT_TAU = torch.tensor(self.DT / self.TAU).to(DEVICE)

          self.dropout = nn.Dropout(DROP)

          if NONLINEAR == 'relu':
              self.Activation = nn.ReLU()
          else:
              self.Activation = nn.Tanh()

          # self.U = nn.Parameter(
          #     torch.randn((self.N_NEURON, int(self.RANK)), device=self.DEVICE)
          # )

          # self.V = nn.Parameter(
          #     torch.randn((self.N_NEURON, int(self.RANK)), device=self.DEVICE)
          # )

          self.Wab = nn.Parameter(torch.randn((self.N_NEURON, self.N_NEURON),
                                              device=self.device)* 0.001)

          # self.linear = nn.Linear(self.N_NEURON, self.N_NEURON, device=DEVICE)

      def update_dynamics(self, rates, ff_input, rec_input, lr):
          noise = torch.randn_like(rates)

          # update hidden state
          hidden = rates @ lr

          rec_input = rec_input * self.EXP_DT_TAU + hidden * self.DT_TAU # + noise * torch.sqrt(self.SIGMA * self.DT_TAU)

          # compute net input
          net_input = ff_input + rec_input + noise * self.SIGMA

          # update rates
          # non_linear = self.Activation(net_input)
          # rates = rates * self.EXP_DT_TAU + non_linear * self.DT_TAU + noise

          # rates = self.GAIN * self.Activation(net_input)
          rates = self.GAIN * self.Activation(net_input)
          # rates = self.linear(self.Activation(net_input))
          # rates = net_input

          return rates, rec_input

      def forward(self, ff_input):

          # initialize state
          rates = torch.zeros(ff_input.size(0), self.N_NEURON, device=self.DEVICE)
          rec_input = torch.zeros(ff_input.size(0), self.N_NEURON, device=self.DEVICE)
          # lr = self.U @ self.V.T / self.N_NEURON
          lr = self.Wab
          # print('ff_input', ff_input.shape, 'rates', rates.shape, 'lr', lr.shape)

          rates_sequence = []
          for step in range(ff_input.size(1)):
              rates, rec_input = self.update_dynamics(rates, ff_input[:, step], rec_input, lr)
              rates_sequence.append(rates.unsqueeze(1))

          rates_sequence = torch.cat(rates_sequence, dim=1)

          return rates_sequence
#+end_src

#+RESULTS:

#+begin_src ipython

#+end_src

#+RESULTS:

* Train on Experimental Data
** Imports

#+begin_src ipython
  import sys
  sys.path.insert(0, '../')

  from src.common.get_data import get_X_y_days, get_X_y_S1_S2
  from src.common.options import set_options
#+end_src

#+RESULTS:

** Parameters

#+begin_src ipython
  mice = ['ChRM04','JawsM15', 'JawsM18', 'ACCM03', 'ACCM04']
  tasks = ['DPA', 'DualGo', 'DualNoGo']
  days = ['first', 'last']

  kwargs = dict()
  kwargs = {'trials': '', 'preprocess': None, 'scaler_BL': 'standard', 'avg_noise':True, 'unit_var_BL':False}

  kwargs['mouse'] = 'JawsM15'
#+end_src

#+RESULTS:

** Load Data

#+begin_src ipython
  options = set_options(**kwargs)
  options['reload'] = False
  options['data_type'] = 'dF'
  options['DCVL'] = 0
#+end_src

#+RESULTS:

#+begin_src ipython
  X_days, y_days = get_X_y_days(**options)
  options['day'] = 'last'
  options['task'] = 'DPA'
  X_data, y_data = get_X_y_S1_S2(X_days, y_days, **options)

  # y_data = y_data[:, np.newaxis]
  print(X_data.shape, y_data.shape)
#+end_src

#+RESULTS:
: (96, 693, 84) (96, 13)

#+begin_src ipython
from  mne.decoding import Scaler
std_scaler = Scaler(scalings='mean')
# X_data = std_scaler.fit_transform(X_data)
print(X_data.shape)
#+end_src

#+RESULTS:
: (96, 693, 84)

#+begin_src ipython
  import numpy as np
  from scipy.ndimage import convolve1d

  def moving_average_multidim(data, window_size, axis=-1):
      """
      Apply a 1D moving average across a specified axis of a multi-dimensional array.

      :param data: multi-dimensional array of data
      :param window_size: size of the moving window
      :param axis: axis along which to apply the moving average
      :return: smoothed data with the same shape as input data
      """
      # Create a moving average filter window
      window = np.ones(window_size) / window_size
      # Apply 1D convolution along the specified axis
      smoothed_data = convolve1d(data, weights=window, axis=axis, mode='reflect')
      return smoothed_data

#+end_src

#+RESULTS:

#+begin_src ipython
from src.decode.bump import circcvl
# smoothed_data = circcvl(X_data, windowSize=2, axis=-1)
print(X_data.shape)
window_size = 6
# from scipy.ndimage import gaussian_filter1d
# smoothed_data = gaussian_filter1d(X_data, axis=-1, sigma=2)
# smoothed_data = moving_average_multidim(X_data[..., :52], window_size, axis=-1)
smoothed_data = moving_average_multidim(X_data, window_size, axis=-1)
#+end_src

#+RESULTS:
: (96, 693, 84)

#+begin_src ipython
import numpy as np

def rescale_to_minus_one_to_one(data):
    """
    Rescale a 3D NumPy array to be between -1 and 1.

    Parameters:
    data (np.ndarray): Input data of shape (N_trials, N_neurons, N_time).

    Returns:
    np.ndarray: Rescaled data with the same shape as input.
    """
    data_min = np.min(data)
    data_max = np.max(data)

    # Avoid division by zero if min and max are equal
    if data_min == data_max:
        return np.zeros_like(data)

    # Rescale data
    normalized_data = 2 * ((data - data_min) / (data_max - data_min)) - 1
    return normalized_data
#+end_src

#+RESULTS:

#+begin_src ipython
  time = np.linspace(0, 14, 84)
  for i in range(10):
      i = np.random.randint(100)
      plt.plot(time, smoothed_data[-1, i,:], alpha=.5)

  plt.ylabel('Rate (Hz)')
  plt.xlabel('Time (s)')
  plt.show()
#+end_src

#+RESULTS:
[[./.ob-jupyter/7c4eeb0b44b7b27b2f262ecd64b558965af41d3c.png]]

** Training

#+begin_src ipython
  # y = np.roll(X_data, -1)
  # y = y[..., :-1]

  X = smoothed_data[..., :-1]
  Y = smoothed_data[..., 1:]

  # X = rescale_to_minus_one_to_one(X)
  # Y = rescale_to_minus_one_to_one(Y)

  X = np.swapaxes(X, 1, -1)
  Y = np.swapaxes(Y, 1, -1)

  print(X.shape, Y.shape)
#+end_src

#+RESULTS:
: (96, 83, 693) (96, 83, 693)

#+begin_src ipython
X = torch.tensor(X, dtype=torch.float32, device=device)
Y = torch.tensor(Y, dtype=torch.float32, device=device)
print(X.shape, Y.shape)
#+end_src

#+RESULTS:

#+RESULTS:

#+begin_src ipython
  # y_data[y_data==-1] = 0
  # Y = torch.tensor(y_data, dtype=torch.float32, device=device)
  # print(Y.shape)
#+end_src

#+RESULTS:

#+begin_src ipython
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

hidden_size = 693
num_layers = 1
num_features = X.shape[-1]

batch_size = 16
train_loader, val_loader = split_data(X, Y, train_perc=0.8, batch_size=batch_size)
#+end_src


#+RESULTS:

#+begin_src ipython
model = LRRNN(N_NEURON=num_features, N_BATCH=batch_size, DEVICE=device, RANK=2)

learning_rate = 0.05
num_epochs = 100
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

run_optim(model, train_loader, val_loader, criterion, optimizer, num_epochs, thresh=.0002, penalty=None, lbd=1)
#+end_src

#+RESULTS:
: 778242c3-fd95-4384-a301-0d9fa028fb80

** Generate series

#+begin_src ipython
  from sklearn.metrics import mean_squared_error

  model.eval()  # Set the model to evaluation mode

  # This function feeds inputs through the model and computes the predictions
  def get_predictions(data_loader):
      predictions = []
      ground_truth = []
      with torch.no_grad():  # Disable gradient computation for evaluation
          for inputs, targets in data_loader:
              inputs, targets = inputs.to(device), targets.to(device)
              outputs = model(inputs)
              predictions.append(outputs.cpu())  # If using cuda, need to move data to cpu
              ground_truth.append(targets.cpu())

      # Concatenate all batches
      predictions = torch.cat(predictions, dim=0)
      ground_truth = torch.cat(ground_truth, dim=0)

      return predictions, ground_truth

  # Call the function using your data loader
  predictions, ground_truth = get_predictions(val_loader)

  print(ground_truth.numpy().shape, predictions.numpy().shape)
  # Calculate the loss or performance metric
  # For example, we can use the Mean Squared Error
  # error = mean_squared_error(ground_truth.numpy(), predictions.numpy())
  # print(f"Mean Squared Error: {error}")
#+end_src

#+RESULTS:
: f49f852a-6ad7-45a8-98e8-651acca52aec

#+begin_src ipython
  import matplotlib.pyplot as plt

  # Assuming predictions and ground_truth are for a single batch or example:
  # predictions: tensor of shape (batch_size, sequence_length, output_size)
  # ground_truth: tensor of shape (batch_size, sequence_length, output_size)

  # Convert tensors to numpy arrays for plotting
  predictions_np = predictions.numpy()
  ground_truth_np = ground_truth.numpy()

  # Plot the predictions on top of the ground truth
  plt.figure()
  pal = sns.color_palette("tab10")
  time = np.linspace(0, 14, 84)[:-1]
  # Example for plotting the first feature dimension
  for i in range(3):
     j = np.random.randint(model.U.shape[0])
     plt.plot(time, ground_truth_np[0, :, j], 'x', label='Ground Truth', color=pal[i], alpha=.2)
     plt.plot(time, predictions_np[0, :, j], '-', label='Model Prediction', color=pal[i], alpha=1)

  # You can loop through more feature dimensions if needed
  # for i in range(output_size):
     # plt.plot(ground_truth_np[0, :, i], label=f'Ground Truth Feature {i}', marker='.')
     # plt.plot(predictions_np[0, :, i], label=f'Prediction Feature {i}', marker='x')

  plt.title("Model Prediction vs Ground Truth")
  plt.xlabel("Time steps")
  plt.ylabel("Value")
  # plt.legend(fontsize=12)
  plt.show()
#+end_src

#+RESULTS:
: 6ca3e9a3-97f6-4217-b709-ed54b25e6e70

#+begin_src ipython
Wij = model.U @ model.V.T
print(Wij.shape)
#+end_src

#+RESULTS:
: bb54c27a-c7fa-4fa1-9512-42effdc19819

#+begin_src ipython
U = model.U.cpu().detach().T
V = model.V.cpu().detach().T
print(U.shape, V.shape)
#+end_src

#+RESULTS:
: fac8e2ab-1fc6-4126-94ca-c34692f02c7e

#+begin_src ipython
fig, ax = plt.subplots(1, 2, figsize= [2 * width, height])
ax[0].scatter(U[0], U[1])
ax[1].scatter(V[0], V[1])
plt.show()
#+end_src

#+RESULTS:
: 3d5b4300-05f8-4be7-9380-1bd25171fc2b

#+begin_src ipython
  X_days, y_days = get_X_y_days(**options)
  options['day'] = 6
  options['task'] = 'DPA'
  X_data, y_data = get_X_y_S1_S2(X_days, y_days, **options)

  print(X_data.shape)
  # X_data = std_scaler.transform(X_data)
  X_data = np.swapaxes(X_data, 1, -1)
  # y_data = y_data[:, np.newaxis]
  print(X_data.shape, y_data.shape)
#+end_src

#+RESULTS:
: b287e270-a14c-4b98-b2b8-49d00799eab7

#+begin_src ipython
def add_vlines(ax=None, mouse=""):
    t_BL = [0, 2]
    t_STIM = [2 , 3]
    t_ED = [3 , 4.5]
    t_DIST = [4.5 , 5.5]
    t_MD = [5.5 , 6.5]
    t_CUE = [6.5 , 7]
    t_RWD = [7 , 7.5]
    t_LD = [7.5 , 9]
    t_TEST = [9 , 10]
    t_RWD2 = [11 , 12]

    if "P" in mouse:
        t_BL = [0 , 2]
        t_STIM = [2 , 3]
        t_ED = [3 , 4]
        t_DIST = [4 , 5]
        t_MD = [5 , 6]
        t_CUE = [6 , 7]
        t_RWD = [7 , 8]
        t_LD = [8 , 9]
        t_TEST = [9 , 10]
        t_RWD2 = [10.5 , 11]

    time_periods = [t_STIM, t_DIST, t_TEST, t_CUE, t_RWD2, t_RWD]
    colors = ["b", "b", "b", "g", "y", "y"]

    if ax is None:
        for period, color in zip(time_periods, colors):
            plt.axvspan(period[0], period[1], alpha=0.05, color=color)
    else:
        for period, color in zip(time_periods, colors):
            ax.axvspan(period[0], period[1], alpha=0.05, color=color)
#+end_src

#+RESULTS:
: 265a9fd5-a387-4c45-8d92-c8d53223989d

#+begin_src ipython
overlap = torch.tensor(X_data).to(torch.float).to(device) @ torch.tensor(U).to(torch.float).to(device)

fig, ax = plt.subplots(1, 2, figsize= [2 * width, height])

ax[0].plot(np.linspace(0, 14, 84), overlap[:8, :, 0].cpu().detach().mean(0) , 'r')
ax[0].plot(np.linspace(0, 14, 84), overlap[8:16, :, 0].cpu().detach().mean(0), 'r--')

ax[0].plot(np.linspace(0, 14, 84), overlap[16:24, :, 0].cpu().detach().mean(0), 'b')
ax[0].plot(np.linspace(0, 14, 84), overlap[24:32, :, 0].cpu().detach().mean(0), 'b--')
ax[0].axhline(0, ls='--', color='k')

ax[1].plot(np.linspace(0, 14, 84), overlap[:8, :, 1].cpu().detach().mean(0), 'r')
ax[1].plot(np.linspace(0, 14, 84), overlap[8:16, :, 1].cpu().detach().mean(0), 'r--')

ax[1].plot(np.linspace(0, 14, 84), overlap[16:24, :, 1].cpu().detach().mean(0), 'b')
ax[1].plot(np.linspace(0, 14, 84), overlap[24:32, :, 1].cpu().detach().mean(0), 'b--')
ax[1].axhline(0, ls='--', color='k')

add_vlines(ax[0])
add_vlines(ax[1])
plt.show()
#+end_src

#+RESULTS:
: e7055923-dbba-47a5-9919-e9bc237485eb

#+begin_src ipython
overlap = torch.tensor(X_data).to(torch.float).to(device) @ torch.tensor(Vt.T).to(torch.float).to(device)
#overlap = torch.tensor(X_data).to(torch.float).to(device) @ model.V

fig, ax = plt.subplots(1, 2, figsize= [2 * width, height])

ax[0].plot(np.linspace(0, 14, 84), overlap[:8, :, 0].cpu().detach().mean(0) , 'r')
ax[0].plot(np.linspace(0, 14, 84), overlap[8:16, :, 0].cpu().detach().mean(0), 'r--')

ax[0].plot(np.linspace(0, 14, 84), overlap[16:24, :, 0].cpu().detach().mean(0), 'b')
ax[0].plot(np.linspace(0, 14, 84), overlap[24:32, :, 0].cpu().detach().mean(0), 'b--')
ax[0].axhline(0, ls='--', color='k')

ax[1].plot(np.linspace(0, 14, 84), overlap[:8, :, 1].cpu().detach().mean(0), 'r')
ax[1].plot(np.linspace(0, 14, 84), overlap[8:16, :, 1].cpu().detach().mean(0), 'r--')

ax[1].plot(np.linspace(0, 14, 84), overlap[16:24, :, 1].cpu().detach().mean(0), 'b')
ax[1].plot(np.linspace(0, 14, 84), overlap[24:32, :, 1].cpu().detach().mean(0), 'b--')
ax[1].axhline(0, ls='--', color='k')

add_vlines(ax[0])
add_vlines(ax[1])
plt.show()
#+end_src

#+RESULTS:
: 40d8889c-1d92-4bf3-b2ac-40b71d045145

#+begin_src ipython

#+end_src

#+RESULTS:
: 71d4ed36-fe41-4a94-af5a-105a9e0ade5c

* Reverse Engineering
** Low rank

#+begin_src ipython
  print(model.U.shape, model.U.shape)
  UdotV = model.V.T @ model.U
  print(UdotV.detach().cpu().numpy())
#+end_src

#+RESULTS:
: torch.Size([693, 3]) torch.Size([693, 3])
: [[-23.906015   15.751429  -10.570989 ]
:  [ 18.55918   -13.707779    9.39692  ]
:  [  4.5675526  -7.132879   -0.5639941]]

#+begin_src ipython
  def angle_AB(A, B):
      A_norm = A / (np.linalg.norm(A) + 1e-5)
      B_norm = B / (np.linalg.norm(B) + 1e-5)

      return int(np.arccos(A_norm @ B_norm) * 180 / np.pi)
#+end_src

#+RESULTS:

#+begin_src ipython
  print(angle_AB(model.U[:, 0].detach().cpu().numpy(), model.U[:, 1].detach().cpu().numpy()))
  # print(angle_AB(model.U[:, 1].detach().cpu().numpy(), model.V[:, 1].detach().cpu().numpy()))
#+end_src

#+RESULTS:
: 155

#+begin_src ipython
  options['task'] = 'DPA'
  options['features'] = 'sample'
  print(options['day'])

  X_data, y_data = get_X_y_S1_S2(X_days, y_days, **options)
  X_data = np.swapaxes(X_data, 1, -1)

  print('X', X_data.shape, 'U', model.U.shape)
  fig, ax = plt.subplots(1, 2, figsize= [1.5 * width, height])

  U_proj = -(X_data @ model.U.detach().cpu().numpy()) * 100
  # V_proj = -(X_data @ model.V.detach().cpu().numpy()) * 100
  print('proj', U_proj.shape)

  idx = np.where(y_data==1)[0]
  # print('idx', idx.shape)

  ax[0].plot(U_proj[idx].mean(0)[..., 0], label='A')
  ax[1].plot(U_proj[idx].mean(0)[..., 1], label='A')

  idx = np.where(y_data==-1)[0]
  # print('idx', idx.shape)

  ax[0].plot(U_proj[idx].mean(0)[..., 0], label='B')
  ax[1].plot(U_proj[idx].mean(0)[..., 1], label='B')

  ax[0].set_ylabel('Axis U')
  ax[1].set_ylabel('Axis V')
  plt.legend(fontsize=10)
  plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: 6
: X (32, 84, 693) U torch.Size([693, 3])
: proj (32, 84, 3)
: /home/leon/tmp/ipykernel_1821654/2333964251.py:24: RuntimeWarning: Mean of empty slice.
:   ax[0].plot(U_proj[idx].mean(0)[..., 0], label='B')
: /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/numpy/core/_methods.py:121: RuntimeWarning: invalid value encountered in divide
:   ret = um.true_divide(
: /home/leon/tmp/ipykernel_1821654/2333964251.py:25: RuntimeWarning: Mean of empty slice.
:   ax[1].plot(U_proj[idx].mean(0)[..., 1], label='B')
[[./.ob-jupyter/8129dd6faf7a9843b3d67a0ff2a3c83c521e877a.png]]
:END:

#+begin_src ipython

#+end_src

#+RESULTS:

* Connectivity

#+begin_src ipython
  # weights = model.rnn.weight_hh_l0.data.cpu().numpy()  # Get the recurring weights of the RNN
  # weights = model.W_hh.cpu().detach().numpy()
  weights = (model.U @ model.V.T).cpu().detach().numpy()
  print(weights.shape)
  # Perform singular value decomposition<
  U, S, Vt = np.linalg.svd(weights)

  u1, u2, u3 = U[:, 0], U[:, 1], U[:, 2]  # First two left singular vectors
  v1, v2, v3 = Vt[0, :], Vt[1, :], Vt[2, :]  # First two right singular vectors
#+end_src

#+RESULTS:
: (693, 693)

#+begin_src ipython
print(U.shape)
#+end_src

#+RESULTS:
: (693, 693)

#+begin_src ipython
ksi1 = S[0] * u1 * v1
ksi2 = S[1] * u2 * v2
ksi3 = S[2] * u3 * v3
print(ksi1.shape)
#+end_src

#+RESULTS:
: (693,)

#+begin_src ipython
  plt.imshow(weights, cmap='jet')
  plt.show()
#+end_src

#+RESULTS:
[[./.ob-jupyter/926d8a8f957ac80d0aba380a81501053dbce73c8.png]]

#+begin_src ipython
print(S[:10])
#+end_src

#+RESULTS:
: [7.3551251e+02 5.0888651e+02 3.8313074e-06 3.6247329e-06 3.4779725e-06
:  3.2987502e-06 3.2684429e-06 3.1708664e-06 3.1461432e-06 3.0895558e-06]

#+begin_src ipython
  theta = np.arctan2(ksi2, ksi1)
  index = theta.argsort()
  print(index.shape)
#+end_src

#+RESULTS:
: (693,)

#+begin_src ipython
  plt.hist(theta*180/np.pi, bins='auto', density=True)
  plt.ylabel('Density')
  plt.xlabel('$\\theta$ (Â°)')
  plt.show()
#+end_src

#+RESULTS:
[[./.ob-jupyter/7caf91cfa5e8f200f857c20dedb986ca13bcd280.png]]

#+begin_src ipython
  plt.scatter(ksi1, ksi2)
  plt.xlabel('$\\xi_{1}$')
  plt.ylabel('$\\xi_{2}$')
  plt.show()
#+end_src

#+RESULTS:
[[./.ob-jupyter/8512e1181f2238ad7784dea5c69ac3c8b4176f29.png]]

#+begin_src ipython
  Jij = weights[index][index]
  print(Jij.shape)
#+end_src

#+RESULTS:
: (693, 693)
#+RESULTS:

#+begin_src ipython
  plt.imshow(Jij, cmap='jet')
  plt.show()
#+end_src

#+RESULTS:
[[./.ob-jupyter/d5d80d268d9f880eadddc8bc6c006c1ecad15ecc.png]]

#+begin_src ipython
  # Plot the singular values
  plt.figure(figsize=(10, 5))
  plt.plot(S)
  plt.yscale('log')  # Log scale can be helpful to see the drop-off more clearly
  plt.title('Singular Values of the Weight Matrix')
  plt.ylabel('Singular values (log scale)')
  plt.xlabel('Index')
  plt.grid(True)
  plt.show()

  # To see the cumulative energy, plot the cumulative sum of squares of singular values
  cumulative_energy = np.cumsum(S*2) / np.sum(S*2)
  plt.figure(figsize=(10, 5))
  plt.plot(cumulative_energy)
  plt.title('Cumulative Sum of Squares of Singular Values')
  plt.ylabel('Cumulative energy')
  plt.xlabel('Index')
  plt.grid(True)
  plt.show()

#+end_src

#+RESULTS:
:RESULTS:
[[./.ob-jupyter/0cbd41f0fcb57c0b31d5d1640490b8f6e136e087.png]]
[[./.ob-jupyter/8cc766218d7aadc44d290612099c1974dfe45058.png]]
:END:

* Train on synthetic data
*** Create synthetic data

#+begin_src ipython
  num_series = 128  # Number of time series samples to generate
  num_steps = 84  # Number of time steps in each time series
  num_features = 100  # Number of features (signals) in each time series

  # Generate synthetic data
  synthetic_data = generate_multivariate_time_series(num_series, num_steps, num_features)

  # Split the data into inputs (X) and targets (Y), e.g., use previous timesteps to predict the next timestep
  X = synthetic_data[:, :-1, :]  # Using all but the last timestep as input
  Y = synthetic_data[:, 1:, :]   # Using all but the first timestep as target (shifted by one)

  print("Input shape:", X.shape)
  print("Target shape:", Y.shape)
#+end_src

#+RESULTS:
: Input shape: torch.Size([128, 83, 100])
: Target shape: torch.Size([128, 83, 100])

#+begin_src ipython
  plt.plot(np.arange(0, num_steps, 180), np.sin(num_steps))
  plt.plot(X.cpu().numpy()[0,:,2], alpha=1)
  plt.plot(X.cpu().numpy()[3,:,0], alpha=1, color='r')
  plt.show()
#+end_src

#+RESULTS:
[[./.ob-jupyter/4ec29a7710e2ba5a441ddf5b12b4b74c011fc6f7.png]]


#+RESULTS:


#+begin_src ipython
batch_size = 32
train_loader, val_loader = split_data(X, Y, train_perc=0.8, batch_size=batch_size)
#+end_src

#+RESULTS:
: X_train shape: torch.Size([102, 83, 100]) Y_train shape: torch.Size([102, 83, 100])
: X_test shape: torch.Size([26, 83, 100]) Y_test shape: torch.Size([26, 83, 100])

*** Train model

#+begin_src ipython
  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

  hidden_size = 100
  num_layers = 1
  num_epochs = 100
  learning_rate = 0.01

  criterion = nn.MSELoss()
  optimizer = optim.Adam(model.parameters(), lr=learning_rate)
  model = LRRNN(N_NEURON=num_features, N_BATCH=batch_size, DEVICE=device, RANK=2)

  num_epochs = 100
  run_optim(model, train_loader, val_loader, criterion, optimizer, num_epochs)
#+end_src

#+RESULTS:
#+begin_example
Epoch 0/100, Training Loss: 0.0239, Validation Loss: 0.0238
/home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Epoch 10/100, Training Loss: 0.0240, Validation Loss: 0.0238
Epoch 20/100, Training Loss: 0.0240, Validation Loss: 0.0239
Epoch 30/100, Training Loss: 0.0239, Validation Loss: 0.0238
Epoch 40/100, Training Loss: 0.0238, Validation Loss: 0.0239
Epoch 50/100, Training Loss: 0.0238, Validation Loss: 0.0239
Epoch 60/100, Training Loss: 0.0238, Validation Loss: 0.0238
Epoch 70/100, Training Loss: 0.0238, Validation Loss: 0.0239
Epoch 80/100, Training Loss: 0.0240, Validation Loss: 0.0238
Epoch 90/100, Training Loss: 0.0238, Validation Loss: 0.0239
#+end_example

*** See data

#+begin_src ipython
  from sklearn.metrics import mean_squared_error

  model.eval()  # Set the model to evaluation mode

  # This function feeds inputs through the model and computes the predictions
  def get_predictions(data_loader):
      predictions = []
      ground_truth = []
      with torch.no_grad():  # Disable gradient computation for evaluation
          for inputs, targets in data_loader:
              inputs, targets = inputs.to(device), targets.to(device)
              outputs = model(inputs)
              predictions.append(outputs.cpu()) # If using cuda, need to move data to cpu
              ground_truth.append(targets.cpu())

      # Concatenate all batches
      predictions = torch.cat(predictions, dim=0)
      ground_truth = torch.cat(ground_truth, dim=0)

      return predictions, ground_truth

  # Call the function using your data loader
  predictions, ground_truth = get_predictions(val_loader)

  print(ground_truth.numpy().shape, predictions.numpy().shape)
  # Calculate the loss or performance metric
  # For example, we can use the Mean Squared Error
  # error = mean_squared_error(ground_truth.numpy(), predictions.numpy())
  # print(f"Mean Squared Error: {error}")
#+end_src

#+RESULTS:
: (26, 83, 100) (26, 83, 100)
