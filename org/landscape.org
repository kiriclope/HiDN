#+STARTUP: fold
#+PROPERTY: header-args:ipython :results both :exports both :async yes :session landscape :kernel dual_data :exports results :output-dir ./figures

#+begin_src emacs-lisp
(defun my/org-babel-tangle-figure-filename ()
  "Generate a comprehensive figure filename based on org entry name and block position."
  (let* ((name (or (org-entry-get nil "name") "figure"))
         (block-counter (let ((counter 1))
                          (save-excursion
                            (goto-char (point-min))
                            (while (re-search-forward "^[ \t]*#\\+begin_src" (point-max) t)
                              (when (< (point) (point-marker))
                                (setq counter (1+ counter)))))
                          counter))
         (filename (concat name "_" (number-to-string block-counter) ".png")))
    filename))
#+end_src

#+RESULTS:
: my/org-babel-tangle-figure-filename


#+BEGIN_SRC ipython :file (my/org-babel-tangle-figure-filename)
    x = np.linspace(-1, 1, 100)
    plt.plot(x, np.cos(x))
    plt.show()
#+end_src

#+RESULTS:
[[./figures/figure_1.png]]

#+BEGIN_SRC ipython :file (my/org-babel-tangle-figure-filename)
    x = np.linspace(-1, 1, 100)
    plt.plot(x, np.cos(x))
    plt.show()
#+end_src

#+RESULTS:
[[./figures/figure_1.png]]

* Notebook Settings

#+begin_src ipython
%load_ext autoreload
%autoreload 2
%reload_ext autoreload

%run /home/leon/dual_task/dual_data/notebooks/setup.py
%matplotlib inline
%config InlineBackend.figure_format = 'png'
#+end_src

#+RESULTS:
: The autoreload extension is already loaded. To reload it, use:
:   %reload_ext autoreload
: Python exe
: /home/leon/mambaforge/envs/dual_data/bin/python

* Imports

#+begin_src ipython
  from sklearn.exceptions import ConvergenceWarning
  warnings.filterwarnings("ignore")

  import sys
  sys.path.insert(0, '/home/leon/dual_task/dual_data/')

  import os
  if not sys.warnoptions:
    warnings.simplefilter("ignore")
    os.environ["PYTHONWARNINGS"] = "ignore"

  import pickle as pkl
  import numpy as np
  import matplotlib.pyplot as plt
  from time import perf_counter

  import torch
  import torch.nn as nn
  import torch.optim as optim
  from skorch import NeuralNetClassifier

  from sklearn.base import clone
  from sklearn.metrics import make_scorer, roc_auc_score
  from sklearn.ensemble import BaggingClassifier
  from sklearn.preprocessing import StandardScaler, RobustScaler
  from sklearn.pipeline import Pipeline
  from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold, LeaveOneOut
  from sklearn.decomposition import PCA

  from mne.decoding import SlidingEstimator, cross_val_multiscore, GeneralizingEstimator, get_coef

  from src.attractor.energy import run_energy, plot_energy
  from src.common.plot_utils import add_vlines, add_vdashed
  from src.common.options import set_options
  from src.stats.bootstrap import my_boots_ci
  from src.decode.bump import decode_bump, circcvl
  from src.common.get_data import get_X_y_days, get_X_y_S1_S2
  from src.preprocess.helpers import avg_epochs
#+end_src

#+RESULTS:

* Helpers
** Perceptron

#+begin_src ipython :tangle ../src/torch/percetron.py
  import torch
  import torch.nn as nn

  class CustomBCEWithLogitsLoss(nn.BCEWithLogitsLoss):
      def __init__(self, pos_weight=None, weight=None, reduction='mean'):
          super(CustomBCEWithLogitsLoss, self).__init__(weight=weight, reduction=reduction, pos_weight=pos_weight)

      def forward(self, input, target):
          target = target.view(-1, 1)  # Make sure target shape is (n_samples, 1)
          return super().forward(input.to(torch.float32), target.to(torch.float32))
#+end_src

#+RESULTS:

#+RESULTS:

#+begin_src ipython :tangle ../src/torch/perceptron.py
  class Perceptron(nn.Module):
      def __init__(self, num_features, dropout_rate=0.0):
          super(Perceptron, self).__init__()
          self.linear = nn.Linear(num_features, 1)
          self.dropout = nn.Dropout(dropout_rate)

      def forward(self, x):
          x = self.dropout(x)
          hidden = self.linear(x)
          return hidden
#+end_src

#+RESULTS:

#+begin_src ipython :tangle ../src/torch/perceptron.py
  class MLP(nn.Module):
      def __init__(self, num_features, hidden_units=64, dropout_rate=0.5):
          super(MLP, self).__init__()
          self.linear = nn.Linear(num_features, hidden_units)
          self.dropout = nn.Dropout(dropout_rate)
          self.relu = nn.ReLU()
          self.linear2 = nn.Linear(hidden_units, 1)

      def forward(self, x):
          x = self.dropout(x)
          x = self.relu(self.linear(x))
          x = self.dropout(x)
          hidden = self.linear2(x)
          return hidden
#+end_src

#+RESULTS:

#+begin_src ipython :tangle ../src/torch/skorch.py
  import torch
  from skorch import NeuralNetClassifier
  from skorch.callbacks import Callback
  from skorch.callbacks import EarlyStopping

  early_stopping = EarlyStopping(
      monitor='train_loss',    # Metric to monitor
      patience=10,              # Number of epochs to wait for improvement
      threshold=0.001,       # Minimum change to qualify as an improvement
      threshold_mode='rel',    # 'rel' for relative change, 'abs' for absolute change
      lower_is_better=True     # Set to True if lower metric values are better
  )

  class RegularizedNet(NeuralNetClassifier):
      def __init__(self, module, alpha=0.001, l1_ratio=0.95, **kwargs):
          self.alpha = alpha  # Regularization strength
          self.l1_ratio = l1_ratio # Balance between L1 and L2 regularization

          super().__init__(module, **kwargs)

      def get_loss(self, y_pred, y_true, X=None, training=False):
          # Call super method to compute primary loss
          if y_pred.shape != y_true.shape:
              y_true = y_true.unsqueeze(-1)

          loss = super().get_loss(y_pred, y_true, X=X, training=training)

          if self.alpha>0:
              elastic_net_reg = 0
              for param in self.module_.parameters():
                  elastic_net_reg += self.alpha * self.l1_ratio * torch.sum(torch.abs(param))
                  elastic_net_reg += self.alpha * (1 - self.l1_ratio) * torch.sum(param ** 2)

          # Add the elastic net regularization term to the primary loss
          return loss + elastic_net_reg
#+end_src

#+RESULTS:

** Model
#+begin_src ipython
  def get_bagged_coefs(clf, n_estimators):
      coefs_list = []
      bias_list = []
      for i in range(n_estimators):
          model = clf.estimators_[i]
          try:
              coefs = model.named_steps['net'].module_.linear.weight.data.cpu().detach().numpy()[0]
              bias = model.named_steps['net'].module_.linear.bias.data.cpu().detach().numpy()[0]
          except:
              coefs = model.named_steps['net'].coef_[0]
              bias = model.named_steps['net'].intercept_[0]

          # coefs, bias = rescale_coefs(model, coefs, bias)

          coefs_list.append(coefs)
          bias_list.append(bias)

      return np.array(coefs_list).mean(0), np.array(bias_list).mean(0)
#+end_src

#+RESULTS:

#+begin_src ipython :tangle ../src/torch/classificationCV.py
    from time import perf_counter
    from sklearn.ensemble import BaggingClassifier
    from sklearn.preprocessing import StandardScaler
    from sklearn.pipeline import Pipeline
    from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold, LeaveOneOut
    from sklearn.decomposition import PCA

    from mne.decoding import SlidingEstimator, cross_val_multiscore

    class ClassificationCV():
        def __init__(self, net, params, **kwargs):

            pipe = []
            self.scaler = kwargs['scaler']
            if self.scaler is not None and self.scaler !=0 :
                pipe.append(("scaler", StandardScaler()))

            self.n_comp = kwargs['n_comp']
            if kwargs['n_comp'] is not None:
                self.n_comp = kwargs['n_comp']
                pipe.append(("pca", PCA(n_components=self.n_comp)))

            pipe.append(("net", net))
            self.model = Pipeline(pipe)

            self.num_features = kwargs['num_features']
            self.scoring =  kwargs['scoring']

            if  kwargs['n_splits']==-1:
                self.cv = LeaveOneOut()
            else:
                self.cv = RepeatedStratifiedKFold(n_splits=kwargs['n_splits'], n_repeats=kwargs['n_repeats'])

            self.params = params
            self.verbose =  kwargs['verbose']
            self.n_jobs =  kwargs['n_jobs']

        def fit(self, X, y):
            start = perf_counter()
            if self.verbose:
                print('Fitting hyperparameters ...')

            try:
                self.model['net'].module__num_features = self.num_features
            except:
                pass

            grid = GridSearchCV(self.model, self.params, refit=True, cv=self.cv, scoring=self.scoring, n_jobs=self.n_jobs)
            grid.fit(X.astype('float32'), y.astype('float32'))
            end = perf_counter()
            if self.verbose:
                print("Elapsed (with compilation) = %dh %dm %ds" % convert_seconds(end - start))

            self.best_model = grid.best_estimator_
            self.best_params = grid.best_params_

            if self.verbose:
                print(self.best_params)

            try:
                self.coefs = self.best_model.named_steps['net'].module_.linear.weight.data.cpu().detach().numpy()[0]
                self.bias = self.best_model.named_steps['net'].module_.linear.bias.data.cpu().detach().numpy()[0]
            except:
                self.coefs = self.best_model.named_steps['net'].coef_[0]
                self.bias = self.best_model.named_steps['net'].intercept_[0]

        def get_bootstrap_coefs(self, X, y, n_boots=10):
            start = perf_counter()
            if self.verbose:
                print('Bootstrapping coefficients ...')

            self.bagging_clf = BaggingClassifier(base_estimator=self.best_model, n_estimators=n_boots)
            self.bagging_clf.fit(X.astype('float32'), y.astype('float32'))
            end = perf_counter()

            if self.verbose:
                print("Elapsed (with compilation) = %dh %dm %ds" % convert_seconds(end - start))

            self.coefs, self.bias = get_bagged_coefs(self.bagging_clf, n_estimators=n_boots)

            return self.coefs, self.bias

        def get_overlap(self, model, X):
            try:
                coefs = model.named_steps['net'].module_.linear.weight.data.cpu().detach().numpy()[0]
                bias = model.named_steps['net'].module_.linear.bias.data.cpu().detach().numpy()[0]
            except:
                coefs = model.named_steps['net'].coef_[0]
                bias = model.named_steps['net'].intercept_[0]

            if self.scaler is not None and self.scaler!=0:
                scaler = model.named_steps['scaler']
                for i in range(X.shape[-1]):
                    X[..., i] = scaler.transform(X[..., i])

            if self.n_comp is not None:
                pca = model.named_steps['pca']
                X_pca = np.zeros((X.shape[0], self.n_comp, X.shape[-1]))

                for i in range(X.shape[-1]):
                    X_pca[..., i] = pca.transform(X[..., i])

                self.overlaps = (np.swapaxes(X_pca, 1, -1) @ coefs + bias) / np.linalg.norm(coefs)
            else:
                self.overlaps = -(np.swapaxes(X, 1, -1) @ coefs + bias) / np.linalg.norm(coefs)

            return self.overlaps

        def get_bootstrap_overlaps(self, X):
            start = perf_counter()
            if self.verbose:
                print('Getting bootstrapped overlaps ...')

            X_copy = np.copy(X)
            overlaps_list = []
            n_boots = len(self.bagging_clf.estimators_)

            for i in range(n_boots):
                model = self.bagging_clf.estimators_[i]
                overlaps = self.get_overlap(model, X_copy)
                overlaps_list.append(overlaps)

            end = perf_counter()
            if self.verbose:
                print("Elapsed (with compilation) = %dh %dm %ds" % convert_seconds(end - start))

            return np.array(overlaps_list).mean(0)

        def get_cv_scores(self, X, y, scoring):
            start = perf_counter()
            if self.verbose:
                print('Computing cv scores ...')

            estimator = SlidingEstimator(clone(self.best_model), n_jobs=1,
                                         scoring=scoring, verbose=False)

            self.scores = cross_val_multiscore(estimator, X.astype('float32'), y.astype('float32'),
                                               cv=self.cv, n_jobs=-1, verbose=False)
            end = perf_counter()
            if self.verbose:
                print("Elapsed (with compilation) = %dh %dm %ds" % convert_seconds(end - start))

            return self.scores
#+end_src

#+RESULTS:


  #+begin_src ipython :tangle ../src/torch/main.py
      from src.common.get_data import get_X_y_days, get_X_y_S1_S2
      from src.preprocess.helpers import avg_epochs

      def get_classification(model, RETURN='overlaps', **options):
              start = perf_counter()

              dum = 0
              if options['features'] == 'distractor':
                      if options['task'] != 'Dual':
                              task = options['task']
                              options['task'] = 'Dual'
                              dum = 1

              X_days, y_days = get_X_y_days(**options)
              X, y = get_X_y_S1_S2(X_days, y_days, **options)
              y[y==-1] = 0
              if options['verbose']:
                  print('X', X.shape, 'y', y.shape)

              X_avg = avg_epochs(X, **options).astype('float32')
              if dum:
                      options['features'] = 'sample'
                      options['task'] = task
                      X, _ = get_X_y_S1_S2(X_days, y_days, **options)

              index = mice.index(options['mouse'])
              model.num_features = N_NEURONS[index]

              if options['class_weight']:
                      pos_weight = torch.tensor(np.sum(y==0) / np.sum(y==1), device=DEVICE).to(torch.float32)
                      print('imbalance', pos_weight)
                      model.criterion__pos_weight = pos_weight

              model.fit(X_avg, y)

              if 'scores' in RETURN:
                  scores = model.get_cv_scores(X, y, options['scoring'])
                  end = perf_counter()
                  print("Elapsed (with compilation) = %dh %dm %ds" % convert_seconds(end - start))
                  return scores
              if 'overlaps' in RETURN:
                  if options['n_boots']>1:
                          coefs, bias = model.get_bootstrap_coefs(X_avg, y, n_boots=options['n_boots'])
                          overlaps = model.get_bootstrap_overlaps(X)
                  else:
                          coefs = model.coefs
                          bias = model.bias
                          overlaps = model.get_overlap(model, X)

                  end = perf_counter()
                  print("Elapsed (with compilation) = %dh %dm %ds" % convert_seconds(end - start))
                  return overlaps
              if 'coefs' in RETURN:
                  if options['n_boots']>1:
                          coefs, bias = model.get_bootstrap_coefs(X_avg, y, n_boots=options['n_boots'])
                  else:
                          coefs = model.coefs
                          bias = model.bias
                  end = perf_counter()
                  print("Elapsed (with compilation) = %dh %dm %ds" % convert_seconds(end - start))
                  return coefs, bias
#+end_src

#+RESULTS:

** Other

#+begin_src ipython :tangle ../src/torch/utils.py
  import numpy as np

  def safe_roc_auc_score(y_true, y_score):
      y_true = np.asarray(y_true)
      if len(np.unique(y_true)) == 1:
          return np.nan  # return np.nan where the score cannot be calculated
      return roc_auc_score(y_true, y_score)
#+end_src

#+RESULTS:

#+begin_src ipython :tangle ../src/torch/utils.py
  def rescale_coefs(model, coefs, bias):

          try:
                  means = model.named_steps["scaler"].mean_
                  scales = model.named_steps["scaler"].scale_

                  # Rescale the coefficients
                  rescaled_coefs = np.true_divide(coefs, scales)

                  # Adjust the intercept
                  rescaled_bias = bias - np.sum(rescaled_coefs * means)

                  return rescaled_coefs, rescaled_bias
          except:
                  return coefs, bias

#+end_src

#+RESULTS:

#+begin_src ipython :tangle ../src/torch/utils.py
  from scipy.stats import bootstrap

  def get_bootstrap_ci(data, statistic=np.mean, confidence_level=0.95, n_resamples=1000, random_state=None):
      result = bootstrap((data,), statistic)
      ci_lower, ci_upper = result.confidence_interval
      return np.array([ci_lower, ci_upper])
#+end_src

#+RESULTS:

#+begin_src ipython :tangle ../src/torch/utils.py
  def convert_seconds(seconds):
      h = seconds // 3600
      m = (seconds % 3600) // 60
      s = seconds % 60
      return h, m, s
#+end_src

#+RESULTS:

#+begin_src ipython :tangle ../src/torch/utils.py
  import pickle as pkl

  def pkl_save(obj, name, path="."):
      pkl.dump(obj, open(path + "/" + name + ".pkl", "wb"))


  def pkl_load(name, path="."):
      return pkl.load(open(path + "/" + name, "rb"))

#+end_src

#+RESULTS:

** Plots
#+begin_src ipython
  def get_theta(a, b, GM=0, IF_NORM=0):

      u, v = a, b

      if GM:
          v = b - np.dot(b, a) / np.dot(a, a) * a

      if IF_NORM:
          u = a / np.linalg.norm(a)
          v = b / np.linalg.norm(b)

      return np.arctan2(v, u) % (2.0 * np.pi)
#+end_src

#+RESULTS:


#+begin_src ipython
  def get_energy(X, y, task, num_bins, bins, window, IF_BOOT=0, IF_NORM=0, IF_HMM=0, n_iter=10):
    ci_ = None
    energy_ = run_energy(X, num_bins, bins, task, window, VERBOSE=0, IF_HMM=IF_HMM, n_iter=n_iter)
    if IF_BOOT:
        _, ci_ = my_boots_ci(X, lambda x: run_energy(x, num_bins, bins, task, window, IF_HMM=IF_HMM, n_iter=n_iter), n_samples=1000)
    if ci_ is not None:
      ci_ = ci_ / 2.0
    return energy_, ci_
#+end_src

#+RESULTS:

#+begin_src ipython
  def plot_theta_energy(theta, energy, ci=None, window=.9, ax=None, SMOOTH=0, color='r'):
      if ax is None:
          fig, ax = plt.subplots()

      theta = np.linspace(0, 360, energy.shape[0], endpoint=False)
      energy = energy[1:]
      theta = theta[1:]

      windowSize = int(window * energy.shape[0])
      if SMOOTH:
          # window = np.ones(windowSize) / windowSize
          # energy = np.convolve(energy, window, mode='same')
          # theta = circcvl(theta, windowSize=windowSize)
          energy = circcvl(energy, windowSize=windowSize)

      ax.plot(theta, energy * 100, lw=4, color=color)

      if ci is not None:
          ax.fill_between(
              theta,
              (energy - ci[:, 0]) * 100,
              (energy + ci[:, 1]) * 100,
              alpha=0.1, color=color
          )

      ax.set_ylabel('Energy')
      ax.set_xlabel('Pref. Location (Â°)')
      ax.set_xticks([0, 90, 180, 270, 360])
#+end_src

#+RESULTS:

#+begin_src ipython
  import numpy as np

  def circcvl(signal, windowSize=10, axis=-1):
      signal_copy = signal.copy()

      if axis != -1 and signal.ndim != 1:
          signal_copy = np.swapaxes(signal_copy, axis, -1)

      # Save the nan positions before replacing them
      nan_mask = np.isnan(signal_copy)
      signal_copy[nan_mask] = np.interp(np.flatnonzero(nan_mask),
                                        np.flatnonzero(~nan_mask),
                                        signal_copy[~nan_mask])

      # Ensure the window size is odd for a centered kernel
      if windowSize % 2 == 0:
          windowSize += 1

      # Create a centered averaging kernel
      kernel = np.ones(windowSize) / windowSize

      # Apply convolution along the last axis or specified axis
      smooth_signal = np.apply_along_axis(lambda m: np.convolve(m, kernel, mode='same'), axis=-1, arr=signal_copy)

      # Substitute the original nan positions back into the result
      smooth_signal[nan_mask] = np.nan

      if axis != -1 and signal.ndim != 1:
          smooth_signal = np.swapaxes(smooth_signal, axis, -1)

      return smooth_signal
#+end_src

#+RESULTS:

#+begin_src ipython
  import numpy as np
  from scipy.optimize import differential_evolution
  from scipy.interpolate import interp1d
  import matplotlib.pyplot as plt

  def get_distance(x, y):
      distance = abs(x - y)
      if distance>180:
          distance -= 360
          distance *= -1
      return distance

  def find_multiple_minima_from_values(x_vals, y_vals, num_minima=2, num_runs=50, tol=0.05, popsize=50, maxiter=10000, min_distance=0.1, ax=None):
      # Interpolate the energy landscape
      energy_function = interp1d(x_vals, y_vals, kind='cubic', fill_value="extrapolate")

      # Define the bounds for the differential evolution
      bounds = [(x_vals.min(), x_vals.max())]

      results = []

      for _ in range(num_runs):
          result = differential_evolution(energy_function, bounds, strategy='best1bin',
                                          maxiter=maxiter, popsize=popsize, tol=0.01,
                                          seed=np.random.randint(0, 10000))
          results.append((result.x[0], result.fun))

      # Filter unique minima within a tolerance and minimum distance
      unique_minima = []
      for x_val, energy in results:

          if not any(np.isclose(x_val, um[0], atol=tol) or get_distance(x_val, um[0]) < min_distance for um in unique_minima):
              unique_minima.append([x_val, energy])

      # Ensure we only return the requested number of unique minima
      unique_minima = sorted(unique_minima, key=lambda x: x[1])[:num_minima]

      if ax is None:
          fig, ax = plt.subplots()
      # Plot the function
      x = np.linspace(x_vals.min(), x_vals.max(), 400)
      y = [energy_function(xi) for xi in x]  # Without noise for plotting
      # ax.plot(x, y)

      for min_x, _ in unique_minima:
          ax.plot(min_x, energy_function(min_x), 'ro')  # Mark the minima points

      return unique_minima

  # Example usage
  # x_vals = np.linspace(-2, 2, 50)
  # y_vals = np.sin(np.pi * x_vals) * 2 + np.cos(2 * np.pi * x_vals) * 2 + 0.1 * x_vals * 2 + np.random.normal(0, 0.1, size=x_vals.shape)

  # find_multiple_minima_from_values(x_vals, y_vals, num_minima=4, num_runs=10, tol=0.05, popsize=15, maxiter=100, min_distance=0.1)
#+end_src

#+RESULTS:

* Parameters

#+begin_src ipython
    DEVICE = 'cuda:0'
    mice = ['ChRM04','JawsM15', 'JawsM18', 'ACCM03', 'ACCM04']
    N_NEURONS = [668, 693, 444, 361, 113]

    tasks = ['DPA', 'DualGo', 'DualNoGo']

    kwargs = {
        'mouse': 'JawsM15',
        'trials': '', 'reload': 0, 'data_type': 'dF',
        'preprocess': True, 'scaler_BL': 'robust',
        'avg_noise':True, 'unit_var_BL': False,
        'random_state': None, 'T_WINDOW': 0.0,
        'l1_ratio': 0.95,
        'n_comp': None, 'scaler': None,
        'bootstrap': 1, 'n_boots': 1024,
        'n_splits': 5, 'n_repeats': 32,
        'class_weight': 1
    }

    options = set_options(**kwargs)
    days = np.arange(1, options['n_days']+1)
    days = ['first', 'last']

    safe_roc_auc = make_scorer(safe_roc_auc_score, needs_proba=True)
    options['scoring'] = safe_roc_auc
    options['n_jobs'] = 30
#+end_src

#+RESULTS:

* Decoding vs days

#+begin_src ipython
    net = RegularizedNet(
        module=Perceptron,
        module__num_features=693,
        module__dropout_rate=0.0,
        alpha=0.01,
        l1_ratio=options['l1_ratio'],
        criterion=CustomBCEWithLogitsLoss,
        criterion__pos_weight=torch.tensor(1.0, device=DEVICE).to(torch.float32),
        optimizer=optim.Adam,
        optimizer__lr=0.1,
        max_epochs=1000,
        callbacks=[early_stopping],
        train_split=None,
        iterator_train__shuffle=False,  # Ensure the data is shuffled each epoch
        verbose=0,
        device= DEVICE if torch.cuda.is_available() else 'cpu',  # Assuming you might want to use CUDA
        compile=True,
        warm_start=True,
    )

    params = { 'net__alpha': np.logspace(-4, 4, 10),
               # 'net__l1_ratio': np.linspace(0, 1, 10),
               # 'net__module__dropout_rate': np.linspace(0, 1, 10),
              }

    options['verbose'] = 0
    model = ClassificationCV(net, params, **options)
    options['verbose'] = 1
  #+end_src

#+RESULTS:

#+begin_src ipython
    from sklearn.linear_model import LogisticRegression
    net = LogisticRegression(penalty='l1', solver='liblinear', class_weight='balanced', n_jobs=None)
    # net = LogisticRegression(penalty='elasticnet', solver='saga', class_weight='balanced', n_jobs=None, l1_ratio=0.95, max_iter=100, tol=.001)

    params = {'net__C': np.logspace(-4, 4, 10)}

    options['n_jobs'] = -1
    options['verbose'] = 0
    model = ClassificationCV(net, params, **options)
    options['verbose'] = 1

#+end_src

#+RESULTS:

#+begin_src ipython
    coefs_sample = []
    coefs_dist = []
    coefs_choice = []

    bias_sample = []
    bias_dist = []
    bias_choice = []

    theta_day = []
    index_day = []

    for day in days:
        options['day'] = day

        options['class_weight'] = 0
        options['task'] = 'all'
        options['features'] = 'sample'
        options['epochs'] = ['ED']
        coefs, bias = get_classification(model, RETURN='coefs', **options)

        coefs_sample.append(coefs)
        bias_sample.append(bias)

        options['task'] = 'Dual'
        options['features'] = 'distractor'
        options['epochs'] = ['MD']
        coefs, bias = get_classification(model, RETURN='coefs', **options)

        coefs_dist.append(coefs)
        bias_dist.append(bias)

        # options['class_weight'] = 1
        # options['task'] = 'all'
        # options['features'] = 'choice'
        # options['epochs'] = ['CHOICE']
        # coefs, bias = get_classification(model, RETURN='coefs', **options)

        # # coefs_choice.append(coefs)
        # bias_choice.append(bias)

        theta = get_theta(-coefs_sample[-1], -coefs_dist[-1], IF_NORM=0, GM=0)
        theta_day.append(theta)
        index_day.append(theta.argsort())

    coefs_save = np.stack((coefs_sample, coefs_dist))
    print(coefs_save.shape)
    pkl_save(coefs_sample, '%s_coefs_%.2f_l1_ratio%s' % (options['mouse'], options['l1_ratio'], options['fname']), path="../data/%s/" % options['mouse'])
    #+end_src

#+RESULTS:
#+begin_example
    Loading files from /home/leon/dual_task/dual_data/data/JawsM15
    PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR False
    DATA: FEATURES sample TASK all TRIALS  DAYS first LASER 0
    multiple days 0 3 0
    X (288, 693, 84) y (288,)
    Elapsed (with compilation) = 0h 0m 28s
    Loading files from /home/leon/dual_task/dual_data/data/JawsM15
    PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR False
    DATA: FEATURES distractor TASK Dual TRIALS  DAYS first LASER 0
    multiple days 0 3 0
    X (192, 693, 84) y (192,)
    Elapsed (with compilation) = 0h 0m 20s
    Loading files from /home/leon/dual_task/dual_data/data/JawsM15
    PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR False
    DATA: FEATURES sample TASK all TRIALS  DAYS last LASER 0
    multiple days 0 3 0
    X (288, 693, 84) y (288,)
    Elapsed (with compilation) = 0h 0m 27s
    Loading files from /home/leon/dual_task/dual_data/data/JawsM15
    PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR False
    DATA: FEATURES distractor TASK Dual TRIALS  DAYS last LASER 0
    multiple days 0 3 0
    X (192, 693, 84) y (192,)
    Elapsed (with compilation) = 0h 0m 19s
    (2, 2, 693)
#+end_example

* Landscape vs days
** Reload data

#+begin_src ipython
  options['features'] = 'sample'
  options['trials'] = ''
  options['reload'] = 0

  X_list = []
  y_list = []
  tasks = ["DPA", "DualGo", "DualNoGo"]

  for i, day in enumerate(days):
      X_dum = []
      y_dum = []
      options['day'] = day
      for task in tasks:
          options['task'] = task
          X_days, y_days = get_X_y_days(**options)
          X_data, y_data = get_X_y_S1_S2(X_days, y_days, **options)
          y_data[y_data==-1] = 0
          print(X_data.shape)
          X_dum.append(X_data[..., index_day[i], :])
          y_dum.append(y_data)

      X_list.append(X_dum)
      y_list.append(y_dum)

  try:
      X_list = np.array(X_list)
      y_list = np.array(y_list)

      print(X_list.shape, y_list.shape)
  except:
      pass
      #+end_src

#+RESULTS:
#+begin_example
    Loading files from /home/leon/dual_task/dual_data/data/JawsM15
    PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR False
    DATA: FEATURES sample TASK DPA TRIALS  DAYS first LASER 0
    multiple days 0 3 0
    (96, 693, 84)
    Loading files from /home/leon/dual_task/dual_data/data/JawsM15
    PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR False
    DATA: FEATURES sample TASK DualGo TRIALS  DAYS first LASER 0
    multiple days 0 3 0
    (96, 693, 84)
    Loading files from /home/leon/dual_task/dual_data/data/JawsM15
    PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR False
    DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS first LASER 0
    multiple days 0 3 0
    (96, 693, 84)
    Loading files from /home/leon/dual_task/dual_data/data/JawsM15
    PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR False
    DATA: FEATURES sample TASK DPA TRIALS  DAYS last LASER 0
    multiple days 0 3 0
    (96, 693, 84)
    Loading files from /home/leon/dual_task/dual_data/data/JawsM15
    PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR False
    DATA: FEATURES sample TASK DualGo TRIALS  DAYS last LASER 0
    multiple days 0 3 0
    (96, 693, 84)
    Loading files from /home/leon/dual_task/dual_data/data/JawsM15
    PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR False
    DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS last LASER 0
    multiple days 0 3 0
    (96, 693, 84)
    (2, 3, 96, 693, 84) (2, 3, 96)
#+end_example

** Energy

#+begin_src ipython
    opts = set_options(T_WINDOW=0.0)
    bins = None
    # bins = np.concatenate( (opts['bins_BL'], opts['bins_ED'], opts['bins_MD'], opts['bins_LD']))
    # bins = np.concatenate( (opts['bins_BL'], opts['bins_ED']))
    # bins = opts['bins_ED']
    # bins = np.concatenate( (opts['bins_BL'], opts['bins_STIM'], opts['bins_ED']))
    # bins = np.concatenate( (opts['bins_ED'], opts['bins_MD'], opts['bins_LD']))
    # bins = opts['bins_PRE_DIST']
    bins = opts['bins_DELAY']
#+end_src

#+RESULTS:

#+begin_src ipython
  task = 'all'
  kwargs['task'] = task

  num_bins = 96
  print('num_bins', num_bins)

  window = 0.1
  print('window', window)

  IF_HMM = 0
  n_iter = 100
  IF_BOOT=1
  IF_NORM=0
#+end_src

#+RESULTS:
: num_bins 96
: window 0.1

#+begin_src ipython
  print(np.array(X_list[0][0]).shape)
#+end_src

#+RESULTS:
: (96, 693, 84)

#+begin_src ipython
  energy_day = []
  ci_day = []

  for i, day in enumerate(days):
      X = np.array(X_list[i])
      energy, ci = get_energy(X, np.array(y_list[i]), task, num_bins, bins, window, IF_BOOT, IF_NORM, IF_HMM, n_iter)

      energy_day.append(energy)
      ci_day.append(ci)
#+end_src

#+RESULTS:
:RESULTS:
: bootstrap: 100% 1000/1000 [00:12<00:00, 77.68it/s]
:
: bootstrap: 100% 1000/1000 [00:12<00:00, 77.87it/s]
:
:END:

#+begin_src ipython
  from scipy.signal import find_peaks
  import numpy as np

  def find_minima(energy, ax, color, window=0.1, prominence=1, distance=90, height=0.5):
      energy = energy[1:]
      windowSize = int(window * energy.shape[0])

      # Smooth the energy data
      # window = np.ones(windowSize) / windowSize
      # energy_smoothed = np.convolve(energy, window, mode='same')
      energy_smoothed = circcvl(energy, windowSize=windowSize)

      # Invert the energy to find minima as peaks
      inv_energy = np.max(energy_smoothed) - energy_smoothed
      # inv_energy = np.mean(energy_smoothed) - energy_smoothed

      # Find peaks with higher prominence for global minima identification
      peaks, properties = find_peaks(inv_energy, prominence=prominence, distance=distance, height=height)

      theta = np.linspace(0, 360, energy.shape[0], endpoint=False)
      minima_angles = theta[peaks]
      minima_energy = energy[peaks]

      # Filter out closely spaced minima based on the threshold
      filtered_minima_angles = []
      filtered_minima_energy = []

      for i in range(len(minima_angles)):
          if minima_energy[i]>0:
              filtered_minima_angles.append(minima_angles[i])
              filtered_minima_energy.append(0)

      print(filtered_minima_angles)
      # print(minima_energy)

      # Plot results
      ax.plot(filtered_minima_angles[:2], filtered_minima_energy[:2], 'o', color=color, ms=10)

      # if len(filtered_minima_angles) >= 2:
      #     angular_distances = np.abs(filtered_minima_angles[0] % 180 - filtered_minima_angles[1] % 180)
      #     print(f"The distance between the two main minima is {angular_distances} degrees.")
      # else:
      #     print("Less than two main minima found.")

      return filtered_minima_angles[:2], filtered_minima_energy[:2]
#+end_src

#+RESULTS:

#+begin_src ipython
  cmap = plt.get_cmap('Blues')
  colors = [cmap((i+1)/len(days)) for i in range(len(days))]
  window = .05

  min_angles, min_energies = [], []
  theta = np.linspace(0, 360, energy_day[0].shape[0]-1, endpoint=False)
  windowSize = int(window * energy_day[0].shape[0]-1)

  fig, ax = plt.subplots()
  for i, day in enumerate(days):
      plot_theta_energy(theta_day[i], energy_day[i], ci_day[i],
                        window=window, ax=ax, SMOOTH=1, color=colors[i])

      # min_angle, min_energy = find_minima(energy_day[i] * 100, window=window,
      #                                     prominence=.001,  ax=ax,  color=colors[i], distance=15, height=None)

      minima = find_multiple_minima_from_values(theta, circcvl(energy_day[i][1:]*100, windowSize), num_minima=2, num_runs=150, tol=.01, ax=ax, min_distance=60, popsize=1)

      try:
          min_angles.append([minima[0][0], minima[1][0]])
          min_energies.append([minima[0][1], minima[1][1]])
      except:
          pass

  fig.savefig('./figs/%s_landscape.svg' % options['mouse'], dpi=300)
#+end_src

#+RESULTS:
[[./.ob-jupyter/675367f884555932c358d791acfa00a61a9b8c47.png]]

#+begin_src ipython
  min_angles = np.array(min_angles).T
  print(np.round(min_angles))
#+end_src

#+RESULTS:
: [[326.]
:  [204.]]

#+begin_src ipython
  distance = np.abs(min_angles[0] - min_angles[1])
  print(distance)
  for i in range(distance.shape[0]):
      if distance[i]>180:
          distance[i] -= 360
          distance[i] *= -1

  print(np.round(distance))

  plt.plot(np.arange(1, len(days)+1), distance, '-o')
  # plt.ylim(0, 45)
  plt.xticks(np.arange(1, len(days)+1))
  # plt.yticks([0, 45, 90, 135, 180])
  plt.xlabel('Day')
  plt.ylabel('Distance')
  fig.savefig('%s_distance_landscape.svg' % options['mouse'], dpi=300)
  plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: [122.71386448]
: [123.]
# [goto error]
#+begin_example
    ---------------------------------------------------------------------------
    ValueError                                Traceback (most recent call last)
    Cell In[140], line 10
          6         distance[i] *= -1
          8 print(np.round(distance))
    ---> 10 plt.plot(np.arange(1, len(days)+1), distance, '-o')
         11 # plt.ylim(0, 45)
         12 plt.xticks(np.arange(1, len(days)+1))

    File ~/mambaforge/envs/dual_data/lib/python3.11/site-packages/matplotlib/pyplot.py:3575, in plot(scalex, scaley, data, *args, **kwargs)
       3567 @_copy_docstring_and_deprecators(Axes.plot)
       3568 def plot(
       3569     *args: float | ArrayLike | str,
       (...)
       3573     **kwargs,
       3574 ) -> list[Line2D]:
    -> 3575     return gca().plot(
       3576         *args,
       3577         scalex=scalex,
       3578         scaley=scaley,
       3579         **({"data": data} if data is not None else {}),
       3580         **kwargs,
       3581     )

    File ~/mambaforge/envs/dual_data/lib/python3.11/site-packages/matplotlib/axes/_axes.py:1721, in Axes.plot(self, scalex, scaley, data, *args, **kwargs)
       1478 """
       1479 Plot y versus x as lines and/or markers.
       1480
       (...)
       1718 (``'green'``) or hex strings (``'#008000'``).
       1719 """
       1720 kwargs = cbook.normalize_kwargs(kwargs, mlines.Line2D)
    -> 1721 lines = [*self._get_lines(self, *args, data=data, **kwargs)]
       1722 for line in lines:
       1723     self.add_line(line)

    File ~/mambaforge/envs/dual_data/lib/python3.11/site-packages/matplotlib/axes/_base.py:303, in _process_plot_var_args.__call__(self, axes, data, *args, **kwargs)
        301     this += args[0],
        302     args = args[1:]
    --> 303 yield from self._plot_args(
        304     axes, this, kwargs, ambiguous_fmt_datakey=ambiguous_fmt_datakey)

    File ~/mambaforge/envs/dual_data/lib/python3.11/site-packages/matplotlib/axes/_base.py:499, in _process_plot_var_args._plot_args(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)
        496     axes.yaxis.update_units(y)
        498 if x.shape[0] != y.shape[0]:
    --> 499     raise ValueError(f"x and y must have same first dimension, but "
        500                      f"have shapes {x.shape} and {y.shape}")
        501 if x.ndim > 2 or y.ndim > 2:
        502     raise ValueError(f"x and y can be no greater than 2D, but have "
        503                      f"shapes {x.shape} and {y.shape}")

    ValueError: x and y must have same first dimension, but have shapes (2,) and (1,)
#+end_example
[[./.ob-jupyter/c23235fbf6ba6084d6d11d02b24f786b35477adb.png]]
:END:

#+begin_src ipython

#+end_src

#+RESULTS:


#+begin_src ipython
      pkl_save(distance, '%s_distance_landscape' % options['mouse'], path="../data/%s/" % options['mouse'])
#+end_src

#+RESULTS:
