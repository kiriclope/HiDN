#+STARTUP: fold
#+PROPERTY: header-args:ipython :results both :exports both :async yes :session dpca :kernel torch :exports results :output-dir ./figures/dpca :file (lc/org-babel-tangle-figure-filename)

* Notebook Settings

#+begin_src ipython
%load_ext autoreload
%autoreload 2
%reload_ext autoreload

%run /home/leon/dual_task/dual_data/notebooks/setup.py
%matplotlib inline
%config InlineBackend.figure_format = 'png'
#+end_src

#+RESULTS:
: The autoreload extension is already loaded. To reload it, use:
:   %reload_ext autoreload
: Python exe
: /home/leon/mambaforge/envs/torch/bin/python

* Imports

#+begin_src ipython
  from sklearn.exceptions import ConvergenceWarning
  warnings.filterwarnings("ignore")

  import sys
  sys.path.insert(0, '/home/leon/dual_task/dual_data/')

  import os
  if not sys.warnoptions:
    warnings.simplefilter("ignore")
    os.environ["PYTHONWARNINGS"] = "ignore"

  import pickle as pkl
  import numpy as np
  import matplotlib.pyplot as plt
  from time import perf_counter

  from src.common.options import set_options
  from src.stats.bootstrap import my_boots_ci
  from src.decode.bump import decode_bump, circcvl
  from src.common.get_data import get_X_y_days, get_X_y_S1_S2
  from src.preprocess.helpers import avg_epochs
#+end_src

#+RESULTS:

#+begin_src ipython
from src.dPCA import dPCA
#+end_src

#+RESULTS:

* Helpers

#+begin_src ipython
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.impute import SimpleImputer
import numpy as np

class CustomImputer(BaseEstimator, TransformerMixin):
    """
    A custom imputer for 5D data that flattens the last dimension(s) to apply
    scikit‐learn's SimpleImputer, then reshapes back to the original 5D form.
    """
    def __init__(self, strategy="mean"):
        """
        Parameters
        ----------
        strategy : str, optional (default="mean")
            The imputation strategy to pass to SimpleImputer.
            Could be "mean", "median", "most_frequent", or "constant".
        """
        self.strategy = strategy
        self.imputer_ = None
        self.original_shape_ = None

    def fit_transform(self, X, y=None):
        self.fit(X, y)
        return self.transform(X)

    def fit(self, X, y=None):
        """
        Fit the imputer to X.

        Parameters
        ----------
        X : ndarray of shape (n1, n2, n3, n4, n5)
            The 5D data to fit.

        Returns
        -------
        self : Custom5DImputer
        """
        self.original_shape_ = X.shape

        # (n1, n2, n3, n4, n5) → reshape to 2D for SimpleImputer
        # One common approach is to treat everything as features except the first axis:
        # e.g. (n1, n2*n3*n4*n5). Or flatten sections differently if needed.
        X_2d = X.reshape(X.shape[0], -1)

        # Create and fit an actual SimpleImputer
        self.imputer_ = SimpleImputer(strategy=self.strategy)
        self.imputer_.fit(X_2d)

        return self

    def transform(self, X):
        """
        Transform (impute) X with the fitted imputer.

        Parameters
        ----------
        X : ndarray of shape (n1, n2, n3, n4, n5)
            The 5D data to transform/impute.

        Returns
        -------
        X_imputed : ndarray of shape (n1, n2, n3, n4, n5)
            The imputed 5D data.
        """
        # Check shape consistency
        if X.shape != self.original_shape_:
            raise ValueError(
                f"Shape of X {X.shape} does not match the fitted shape {self.original_shape_}."
            )

        # Reshape to 2D
        X_2d = X.reshape(X.shape[0], -1)

        # Transform
        X_2d_imputed = self.imputer_.transform(X_2d)

        # Reshape back to 5D
        X_imputed = X_2d_imputed.reshape(self.original_shape_)

        return X_imputed
#+end_src

#+RESULTS:

** Other


#+begin_src ipython
class standard_scaler():
      def __init__(self, axis=0):
            self.axis = axis

      def fit(self, X):
            self.mean = np.nanmean(X, axis=axis, keepdims=True)
            self.std = np.nanstd(X, axis=axis, keepdims=True) + 1e-6

            return self

      def transform(self, X):
            return (X - self.mean) / self.std

      def fit_transform(self, X):
            self.fit(X)
            return self.transform(X)
#+end_src

#+RESULTS:

#+begin_src ipython :tangle ../src/torch/utils.py
  from scipy.stats import bootstrap

  def get_bootstrap_ci(data, statistic=np.mean, confidence_level=0.95, n_resamples=1000, random_state=None):
      result = bootstrap((data,), statistic)
      ci_lower, ci_upper = result.confidence_interval

#+RESULTS:

#+RESULTS:

      return np.array([ci_lower, ci_upper])
#+end_src

#+RESULTS:

#+begin_src ipython :tangle ../src/torch/utils.py
  def convert_seconds(seconds):
      h = seconds // 3600
      m = (seconds % 3600) // 60
      s = seconds % 60
      return h, m, s
#+end_src

#+RESULTS:

#+begin_src ipython
def angle_AB(A, B):
      A_norm = A / (np.linalg.norm(A) + 1e-5)
      B_norm = B / (np.linalg.norm(B) + 1e-5)

      cos_theta = A_norm @ B_norm.T
      angle_radians = np.arccos(np.clip(cos_theta, -1.0, 1.0))

      return np.degrees(angle_radians)
#+end_src

#+RESULTS:

#+begin_src ipython :tangle ../src/torch/utils.py
  import pickle as pkl

  def pkl_save(obj, name, path="."):
      pkl.dump(obj, open(path + "/" + name + ".pkl", "wb"))


  def pkl_load(name, path="."):
      return pkl.load(open(path + "/" + name + '.pkl', "rb"))

#+end_src

#+RESULTS:

* Parameters

#+begin_src ipython
  DEVICE = 'cuda:0'
  mice = ['ChRM04','JawsM15', 'JawsM18', 'ACCM03', 'ACCM04']
  # mice = ['JawsM01', 'JawsM06', 'JawsM12', 'JawsM15', 'JawsM18', 'ChRM04', 'ChRM23', 'ACCM03', 'ACCM04']
  mice = ['JawsM15']
  tasks = ['DPA']
  # mice = ['AP02', 'AP12']
  # mice = ['PP09', 'PP17']

  kwargs = {
      'mouse': mice[0], 'laser': 0,
      'tasks': tasks,
      'trials': '', 'reload': 0, 'data_type': 'dF',
      'prescreen': None, 'pval': 0.05, 'n_comp': 0,
      'preprocess': False, 'scaler_BL': 'robust',
      'avg_noise': True, 'unit_var_BL': True,
      'random_state': None, 'T_WINDOW': 0.0,
      'l1_ratio': 0.95,
      'n_comp': None, 'scaler': None,
      'bootstrap': 1, 'n_boots': 1000,
      'n_splits': 5, 'n_repeats': 10,
      'class_weight': 0,
      'multilabel':0,
      'mne_estimator': 'generalizing', # sliding or generalizing
      'n_jobs': 128,
      'bolasso_penalty': 'l2',
      'bolasso_pval': 0.05,
      'laser' : 0,
  }

  # kwargs['days'] = ['first', 'middle', 'last']
  kwargs['days'] = ['first', 'last']
  # kwargs['days'] = 'all'
  # kwargs['days'] = ['first']

  options = set_options(**kwargs)
  print(options['days'])
  options['mice'] = mice
  name = '5folds'
#+end_src

#+RESULTS:
: ['first', 'last']

#+begin_src ipython
import pandas as pd

new_mice = ['JawsM01', 'JawsM06', 'JawsM12', 'ChRM23']

options['reload'] = 0
X_mouse, y_mouse = [], []
y_laser = []
y_choice = []
y_dfs = []

for idx, mouse in enumerate(options['mice']):
    options['mouse'] = mouse
    options['features'] = 'sample'
    options['verbose'] = 0

    options['trials'] = ''
    options['reload'] = 0

    if mouse in new_mice:
        options['NEW_DATA'] = 1
    else:
        options['NEW_DATA'] = 0

    options = set_options(**options)

    X_list = []
    y_df__ = []

    for i, day in enumerate(options['days']):
        X_dum = []
        y_df_ = []

        options['day'] = day

        for task in options['tasks']:
            options['task'] = task
            X_days, y_days = get_X_y_days(**options)
            options['reload'] = 0
            X_data, y_data = get_X_y_S1_S2(X_days, y_days, **options)

            X_dum.append(X_data)
            y_df_.append(y_data)

        y_df_ = pd.concat(y_df_)
        y_df_['DAY'] = day
        y_df__.append(y_df_)

        X_list.append(X_dum)

    X_mouse.append(X_list)
    y_df__ = pd.concat(y_df__)
    y_df__['mouse'] = mouse
    y_dfs.append(y_df__)

y_dfs = pd.concat(y_dfs)
#+end_src

#+RESULTS:
: first [1. 2. 3.] [0. 1.]
: last [4. 5. 6.] [1. 0.]


#+begin_src ipython

#+end_src

#+RESULTS:
#+begin_src ipython
import numpy as np

class StandardScaler:
    def __init__(self, axis=0, if_scale=1):
        self.axis = axis
        self.center_ = None
        self.scale_ = None
        self.if_scale_ = if_scale

    def fit(self, X):
        self.center_ = np.nanmean(X, axis=self.axis, keepdims=True)
        self.scale_ = np.nanstd(X, axis=self.axis, keepdims=True)
        # Prevent division by zero
        self.scale_ = np.where(self.scale_ == 0, 1, self.scale_)
        return self

    def transform(self, X):
        if self.if_scale_:
            return (X - self.center_) / self.scale_
        return (X - self.center_)

    def fit_transform(self, X):
        self.fit(X)
        return self.transform(X)
#+end_src

#+RESULTS:


* Pair * time

#+begin_src ipython
i_mouse = 0
i_day=0
print(options['mice'][i_mouse])
print(np.vstack(X_mouse[i_mouse][i_day]).shape)
#+end_src

#+RESULTS:
: JawsM15
: (96, 693, 84)

#+begin_src ipython
from collections import defaultdict

Z_day = []
EV_day = []

scaler = StandardScaler(axis=0)

for i_day, day in enumerate(['first', 'last']):
    X = np.vstack(X_mouse[i_mouse][i_day])
    mouse = options['mice'][i_mouse]
    y = y_dfs[(y_dfs.mouse==mouse) & (y_dfs.DAY == day) & (y_dfs.tasks=='DPA')]

    print(X.shape, y.shape, y.day.unique())
    X = scaler.fit_transform(X)

    idx_off = (y.laser==0)

    #  n_trials, n_neurons, Z1, Z2, ..., n_time, lets do odor pair * choice
    X_dpca = np.zeros((2, 2, int(X.shape[0]/2), X.shape[-2],  X.shape[-1]))
    print('X_dpca', X_dpca.shape)

    for i in range(2):
        for j in range(2):
            dum = X[(y.sample_odor==i) & (y.test_odor==j)]
            mean_dum = np.mean(dum, axis=0)[np.newaxis]

            X_dpca[i, j, :dum.shape[0]] = dum
            X_dpca[i, j, dum.shape[0]:] = mean_dum

    X_dpca = np.transpose(X_dpca, (2, 3, 0, 1, 4))
    X_dpca_avg = np.nanmean(X_dpca, axis=0)
    print(X_dpca.shape, X_dpca_avg.shape)

    dpca = dPCA.dPCA(labels='pct', n_components=2, regularizer='auto')
    dpca.protect = ['t']

    axes = (1, 2)
    X_dpca -= np.mean(X_dpca_avg, axis=axes, keepdims=True)

    X_dpca_avg -= np.mean(X_dpca_avg, axis=axes, keepdims=True)

    Z = dpca.fit_transform(X_dpca_avg, X_dpca)
    EV = dpca.explained_variance_ratio_

    print(Z['p'].shape)

    Z_day.append(Z)
    EV_day.append(EV)
#+end_src

#+RESULTS:
#+begin_example
(96, 693, 84) (96, 17) [1. 2. 3.]
X_dpca (2, 2, 48, 693, 84)
(48, 693, 2, 2, 84) (693, 2, 2, 84)
Start optimizing regularization.
Starting trial  1 / 3
Starting trial  2 / 3
Starting trial  3 / 3
Optimized regularization, optimal lambda =  0.0017286737396774677
Regularization will be fixed; to compute the optimal                    parameter again on the next fit, please                    set opt_regularizer_flag to True.
(2, 2, 2, 84)
(96, 693, 84) (96, 17) [4. 5. 6.]
X_dpca (2, 2, 48, 693, 84)
(48, 693, 2, 2, 84) (693, 2, 2, 84)
Start optimizing regularization.
Starting trial  1 / 3
Starting trial  2 / 3
Starting trial  3 / 3
Optimized regularization, optimal lambda =  0.001234766956912477
Regularization will be fixed; to compute the optimal                    parameter again on the next fit, please                    set opt_regularizer_flag to True.
(2, 2, 2, 84)
#+end_example

#+begin_src ipython
Z_all = defaultdict(list)
for d in Z_day:
    for k, v in d.items():
        Z_all[k].append(v)

print(np.array(Z_all['p']).shape)
#+end_src

#+RESULTS:
: (2, 2, 2, 2, 84)

#+begin_src ipython
EV_all = defaultdict(list)
for d in EV_day:
    for k, v in d.items():
        print(k, np.round(v[0]*100))
        EV_all[k].append(v)
#+end_src

#+RESULTS:
#+begin_example
p 0.0
c 0.0
t 0.0
pc 0.0
pt 0.0
ct 0.0
pct 0.0
p 0.0
c 0.0
t 0.0
pc 0.0
pt 0.0
ct 0.0
pct 0.0
#+end_example


#+begin_src ipython
from src.common.plot_utils import add_vlines

ls = ['-', '--']
label = ['AD', 'AC', 'BC', 'BD']
pc = ['Sample', 'Choice', 'Choice * Time']
xtime = np.linspace(0, 14, 84)

fig, ax = plt.subplots(1, 3, figsize=[3* width, height])

n_comp = 0
pair = 0

for i in range(2):
        for j in range(2):
                ax[0].plot(xtime, Z['p'][n_comp][i][j], ls = ls[i], label=label[i+j*2])
                ax[1].plot(xtime, Z['c'][n_comp][i][j], ls = ls[i], label=label[i+j*2])
                ax[2].plot(xtime, Z['ct'][n_comp][i][j], ls = ls[i], label=label[i+j*2])

for k in range(3):
        add_vlines(ax[k])
        ax[k].set_xlim([0, 12])
        ax[k].axhline(0, ls=':')
        ax[k].set_xlabel('Time (s)')
        ax[k].set_ylabel('%s' %pc[k])

ax[0].legend(fontsize=14, frameon=0)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/dpca/figure_17.png]]


#+begin_src ipython
from src.common.plot_utils import add_vlines

ls = ['-', '--']
color = ['r', 'b']
label = ['lick', 'nolick']
pc = ['Sample', 'Choice', 'Choice * Time']
xtime = np.linspace(0, 14, 84)

fig, ax = plt.subplots(1, 3, figsize=[3* width, height])

n_comp = 0
pair = 0
for i in range(2):
        for j in range(2):
                ax[0].plot(xtime, (Z_day[j]['p'][n_comp][pair][i] - Z_day[j]['p'][n_comp][1][i])/2, ls = ls[i], label=label[i], color=color[i], alpha=j/2+0.5)
                ax[1].plot(xtime, (Z_day[j]['c'][n_comp][pair][i] + Z_day[j]['c'][n_comp][1][i])/2, ls = ls[i], label=label[i], color=color[i], alpha=j/2+0.5)
                ax[2].plot(xtime, (Z_day[j]['ct'][n_comp][pair][i] + Z_day[j]['p'][j][n_comp][1][i])/2, ls = ls[i], label=label[i], color=color[i], alpha=j/2+0.5)

for k in range(3):
        add_vlines(ax[k])
        ax[k].set_xlim([0, 12])
        ax[k].axhline(0, ls=':')
        ax[k].set_xlabel('Time (s)')
        ax[k].set_ylabel('%s' %pc[k])

plt.legend(fontsize=14, frameon=0)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/dpca/figure_18.png]]


#+begin_src ipython

#+end_src

#+RESULTS:



* Pair * choice * time

#+begin_src ipython
from sklearn.model_selection import KFold, LeaveOneOut, StratifiedKFold, RepeatedKFold
from collections import defaultdict

def dpca_cv(X, dPCA, n_splits=5):
    kf = KFold(n_splits, shuffle=True, random_state=None)

    Z_test = []
    for train_idx, test_idx in kf.split(X):
        X_train, X_test = X[train_idx], X[test_idx]
        print(X_train.shape, X_test.shape)

        X_train_avg = np.nanmean(X_train, axis=0)

        axes = tuple(range(1, X_train_avg.ndim-1))
        X_train_scale = np.nanmean(X_train_avg, axis=axes, keepdims=True)
        X_train_avg -= X_train_scale

        X_test_avg = np.nanmean(X_test, axis=0)
        X_test_avg -= X_train_scale

        dPCA.fit_transform(X_train_avg, X_train)
        Z_test.append(dPCA.transform(X_test_avg))

    result = defaultdict(list)
    for d in Z_test:
        for k, v in d.items():
            result[k].append(v)

    means = {k: np.mean(vs, axis=0) for k, vs in result.items()}

    return means
#+end_src

#+RESULTS:

#+begin_src ipython
# crossvalidate the regularization parameter
from sklearn.model_selection import KFold, LeaveOneOut, StratifiedKFold, RepeatedKFold

def crossval_dpca_reg(X_trials, marginalization='st', lambdas=np.logspace(-3, 3, 10)):
    """ Crossvalidates (5-fold) the lambda for the PCA model. Better than built-in function as built-in does not crossvalidate.
    Inputs:         X_trials:           trial-by-trial input vector (trials, neurons, stimuli, times)
                    marginalization:    marginalization points
                    lambdas:            range of potential lambdas to test for

    Outputs:        best_lambda:    Best crossvalidated lambda parameter
                    all_scores:     dictionary of all lambdas with their respective crossvalidation scores
    """
    kf = KFold(n_splits=3, shuffle=True, random_state=None)

    X = X_trials.copy()
    # axes = tuple(range(1, X_trials.ndim))
    # mask = np.any(np.isnan(X_trials), axis=axes)
    # X = X_trials[~mask]

    print(X.shape)

    best_lambda = None
    best_score = -np.inf
    all_scores = {}
    for lam in lambdas:
        scores = []
        for train_idx, test_idx in kf.split(X):
            # Average over training trials
            X_train_avg = np.nanmean(X[train_idx], axis=0)

            axes = tuple(range(1, X_train_avg.ndim-1)) # avg over marg (1, 2), 0 is neurons and -1 is time
            X_train_scale = np.nanmean(X_train_avg, axis=axes, keepdims=True)
            X_train_avg -= X_train_scale

            # Average over testing trials
            X_test = X[test_idx]  # keep trials separate
            X_test_avg = np.nanmean(X_test, axis=0)
            X_test_avg -= X_train_scale


            # Fit dPCA on training average
            dpca = dPCA.dPCA(labels=marginalization, regularizer=lam)
            dpca.protect = ['t']
            Z = dpca.fit_transform(X_train_avg, X[train_idx])

            # Compute reconstruction trial-averages from components of training data
            X_recons = []
            for marg in Z:
                X_marg = dpca.inverse_transform(Z[marg], marg)
                X_recons.append(X_marg)
            X_reconstructed = np.sum(X_recons, axis=0)

            # Evaluate reconstruction of the test average
            mse = np.mean((X_test_avg - X_reconstructed) ** 2)
            total_var = np.mean(X_test_avg ** 2)
            explained_ratio = 1 - mse / total_var
            scores.append(explained_ratio)

        # average out crossvalidations
        mean_score = np.mean(scores)
        all_scores[lam] = mean_score
        if mean_score > best_score:
            best_score = mean_score
            best_lambda = lam

    return best_lambda
#+end_src

#+RESULTS:

#+begin_src ipython
print(mouse)
X = np.vstack(X_mouse[0][1])
y = y_dfs[(y_dfs.mouse==mouse) & (y_dfs.DAY == 'last')]
print(X.shape, y.shape)

idx_off = (y.laser==0)

#  n_trials, n_neurons, Z1, Z2, ..., n_time, lets do odor pair * choice
X_dpca = np.zeros((4, 2, int(X.shape[0]/4), X.shape[-2],  X.shape[-1]))
print('X_dpca', X_dpca.shape)

for i in range(4):
    for j in range(2):
        dum = X[(y.odor_pair==i) & (y.choice==j)]
        mean_dum = np.mean(dum, axis=0)[np.newaxis]

        X_dpca[i, j, :dum.shape[0]] = dum
        X_dpca[i, j, dum.shape[0]:] = mean_dum

X_dpca = np.transpose(X_dpca, (2, 3, 0, 1, 4))
X_dpca_avg = np.nanmean(X_dpca, axis=0)
print(X_dpca.shape, X_dpca_avg.shape)
#+end_src

#+RESULTS:
: JawsM15
: (96, 693, 84) (96, 17)
: X_dpca (4, 2, 24, 693, 84)
: (24, 693, 4, 2, 84) (693, 4, 2, 84)

#+begin_src ipython
reg = crossval_dpca_reg(X_dpca, marginalization='pct', lambdas=np.logspace(-3, 3, 10))
print(reg)
#+end_src

#+RESULTS:
:RESULTS:
: (24, 693, 4, 2, 84)
# [goto error]
#+begin_example
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[44], line 1
----> 1 reg = crossval_dpca_reg(X_dpca, marginalization='pct', lambdas=np.logspace(-3, 3, 10))
      2 print(reg)

Cell In[42], line 44, in crossval_dpca_reg(X_trials, marginalization, lambdas)
     42 dpca = dPCA.dPCA(labels=marginalization, regularizer=lam)
     43 dpca.protect = ['t']
---> 44 Z = dpca.fit_transform(X_train_avg, X[train_idx])
     46 # Compute reconstruction trial-averages from components of training data
     47 X_recons = []

File ~/dual_task/dual_data/src/dPCA/dPCA.py:164, in dPCA.fit_transform(self, X, trialX)
    147 def fit_transform(self, X, trialX=None):
    148     """Fit the model with X and apply the dimensionality reduction on X.
    149
    150     Parameters
   (...)
    162
    163     """
--> 164     self._fit(X,trialX=trialX)
    166     return self.transform(X)

File ~/dual_task/dual_data/src/dPCA/dPCA.py:566, in dPCA._fit(self, X, trialX, mXs, center, SVD, optimize)
    563     regX, regmXs, pregX = X, mXs, pinv(X.reshape((n_features,-1)))
    565 # compute closed-form solution
--> 566 self.P, self.D = self._randomized_dpca(regX,regmXs,pinvX=pregX)

File ~/dual_task/dual_data/src/dPCA/dPCA.py:468, in dPCA._randomized_dpca(self, X, mXs, pinvX)
    466     U,s,V = randomized_svd(np.dot(C,rX),n_components=self.n_components[key],n_iter=self.n_iter,random_state=np.random.randint(10e5))
    467 else:
--> 468     U,s,V = randomized_svd(np.dot(C,rX),n_components=self.n_components,n_iter=self.n_iter,random_state=np.random.randint(10e5))
    470 P[key] = U
    471 D[key] = np.dot(U.T,C).T

File ~/mambaforge/envs/torch/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:218, in validate_params.<locals>.decorator.<locals>.wrapper(*args, **kwargs)
    212 try:
    213     with config_context(
    214         skip_parameter_validation=(
    215             prefer_skip_nested_validation or global_skip_validation
    216         )
    217     ):
--> 218         return func(*args, **kwargs)
    219 except InvalidParameterError as e:
    220     # When the function is just a wrapper around an estimator, we allow
    221     # the function to delegate validation to the estimator, but we replace
    222     # the name of the estimator by the name of the function in the error
    223     # message to avoid confusion.
    224     msg = re.sub(
    225         r"parameter of \w+ must be",
    226         f"parameter of {func.__qualname__} must be",
    227         str(e),
    228     )

File ~/mambaforge/envs/torch/lib/python3.10/site-packages/sklearn/utils/extmath.py:517, in randomized_svd(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state, svd_lapack_driver)
    360 @validate_params(
    361     {
    362         "M": ["array-like", "sparse matrix"],
   (...)
    384     svd_lapack_driver="gesdd",
    385 ):
    386     """Compute a truncated randomized SVD.
    387
    388     This method solves the fixed-rank approximation problem described in [1]_
   (...)
    515     ((3, 2), (2,), (2, 4))
    516     """
--> 517     M = check_array(M, accept_sparse=True)
    518     return _randomized_svd(
    519         M,
    520         n_components=n_components,
   (...)
    527         svd_lapack_driver=svd_lapack_driver,
    528     )

File ~/mambaforge/envs/torch/lib/python3.10/site-packages/sklearn/utils/validation.py:1105, in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)
   1099     raise ValueError(
   1100         f"Found array with dim {array.ndim},"
   1101         f" while dim <= 2 is required{context}."
   1102     )
   1104 if ensure_all_finite:
-> 1105     _assert_all_finite(
   1106         array,
   1107         input_name=input_name,
   1108         estimator_name=estimator_name,
   1109         allow_nan=ensure_all_finite == "allow-nan",
   1110     )
   1112 if copy:
   1113     if _is_numpy_namespace(xp):
   1114         # only make a copy if `array` and `array_orig` may share memory`

File ~/mambaforge/envs/torch/lib/python3.10/site-packages/sklearn/utils/validation.py:120, in _assert_all_finite(X, allow_nan, msg_dtype, estimator_name, input_name)
    117 if first_pass_isfinite:
    118     return
--> 120 _assert_all_finite_element_wise(
    121     X,
    122     xp=xp,
    123     allow_nan=allow_nan,
    124     msg_dtype=msg_dtype,
    125     estimator_name=estimator_name,
    126     input_name=input_name,
    127 )

File ~/mambaforge/envs/torch/lib/python3.10/site-packages/sklearn/utils/validation.py:169, in _assert_all_finite_element_wise(X, xp, allow_nan, msg_dtype, estimator_name, input_name)
    152 if estimator_name and input_name == "X" and has_nan_error:
    153     # Improve the error message on how to handle missing values in
    154     # scikit-learn.
    155     msg_err += (
    156         f"\n{estimator_name} does not accept missing values"
    157         " encoded as NaN natively. For supervised learning, you might want"
   (...)
    167         "#estimators-that-handle-nan-values"
    168     )
--> 169 raise ValueError(msg_err)

ValueError: Input contains NaN.
#+end_example
:END:

#+begin_src ipython
dpca = dPCA.dPCA(labels='pct', n_components=2, regularizer=1.0 / X_dpca.shape[1])
dpca.protect = ['t']

Z = dpca_cv(X_dpca, dpca, n_splits=X_dpca.shape[0])
#+end_src

#+RESULTS:
:RESULTS:
: (23, 693, 4, 2, 84) (1, 693, 4, 2, 84)
# [goto error]
#+begin_example
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[45], line 4
      1 dpca = dPCA.dPCA(labels='pct', n_components=2, regularizer=1.0 / X_dpca.shape[1])
      2 dpca.protect = ['t']
----> 4 Z = dpca_cv(X_dpca, dpca, n_splits=X_dpca.shape[0])

Cell In[41], line 21, in dpca_cv(X, dPCA, n_splits)
     18     X_test_avg = np.nanmean(X_test, axis=0)
     19     X_test_avg -= X_train_scale
---> 21     dPCA.fit_transform(X_train_avg, X_train)
     22     Z_test.append(dPCA.transform(X_test_avg))
     24 result = defaultdict(list)

File ~/dual_task/dual_data/src/dPCA/dPCA.py:164, in dPCA.fit_transform(self, X, trialX)
    147 def fit_transform(self, X, trialX=None):
    148     """Fit the model with X and apply the dimensionality reduction on X.
    149
    150     Parameters
   (...)
    162
    163     """
--> 164     self._fit(X,trialX=trialX)
    166     return self.transform(X)

File ~/dual_task/dual_data/src/dPCA/dPCA.py:566, in dPCA._fit(self, X, trialX, mXs, center, SVD, optimize)
    563     regX, regmXs, pregX = X, mXs, pinv(X.reshape((n_features,-1)))
    565 # compute closed-form solution
--> 566 self.P, self.D = self._randomized_dpca(regX,regmXs,pinvX=pregX)

File ~/dual_task/dual_data/src/dPCA/dPCA.py:468, in dPCA._randomized_dpca(self, X, mXs, pinvX)
    466     U,s,V = randomized_svd(np.dot(C,rX),n_components=self.n_components[key],n_iter=self.n_iter,random_state=np.random.randint(10e5))
    467 else:
--> 468     U,s,V = randomized_svd(np.dot(C,rX),n_components=self.n_components,n_iter=self.n_iter,random_state=np.random.randint(10e5))
    470 P[key] = U
    471 D[key] = np.dot(U.T,C).T

File ~/mambaforge/envs/torch/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:218, in validate_params.<locals>.decorator.<locals>.wrapper(*args, **kwargs)
    212 try:
    213     with config_context(
    214         skip_parameter_validation=(
    215             prefer_skip_nested_validation or global_skip_validation
    216         )
    217     ):
--> 218         return func(*args, **kwargs)
    219 except InvalidParameterError as e:
    220     # When the function is just a wrapper around an estimator, we allow
    221     # the function to delegate validation to the estimator, but we replace
    222     # the name of the estimator by the name of the function in the error
    223     # message to avoid confusion.
    224     msg = re.sub(
    225         r"parameter of \w+ must be",
    226         f"parameter of {func.__qualname__} must be",
    227         str(e),
    228     )

File ~/mambaforge/envs/torch/lib/python3.10/site-packages/sklearn/utils/extmath.py:517, in randomized_svd(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state, svd_lapack_driver)
    360 @validate_params(
    361     {
    362         "M": ["array-like", "sparse matrix"],
   (...)
    384     svd_lapack_driver="gesdd",
    385 ):
    386     """Compute a truncated randomized SVD.
    387
    388     This method solves the fixed-rank approximation problem described in [1]_
   (...)
    515     ((3, 2), (2,), (2, 4))
    516     """
--> 517     M = check_array(M, accept_sparse=True)
    518     return _randomized_svd(
    519         M,
    520         n_components=n_components,
   (...)
    527         svd_lapack_driver=svd_lapack_driver,
    528     )

File ~/mambaforge/envs/torch/lib/python3.10/site-packages/sklearn/utils/validation.py:1105, in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)
   1099     raise ValueError(
   1100         f"Found array with dim {array.ndim},"
   1101         f" while dim <= 2 is required{context}."
   1102     )
   1104 if ensure_all_finite:
-> 1105     _assert_all_finite(
   1106         array,
   1107         input_name=input_name,
   1108         estimator_name=estimator_name,
   1109         allow_nan=ensure_all_finite == "allow-nan",
   1110     )
   1112 if copy:
   1113     if _is_numpy_namespace(xp):
   1114         # only make a copy if `array` and `array_orig` may share memory`

File ~/mambaforge/envs/torch/lib/python3.10/site-packages/sklearn/utils/validation.py:120, in _assert_all_finite(X, allow_nan, msg_dtype, estimator_name, input_name)
    117 if first_pass_isfinite:
    118     return
--> 120 _assert_all_finite_element_wise(
    121     X,
    122     xp=xp,
    123     allow_nan=allow_nan,
    124     msg_dtype=msg_dtype,
    125     estimator_name=estimator_name,
    126     input_name=input_name,
    127 )

File ~/mambaforge/envs/torch/lib/python3.10/site-packages/sklearn/utils/validation.py:169, in _assert_all_finite_element_wise(X, xp, allow_nan, msg_dtype, estimator_name, input_name)
    152 if estimator_name and input_name == "X" and has_nan_error:
    153     # Improve the error message on how to handle missing values in
    154     # scikit-learn.
    155     msg_err += (
    156         f"\n{estimator_name} does not accept missing values"
    157         " encoded as NaN natively. For supervised learning, you might want"
   (...)
    167         "#estimators-that-handle-nan-values"
    168     )
--> 169 raise ValueError(msg_err)

ValueError: Input contains NaN.
#+end_example
:END:

#+begin_src ipython
print(Z['p'].shape)
#+end_src

#+RESULTS:
: (2, 2, 2, 84)

#+begin_src ipython
dpca = dPCA.dPCA(labels='pct', n_components=2, regularizer=reg)
dpca.protect = ['t']

# axes = (1, 2, 3)
# X_dpca -= np.mean(X_dpca, axis=axes, keepdims=True)

axes = (1, 2)
X_dpca_avg -= np.mean(X_dpca_avg, axis=axes, keepdims=True)

Z = dpca.fit_transform(X_dpca_avg, X_dpca)
print(Z['p'].shape)
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
: ---------------------------------------------------------------------------
: NameError                                 Traceback (most recent call last)
: Cell In[47], line 1
: ----> 1 dpca = dPCA.dPCA(labels='pct', n_components=2, regularizer=reg)
:       2 dpca.protect = ['t']
:       4 # axes = (1, 2, 3)
:       5 # X_dpca -= np.mean(X_dpca, axis=axes, keepdims=True)
:
: NameError: name 'reg' is not defined
:END:

#+begin_src ipython
from src.common.plot_utils import add_vlines

ls = ['-', '--']
label = ['lick', 'nolick']
pc = ['Pair', 'Choice', 'Choice * Time']
xtime = np.linspace(0, 14, 84)

fig, ax = plt.subplots(1, 3, figsize=[3* width, height])

n_comp = 0
pair = 0
for i in range(2):
        ax[0].plot(xtime, Z['p'][n_comp][pair][i], ls = ls[i], label=label[i])
        ax[1].plot(xtime, Z['c'][n_comp][pair][i], ls = ls[i], label=label[i])
        ax[2].plot(xtime, Z['ct'][n_comp][pair][i], ls = ls[i], label=label[i])

for k in range(3):
        add_vlines(ax[k])
        ax[k].set_xlim([0, 12])
        ax[k].axhline(0, ls=':')
        ax[k].set_xlabel('Time (s)')
        ax[k].set_ylabel('%s' %pc[k])

plt.legend(fontsize=14, frameon=0)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/dpca/figure_27.png]]

#+begin_src ipython
from src.common.plot_utils import add_vlines

ls = ['-', '--']
label = ['lick', 'nolick']
pc = ['Pair', 'Choice', 'Choice * Time']

xtime = np.linspace(0, 14, 84)

fig, ax = plt.subplots(1, 3, figsize=[3* width, height])

pair = [0, 2]
for i in range(2):
        ax[0].plot(xtime, (Z['p'][0][pair[0]][i] - Z['p'][0][pair[1]][i])/2, ls = ls[i], label=label[i])
        ax[1].plot(xtime, (Z['c'][0][pair[0]][i] + Z['c'][0][pair[1]][i])/2, ls = ls[i], label=label[i])
        ax[2].plot(xtime, (Z['ct'][0][pair[0]][i] + Z['ct'][0][pair[1]][i])/2, ls = ls[i], label=label[i])

for k in range(3):
        add_vlines(ax[k])
        ax[k].set_xlim([0, 12])
        ax[k].axhline(0, ls=':')
        ax[k].set_xlabel('Time (s)')
        ax[k].set_ylabel('%s' % pc[k])

plt.legend(fontsize=14, frameon=0)
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
: ---------------------------------------------------------------------------
: IndexError                                Traceback (most recent call last)
: Cell In[49], line 13
:      11 pair = [0, 2]
:      12 for i in range(2):
: ---> 13         ax[0].plot(xtime, (Z['p'][0][pair[0]][i] - Z['p'][0][pair[1]][i])/2, ls = ls[i], label=label[i])
:      14         ax[1].plot(xtime, (Z['c'][0][pair[0]][i] + Z['c'][0][pair[1]][i])/2, ls = ls[i], label=label[i])
:      15         ax[2].plot(xtime, (Z['ct'][0][pair[0]][i] + Z['ct'][0][pair[1]][i])/2, ls = ls[i], label=label[i])
:
: IndexError: index 2 is out of bounds for axis 0 with size 2
[[file:./figures/dpca/figure_28.png]]
:END:

#+begin_src ipython
from src.common.plot_utils import add_vlines

ls = ['-', '--']
label = ['lick', 'nolick']
pc = ['Pair', 'Choice', 'Choice * Time']

xtime = np.linspace(0, 14, 84)

fig, ax = plt.subplots(1, 3, figsize=[3* width, height])

pair = [1, 3]
for i in range(2):
        ax[0].plot(xtime, (Z['p'][0][pair[0]][i] - Z['p'][0][pair[1]][i])/2, ls = ls[i], label=label[i])
        ax[1].plot(xtime, (Z['c'][0][pair[0]][i] + Z['c'][0][pair[1]][i])/2, ls = ls[i], label=label[i])
        ax[2].plot(xtime, (Z['ct'][0][pair[0]][i] + Z['ct'][0][pair[1]][i])/2, ls = ls[i], label=label[i])

for k in range(3):
        add_vlines(ax[k])
        ax[k].set_xlim([0, 12])
        ax[k].axhline(0, ls=':')
        ax[k].set_xlabel('Time (s)')
        ax[k].set_ylabel('%s' % pc[k])

plt.legend(fontsize=14, frameon=0)
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
: ---------------------------------------------------------------------------
: IndexError                                Traceback (most recent call last)
: Cell In[50], line 13
:      11 pair = [1, 3]
:      12 for i in range(2):
: ---> 13         ax[0].plot(xtime, (Z['p'][0][pair[0]][i] - Z['p'][0][pair[1]][i])/2, ls = ls[i], label=label[i])
:      14         ax[1].plot(xtime, (Z['c'][0][pair[0]][i] + Z['c'][0][pair[1]][i])/2, ls = ls[i], label=label[i])
:      15         ax[2].plot(xtime, (Z['ct'][0][pair[0]][i] + Z['ct'][0][pair[1]][i])/2, ls = ls[i], label=label[i])
:
: IndexError: index 3 is out of bounds for axis 0 with size 2
[[file:./figures/dpca/figure_29.png]]
:END:


#+begin_src ipython

#+end_src

#+RESULTS:

* Sample * choice * time

#+begin_src ipython
i_mouse = 5
print(options['mice'][i_mouse])
#+end_src

#+RESULTS:
: ChRM04

#+begin_src ipython
from collections import defaultdict

Z_day = []
EV_day = []

for i_day, day in enumerate(['first', 'last']):
    X = np.vstack(X_mouse[i_mouse][i_day])
    mouse = options['mice'][i_mouse]
    y = y_dfs[(y_dfs.mouse==mouse) & (y_dfs.DAY == day)]

    print(X.shape, y.shape, y.day.unique())

    idx_off = (y.laser==0)

    #  n_trials, n_neurons, Z1, Z2, ..., n_time, lets do odor pair * choice
    X_dpca = np.zeros((2, 2, int(X.shape[0]/2), X.shape[-2],  X.shape[-1]))
    print('X_dpca', X_dpca.shape)

    for i in range(2):
        for j in range(2):
            dum = X[(y.sample_odor==i) & (y.choice==j)]
            mean_dum = np.mean(dum, axis=0)[np.newaxis]

            X_dpca[i, j, :dum.shape[0]] = dum
            X_dpca[i, j, dum.shape[0]:] = mean_dum

    X_dpca = np.transpose(X_dpca, (2, 3, 0, 1, 4))
    X_dpca_avg = np.nanmean(X_dpca, axis=0)
    print(X_dpca.shape, X_dpca_avg.shape)

    dpca = dPCA.dPCA(labels='pct', n_components=2, regularizer=1/X.shape[0])
    dpca.protect = ['t']

    axes = (1, 2)
    X_dpca_avg -= np.mean(X_dpca_avg, axis=axes, keepdims=True)

    Z = dpca.fit_transform(X_dpca_avg, X_dpca)
    EV = dpca.explained_variance_ratio_

    print(Z['p'].shape)

    Z_day.append(Z)
    EV_day.append(EV)

#+end_src

#+RESULTS:
: (288, 668, 84) (288, 17) [1. 2. 3.]
: X_dpca (2, 2, 144, 668, 84)
: (144, 668, 2, 2, 84) (668, 2, 2, 84)
: (2, 2, 2, 84)
: (288, 668, 84) (288, 17) [4. 5. 6.]
: X_dpca (2, 2, 144, 668, 84)
: (144, 668, 2, 2, 84) (668, 2, 2, 84)
: (2, 2, 2, 84)

#+begin_src ipython
Z_all = defaultdict(list)
for d in Z_day:
    for k, v in d.items():
        Z_all[k].append(v)

print(np.array(Z_all['p']).shape)
#+end_src

#+RESULTS:
: (2, 2, 2, 2, 84)

#+begin_src ipython
EV_all = defaultdict(list)
for d in EV_day:
    for k, v in d.items():
        print(k, np.round(v[0]*100))
        EV_all[k].append(v)
#+end_src

#+RESULTS:

#+begin_src ipython
from collections import defaultdict
n_shuffles = 0
explained_shuffled = []

Z_shuf = []
for _ in range(n_shuffles):
    # Permute labels independently
    shuffled_sample = np.random.permutation(y.sample_odor.values)
    shuffled_choice = np.random.permutation(y.choice.values)

    # Reconstruct X_dpca with shuffled labels
    X_dpca_shuf = np.zeros((2, 2, int(X.shape[0]/2), X.shape[-2],  X.shape[-1]))
    idxs = np.arange(X.shape[0])

    for i in range(2):
        for j in range(2):
            mask = (shuffled_sample == i) & (shuffled_choice == j)
            dum = X[mask]
            mean_dum = np.mean(dum, axis=0)[np.newaxis]
            X_dpca_shuf[i, j, :dum.shape[0]] = dum
            X_dpca_shuf[i, j, dum.shape[0]:] = mean_dum

    X_dpca_shuf = np.transpose(X_dpca_shuf, (2, 3, 0, 1, 4))
    X_dpca_shuf_avg = np.nanmean(X_dpca_shuf, axis=0)
    X_dpca_shuf_avg -= np.mean(X_dpca_shuf_avg, axis=axes, keepdims=True)

    Z_shuf.append(dpca.fit_transform(X_dpca_shuf_avg, X_dpca_shuf))
    # explained_shuffled.append(dpca.explained_variance_ratio_.copy())

result = defaultdict(list)
for d in Z_shuf:
    for k, v in d.items():
        result[k].append(v)

mean_shuf = {k: np.mean(vs, axis=0) for k, vs in result.items()}
std_shuf = {k: np.std(vs, axis=0) for k, vs in result.items()}
#+end_src

#+RESULTS:

#+begin_src ipython
# Z_scored = {k: (Z[k] - mean_shuf[k])  for k in Z}
#+end_src

#+RESULTS:

#+begin_src ipython
from src.common.plot_utils import add_vlines

ls = ['-', '--']
label = ['lick', 'nolick']
pc = ['Sample', 'Choice', 'Choice * Time']
xtime = np.linspace(0, 14, 84)

fig, ax = plt.subplots(1, 3, figsize=[3* width, height])

n_comp = 0
pair = 0
for i in range(2):
        ax[0].plot(xtime, Z['p'][n_comp][pair][i], ls = ls[i], label=label[i])
        ax[1].plot(xtime, Z['c'][n_comp][pair][i], ls = ls[i], label=label[i])
        ax[2].plot(xtime, Z['ct'][n_comp][pair][i], ls = ls[i], label=label[i])

for k in range(3):
        add_vlines(ax[k])
        ax[k].set_xlim([0, 12])
        ax[k].axhline(0, ls=':')
        ax[k].set_xlabel('Time (s)')
        ax[k].set_ylabel('%s' %pc[k])

plt.legend(fontsize=14, frameon=0)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/dpca/figure_27.png]]

#+begin_src ipython
from src.common.plot_utils import add_vlines

ls = ['-', '--']
color = ['r', 'b']
label = ['lick', 'nolick']
pc = ['Sample', 'Choice', 'Choice * Time']
xtime = np.linspace(0, 14, 84)

fig, ax = plt.subplots(1, 3, figsize=[3* width, height])

n_comp = 0
pair = 0
for i in range(2):
        for j in range(2):
                ax[0].plot(xtime, (Z_day[j]['p'][n_comp][pair][i] - Z_day[j]['p'][n_comp][1][i])/2, ls = ls[i], label=label[i], color=color[i], alpha=j/2+0.5)
                ax[1].plot(xtime, (Z_day[j]['c'][n_comp][pair][i] + Z_day[j]['c'][n_comp][1][i])/2, ls = ls[i], label=label[i], color=color[i], alpha=j/2+0.5)
                ax[2].plot(xtime, (Z_day[j]['ct'][n_comp][pair][i] + Z_day[j]['p'][j][n_comp][1][i])/2, ls = ls[i], label=label[i], color=color[i], alpha=j/2+0.5)

for k in range(3):
        add_vlines(ax[k])
        ax[k].set_xlim([0, 12])
        ax[k].axhline(0, ls=':')
        ax[k].set_xlabel('Time (s)')
        ax[k].set_ylabel('%s' %pc[k])

plt.legend(fontsize=14, frameon=0)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/dpca/figure_30.png]]


#+begin_src ipython

#+end_src

#+RESULTS:


* Sample * test * time

#+begin_src ipython
i_mouse = 0
print(options['mice'][i_mouse])
#+end_src

#+RESULTS:
: JawsM15

#+begin_src ipython
from collections import defaultdict

Z_day = []

for i_day, day in enumerate(['first', 'last']):
    X = np.vstack(X_mouse[i_mouse][i_day])
    mouse = options['mice'][i_mouse]
    y = y_dfs[(y_dfs.mouse==mouse) & (y_dfs.DAY == day)]
    print(X.shape, y.shape, y.day.unique())

    idx_off = (y.laser==0)

    #  n_trials, n_neurons, Z1, Z2, ..., n_time, lets do odor pair * choice
    X_dpca = np.zeros((2, 2, int(X.shape[0]/2), X.shape[-2],  X.shape[-1]))
    print('X_dpca', X_dpca.shape)

    for i in range(2):
        for j in range(2):
            dum = X[(y.sample_odor==i) & (y.test_odor==j)]
            mean_dum = np.mean(dum, axis=0)[np.newaxis]

            X_dpca[i, j, :dum.shape[0]] = dum
            X_dpca[i, j, dum.shape[0]:] = mean_dum

    X_dpca = np.transpose(X_dpca, (2, 3, 0, 1, 4))
    X_dpca_avg = np.nanmean(X_dpca, axis=0)
    print(X_dpca.shape, X_dpca_avg.shape)

    dpca = dPCA.dPCA(labels='pct', n_components=2, regularizer=1/X.shape[0])
    dpca.protect = ['t']

    axes = (1, 2)
    X_dpca_avg -= np.mean(X_dpca_avg, axis=axes, keepdims=True)

    Z = dpca.fit_transform(X_dpca_avg, X_dpca)
    print(Z['p'].shape)

    Z_day.append(Z)
#+end_src

#+RESULTS:
: (288, 693, 84) (288, 17) [1. 2. 3.]
: X_dpca (2, 2, 144, 693, 84)
: (144, 693, 2, 2, 84) (693, 2, 2, 84)
: (2, 2, 2, 84)
: (288, 693, 84) (288, 17) [4. 5. 6.]
: X_dpca (2, 2, 144, 693, 84)
: (144, 693, 2, 2, 84) (693, 2, 2, 84)
: (2, 2, 2, 84)

#+begin_src ipython
Z_all = defaultdict(list)
for d in Z_day:
    for k, v in d.items():
        Z_all[k].append(v)

print(np.array(Z_all['p']).shape)
#+end_src

#+RESULTS:
: (2, 2, 2, 2, 84)

#+begin_src ipython
from src.common.plot_utils import add_vlines

ls = ['-', '--']
color = ['r', 'b']
label = ['lick', 'nolick']
pc = ['Sample', 'Test', 'Test * Time']
xtime = np.linspace(0, 14, 84)

fig, ax = plt.subplots(1, 3, figsize=[3* width, height])

n_comp = 0
pair = 0
for i in range(2):
        for j in range(2):
                ax[0].plot(xtime, (Z_day[j]['p'][n_comp][pair][i] - Z_day[j]['p'][n_comp][1][i])/2, ls = ls[i], label=label[i], color=color[i], alpha=j/2+0.5)
                ax[1].plot(xtime, (Z_day[j]['c'][n_comp][pair][i] + Z_day[j]['c'][n_comp][1][i])/2, ls = ls[i], label=label[i], color=color[i], alpha=j/2+0.5)
                ax[2].plot(xtime, (Z_day[j]['ct'][n_comp][pair][i] + Z_day[j]['p'][j][n_comp][1][i])/2, ls = ls[i], label=label[i], color=color[i], alpha=j/2+0.5)

for k in range(3):
        add_vlines(ax[k])
        ax[k].set_xlim([0, 12])
        ax[k].axhline(0, ls=':')
        ax[k].set_xlabel('Time (s)')
        ax[k].set_ylabel('%s' %pc[k])

plt.legend(fontsize=14, frameon=0)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/dpca/figure_35.png]]

#+begin_src ipython

#+end_src

#+RESULTS:

* Sample * task * time

#+begin_src ipython
i_mouse = 5
print(options['mice'][i_mouse])
#+end_src

#+RESULTS:
: ChRM04

#+begin_src ipython
from collections import defaultdict

Z_day = []
tasks = ['DPA', 'DualGo', 'DualNoGo']
for i_day, day in enumerate(['first', 'last']):
    X = np.vstack(X_mouse[i_mouse][i_day])
    mouse = options['mice'][i_mouse]
    y = y_dfs[(y_dfs.mouse==mouse) & (y_dfs.DAY == day)]

    idx_off = (y.laser==0)

    #  n_trials, n_neurons, Z1, Z2, ..., n_time, lets do odor pair * choice
    X_dpca = np.zeros((2, 3, int(X.shape[0]/3), X.shape[-2],  X.shape[-1]))

    for i in range(2):
        for j in range(3):
            dum = X[(y.sample_odor==i) & (y.tasks==tasks[j])]
            mean_dum = np.mean(dum, axis=0)[np.newaxis]

            X_dpca[i, j, :dum.shape[0]] = dum
            X_dpca[i, j, dum.shape[0]:] = mean_dum

    X_dpca = np.transpose(X_dpca, (2, 3, 0, 1, 4))
    X_dpca_avg = np.nanmean(X_dpca, axis=0)

    dpca = dPCA.dPCA(labels='pct', n_components=2, regularizer=1/X.shape[0])
    dpca.protect = ['t']

    axes = (1, 2)
    X_dpca_avg -= np.mean(X_dpca_avg, axis=axes, keepdims=True)

    Z = dpca.fit_transform(X_dpca_avg, X_dpca)
    Z_day.append(Z)
#+end_src

#+RESULTS:
: (288, 668, 84) (288, 17) [1. 2. 3.]
: X_dpca (2, 3, 96, 668, 84)
: (96, 668, 2, 3, 84) (668, 2, 3, 84)
: (2, 2, 3, 84)
: (288, 668, 84) (288, 17) [4. 5. 6.]
: X_dpca (2, 3, 96, 668, 84)
: (96, 668, 2, 3, 84) (668, 2, 3, 84)
: (2, 2, 3, 84)

#+begin_src ipython
Z_all = defaultdict(list)
for d in Z_day:
    for k, v in d.items():
        Z_all[k].append(v)

print(np.array(Z_all['p']).shape)
#+end_src

#+RESULTS:
: (2, 2, 2, 3, 84)

#+begin_src ipython
from src.common.plot_utils import add_vlines

ls = ['-', '--', ':']
color = ['r', 'b', 'g']
label = ['DPA', 'Go', 'NoGo']
pc = ['Sample', 'Task', 'Task * Time']
xtime = np.linspace(0, 14, 84)

fig, ax = plt.subplots(1, 3, figsize=[3* width, height])

n_comp = 0
pair = 0
for i in range(1):
        for j in range(2):
                ax[0].plot(xtime, (Z_day[j]['p'][n_comp][pair][i] - Z_day[j]['p'][n_comp][1][i])/2, ls = ls[i], label=label[i], color=color[i], alpha=j/2+0.5)
                ax[1].plot(xtime, (Z_day[j]['c'][n_comp][pair][i] + Z_day[j]['c'][n_comp][1][i])/2, ls = ls[i], label=label[i], color=color[i], alpha=j/2+0.5)
                ax[2].plot(xtime, (Z_day[j]['ct'][n_comp][pair][i] + Z_day[j]['p'][j][n_comp][1][i])/2, ls = ls[i], label=label[i], color=color[i], alpha=j/2+0.5)

for k in range(3):
        add_vlines(ax[k])
        ax[k].set_xlim([0, 12])
        ax[k].axhline(0, ls=':')
        ax[k].set_xlabel('Time (s)')
        ax[k].set_ylabel('%s' %pc[k])

plt.legend(fontsize=14, frameon=0)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/dpca/figure_40.png]]

#+begin_src ipython

#+end_src

#+RESULTS:

* Sample * Test * Choice * Time

#+begin_src ipython
i_mouse = 0
print(options['mice'][i_mouse])
#+end_src

#+RESULTS:
: JawsM15

#+begin_src ipython
from collections import defaultdict

# mice = ['ChRM04','JawsM15', 'JawsM18', 'ACCM03', 'ACCM04']
tasks = ['DPA']

Z_day = []
for i_day, day in enumerate(['first', 'last']):

    Z_mouse = []
    for i_mouse, mouse in enumerate(mice):
        mouse = options['mice'][i_mouse]


        X = np.vstack(X_mouse[i_mouse][i_day])
        y = y_dfs[(y_dfs.mouse==mouse) & (y_dfs.DAY == day)]

        X_dpca = np.zeros((2, 2, 2, int(X.shape[0]/4), X.shape[-2],  X.shape[-1]))

        for i in range(2):
            for j in range(2):
                for k in range(2):
                    dum = X[(y.sample_odor==i) & (y.test_odor==j) & (y.choice==k)]
                    if dum.shape[0]>0:
                        mean_dum = np.mean(dum, axis=0)[np.newaxis]

                        X_dpca[i, j, k, :dum.shape[0]] = dum
                        X_dpca[i, j, k, dum.shape[0]:] = mean_dum


        X_dpca = np.transpose(X_dpca, (3, 4, 0, 1, 2, 5))
        X_dpca_avg = np.nanmean(X_dpca, axis=0)

        dpca = dPCA.dPCA(labels='pcdt', n_components=2, regularizer=1/X.shape[0])
        dpca.protect = ['t']

        axes = (1, 2, 3)
        X_dpca_avg -= np.mean(X_dpca_avg, axis=axes, keepdims=True)

        Z = dpca.fit_transform(X_dpca_avg, X_dpca)
        Z_mouse.append(Z)

    Z_all = defaultdict(list)
    for d in Z_mouse:
        for k, v in d.items():
            Z_all[k].append(v)

    Z_day.append({k: np.mean(vs, axis=0) for k, vs in Z_all.items()})
#+end_src

#+RESULTS:

#+begin_src ipython
from src.common.plot_utils import add_vlines

ls = ['-', '--', ':']
color = ['r', 'b', 'g']
label = ['Pair', 'Unpair']
pc = ['Sample', 'Test', 'Choice']
xtime = np.linspace(0, 14, 84)

fig, ax = plt.subplots(1, 3, figsize=[3* width, height])

n_comp = 0
pair = 0
k = 1

for i in range(2):
        for j in range(2):
                ax[0].plot(xtime, (Z_day[j]['p'][n_comp][pair][i][k] - Z_day[j]['p'][n_comp][1][i][k])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)
                ax[1].plot(xtime, (Z_day[j]['c'][n_comp][pair][i][k] + Z_day[j]['c'][n_comp][1][i][k])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)
                ax[2].plot(xtime, (Z_day[j]['d'][n_comp][pair][i][k] + Z_day[j]['p'][j][n_comp][1][i][k])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)

for k in range(3):
        add_vlines(ax[k])
        ax[k].set_xlim([0, 12])
        ax[k].axhline(0, ls=':')
        ax[k].set_xlabel('Time (s)')
        ax[k].set_ylabel('%s' %pc[k])

plt.legend(fontsize=14, frameon=0)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/dpca/figure_44.png]]

#+begin_src ipython
from src.common.plot_utils import add_vlines

ls = ['-', '--', ':']
color = ['r', 'b', 'g']
label = ['Lick', 'No Lick']
pc = ['Sample', 'Test', 'Choice']
xtime = np.linspace(0, 14, 84)

fig, ax = plt.subplots(1, 3, figsize=[3* width, height])

n_comp = 0
pair = 0
k = 0

for i in range(2):
        for j in range(2):
                ax[0].plot(xtime, (Z_day[j]['p'][n_comp][pair][i][k] - Z_day[j]['p'][n_comp][1][i][k])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)
                ax[1].plot(xtime, (Z_day[j]['c'][n_comp][pair][i][k] + Z_day[j]['c'][n_comp][1][i][k])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)
                ax[2].plot(xtime, (Z_day[j]['d'][n_comp][pair][i][k] + Z_day[j]['p'][j][n_comp][1][i][k])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)

for k in range(3):
        add_vlines(ax[k])
        ax[k].set_xlim([0, 12])
        ax[k].axhline(0, ls=':')
        ax[k].set_xlabel('Time (s)')
        ax[k].set_ylabel('%s' %pc[k])

plt.legend(fontsize=14, frameon=0)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/dpca/figure_45.png]]

#+begin_src ipython

#+end_src

#+RESULTS:

* Sample * Test * Task * Time

#+begin_src ipython
i_mouse = 7
print(options['mice'][i_mouse])
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
: ---------------------------------------------------------------------------
: IndexError                                Traceback (most recent call last)
: Cell In[204], line 2
:       1 i_mouse = 7
: ----> 2 print(options['mice'][i_mouse])
:
: IndexError: list index out of range
:END:

#+begin_src ipython
from collections import defaultdict

Z_day = []
tasks = ['DPA',]
for i_day, day in enumerate(['first', 'last']):
    X = np.vstack(X_mouse[i_mouse][i_day])
    mouse = options['mice'][i_mouse]
    y = y_dfs[(y_dfs.mouse==mouse) & (y_dfs.DAY == day)]

    idx_off = (y.laser==0)

    #  n_trials, n_neurons, Z1, Z2, ..., n_time, lets do odor pair * choice
    X_dpca = np.zeros((2, 2, 3, int(X.shape[0]/12), X.shape[-2],  X.shape[-1]))

    for i in range(2):
        for j in range(2):
            for k in range(3):
                dum = X[(y.sample_odor==i) & (y.test_odor==j) & (y.tasks==tasks[k])]
                mean_dum = np.mean(dum, axis=0)[np.newaxis]

                X_dpca[i, j, k, :dum.shape[0]] = dum
                X_dpca[i, j, k, dum.shape[0]:] = mean_dum

    X_dpca = np.transpose(X_dpca, (3, 4, 0, 1, 2, 5))
    X_dpca_avg = np.nanmean(X_dpca, axis=0)

    dpca = dPCA.dPCA(labels='pcdt', n_components=2, regularizer=1/X.shape[0])
    dpca.protect = ['t']

    axes = (1, 2, 3)
    X_dpca_avg -= np.mean(X_dpca_avg, axis=axes, keepdims=True)

    Z = dpca.fit_transform(X_dpca_avg, X_dpca)
    Z_day.append(Z)
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
: ---------------------------------------------------------------------------
: IndexError                                Traceback (most recent call last)
: Cell In[203], line 6
:       4 tasks = ['DPA',]
:       5 for i_day, day in enumerate(['first', 'last']):
: ----> 6     X = np.vstack(X_mouse[i_mouse][i_day])
:       7     mouse = options['mice'][i_mouse]
:       8     y = y_dfs[(y_dfs.mouse==mouse) & (y_dfs.DAY == day)]
:
: IndexError: list index out of range
:END:

#+begin_src ipython
Z_all = defaultdict(list)
for d in Z_day:
    for k, v in d.items():
        Z_all[k].append(v)

print(np.array(Z_all['p']).shape)
#+end_src

#+RESULTS:
: (2, 2, 2, 2, 3, 84)

#+begin_src ipython
from src.common.plot_utils import add_vlines

ls = ['-', '--', ':']
color = ['r', 'b', 'g']
label = ['Lick', 'No Lick']
pc = ['Sample', 'Test', 'Task']
xtime = np.linspace(0, 14, 84)

fig, ax = plt.subplots(1, 3, figsize=[3* width, height])

n_comp = 0
pair = 0
k = 0

for i in range(2):
        for j in range(2):
                ax[0].plot(xtime, (Z_day[j]['p'][n_comp][pair][i][k] - Z_day[j]['p'][n_comp][1][i][k])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)
                ax[1].plot(xtime, (Z_day[j]['c'][n_comp][pair][i][k] + Z_day[j]['c'][n_comp][1][i][k])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)
                ax[2].plot(xtime, (Z_day[j]['d'][n_comp][pair][i][k] + Z_day[j]['p'][j][n_comp][1][i][k])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)


for k in range(3):
        add_vlines(ax[k])
        ax[k].set_xlim([0, 12])
        ax[k].axhline(0, ls=':')
        ax[k].set_xlabel('Time (s)')
        ax[k].set_ylabel('%s' %pc[k])

plt.legend(fontsize=14, frameon=0)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/dpca/figure_51.png]]

#+begin_src ipython
from src.common.plot_utils import add_vlines

ls = ['-', '--', ':']
color = ['r', 'b', 'g']
label = ['Lick', 'No Lick']
pc = ['Sample', 'Test', 'Task']
xtime = np.linspace(0, 14, 84)

fig, ax = plt.subplots(1, 3, figsize=[3* width, height])

n_comp = 0
pair = 0
k = 1

for i in range(2):
        for j in range(2):
                ax[0].plot(xtime, (Z_day[j]['p'][n_comp][pair][i][k] - Z_day[j]['p'][n_comp][1][i][k])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)
                ax[1].plot(xtime, (Z_day[j]['c'][n_comp][pair][i][k] + Z_day[j]['c'][n_comp][1][i][k])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)
                ax[2].plot(xtime, (Z_day[j]['d'][n_comp][pair][i][k] + Z_day[j]['p'][j][n_comp][1][i][k])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)

for k in range(3):
        add_vlines(ax[k])
        ax[k].set_xlim([0, 12])
        ax[k].axhline(0, ls=':')
        ax[k].set_xlabel('Time (s)')
        ax[k].set_ylabel('%s' %pc[k])

plt.legend(fontsize=14, frameon=0)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/dpca/figure_52.png]]

#+begin_src ipython
from src.common.plot_utils import add_vlines

ls = ['-', '--', ':']
color = ['r', 'b', 'g']
label = ['Lick', 'No Lick']
pc = ['Sample', 'Test', 'Task']
xtime = np.linspace(0, 14, 84)

fig, ax = plt.subplots(1, 3, figsize=[3* width, height])

n_comp = 0
pair = 0
k = -1

for i in range(2):
        for j in range(2):
                ax[0].plot(xtime, (Z_day[j]['p'][n_comp][pair][i][k] - Z_day[j]['p'][n_comp][1][i][k])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)
                ax[1].plot(xtime, (Z_day[j]['c'][n_comp][pair][i][k] + Z_day[j]['c'][n_comp][1][i][k])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)
                ax[2].plot(xtime, (Z_day[j]['d'][n_comp][pair][i][k] + Z_day[j]['p'][j][n_comp][1][i][k])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)

for k in range(3):
        add_vlines(ax[k])
        ax[k].set_xlim([0, 12])
        ax[k].axhline(0, ls=':')
        ax[k].set_xlabel('Time (s)')
        ax[k].set_ylabel('%s' %pc[k])

plt.legend(fontsize=14, frameon=0)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/dpca/figure_53.png]]

#+begin_src ipython

#+end_src

#+RESULTS:

* Sample * Choice * Task * Time

#+begin_src ipython
i_mouse = 7
print(options['mice'][i_mouse])
#+end_src

#+RESULTS:
: ACCM03

#+begin_src ipython
from collections import defaultdict

Z_day = []
tasks = ['DPA', 'DualGo', 'DualNoGo']
for i_day, day in enumerate(['first', 'last']):
    X = np.vstack(X_mouse[i_mouse][i_day])
    mouse = options['mice'][i_mouse]
    y = y_dfs[(y_dfs.mouse==mouse) & (y_dfs.DAY == day)]

    idx_off = (y.laser==0)

    #  n_trials, n_neurons, Z1, Z2, ..., n_time, lets do odor pair * choice
    X_dpca = np.zeros((2, 2, 3, int(X.shape[0]/6), X.shape[-2],  X.shape[-1]))

    for i in range(2):
        for j in range(2):
            for k in range(3):
                dum = X[(y.sample_odor==i) & (y.choice==j) & (y.tasks==tasks[k])]
                mean_dum = np.mean(dum, axis=0)[np.newaxis]

                X_dpca[i, j, k, :dum.shape[0]] = dum
                X_dpca[i, j, k, dum.shape[0]:] = mean_dum

    X_dpca = np.transpose(X_dpca, (3, 4, 0, 1, 2, 5))
    X_dpca_avg = np.nanmean(X_dpca, axis=0)

    dpca = dPCA.dPCA(labels='pcdt', n_components=2, regularizer=1/X.shape[0])
    dpca.protect = ['t']

    axes = (1, 2, 3)
    X_dpca_avg -= np.mean(X_dpca_avg, axis=axes, keepdims=True)

    Z = dpca.fit_transform(X_dpca_avg, X_dpca)
    Z_day.append(Z)
#+end_src

#+RESULTS:

#+begin_src ipython
Z_all = defaultdict(list)
for d in Z_day:
    for k, v in d.items():
        Z_all[k].append(v)

print(np.array(Z_all['p']).shape)
#+end_src

#+RESULTS:
: (2, 2, 2, 2, 3, 84)

#+begin_src ipython
from src.common.plot_utils import add_vlines

ls = ['-', '--', ':']
color = ['r', 'b', 'g']
label = ['Lick', 'No Lick']
pc = ['Sample', 'Choice', 'Task']
xtime = np.linspace(0, 14, 84)

fig, ax = plt.subplots(1, 3, figsize=[3* width, height])

n_comp = 0
pair = 0
k = 0

for i in range(2):
        for j in range(2):
                ax[0].plot(xtime, (Z_day[j]['p'][n_comp][pair][i][k] - Z_day[j]['p'][n_comp][1][i][k])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)
                ax[1].plot(xtime, (Z_day[j]['c'][n_comp][pair][i][k] + Z_day[j]['c'][n_comp][1][i][k])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)
                ax[2].plot(xtime, (Z_day[j]['d'][n_comp][pair][i][k] + Z_day[j]['p'][j][n_comp][1][i][k])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)

for k in range(3):
        add_vlines(ax[k])
        ax[k].set_xlim([0, 12])
        ax[k].axhline(0, ls=':')
        ax[k].set_xlabel('Time (s)')
        ax[k].set_ylabel('%s' %pc[k])

plt.legend(fontsize=14, frameon=0)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/dpca/figure_58.png]]

#+begin_src ipython
from src.common.plot_utils import add_vlines

ls = ['-', '--', ':']
color = ['r', 'b', 'g']
label = ['Lick', 'No Lick']
pc = ['Sample', 'Choice', 'Task']
xtime = np.linspace(0, 14, 84)

fig, ax = plt.subplots(1, 3, figsize=[3* width, height])

n_comp = 0
pair = 0
k = 1

for i in range(2):
        for j in range(2):
                ax[0].plot(xtime, (Z_day[j]['p'][n_comp][pair][i][k] - Z_day[j]['p'][n_comp][1][i][k])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)
                ax[1].plot(xtime, (Z_day[j]['c'][n_comp][pair][i][k] + Z_day[j]['c'][n_comp][1][i][k])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)
                ax[2].plot(xtime, (Z_day[j]['d'][n_comp][pair][i][k] + Z_day[j]['p'][j][n_comp][1][i][k])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)

for k in range(3):
        add_vlines(ax[k])
        ax[k].set_xlim([0, 12])
        ax[k].axhline(0, ls=':')
        ax[k].set_xlabel('Time (s)')
        ax[k].set_ylabel('%s' %pc[k])

plt.legend(fontsize=14, frameon=0)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/dpca/figure_59.png]]

#+begin_src ipython
from src.common.plot_utils import add_vlines

ls = ['-', '--', ':']
color = ['r', 'b', 'g']
label = ['Lick', 'No Lick']
pc = ['Sample', 'Choice', 'Task']
xtime = np.linspace(0, 14, 84)

fig, ax = plt.subplots(1, 3, figsize=[3* width, height])

n_comp = 0
pair = 0
k = -1

for i in range(2):
        for j in range(2):
                ax[0].plot(xtime, (Z_day[j]['p'][n_comp][pair][i][k] - Z_day[j]['p'][n_comp][1][i][k])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)
                ax[1].plot(xtime, (Z_day[j]['c'][n_comp][pair][i][k] + Z_day[j]['c'][n_comp][1][i][k])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)
                ax[2].plot(xtime, (Z_day[j]['d'][n_comp][pair][i][k] + Z_day[j]['p'][j][n_comp][1][i][k])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)

for k in range(3):
        add_vlines(ax[k])
        ax[k].set_xlim([0, 12])
        ax[k].axhline(0, ls=':')
        ax[k].set_xlabel('Time (s)')
        ax[k].set_ylabel('%s' %pc[k])

plt.legend(fontsize=14, frameon=0)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/dpca/figure_60.png]]

#+begin_src ipython

#+end_src

#+RESULTS:

* Sample * Test * Choice * Task * Time

#+begin_src ipython
i_mouse = 7
print(options['mice'][i_mouse])
#+end_src

#+RESULTS:
: ACCM03

#+begin_src ipython
from collections import defaultdict

Z_day = []
tasks = ['DPA', 'DualGo', 'DualNoGo']
for i_day, day in enumerate(['first', 'last']):
    X = np.vstack(X_mouse[i_mouse][i_day])
    mouse = options['mice'][i_mouse]
    y = y_dfs[(y_dfs.mouse==mouse) & (y_dfs.DAY == day)]

    idx_off = (y.laser==0)

    #  n_trials, n_neurons, Z1, Z2, ..., n_time, lets do odor pair * choice
    X_dpca = np.zeros((2, 2, 2, 3, int(X.shape[0]/3), X.shape[-2],  X.shape[-1]))

    for i in range(2):
        for j in range(2):
            for k in range(2):
                for l in range(3):
                    dum = X[(y.sample_odor==i) & (y.test_odor==j)   & (y.choice==k) & (y.tasks==tasks[l])]

                    if dum.shape[0]>0:
                        mean_dum = np.mean(dum, axis=0)[np.newaxis]

                        X_dpca[i, j, k, l, :dum.shape[0]] = dum
                        X_dpca[i, j, k, l, dum.shape[0]:] = mean_dum

    X_dpca = np.transpose(X_dpca, (4, 5, 0, 1, 2, 3, 6))

    X_dpca_avg = np.nanmean(X_dpca, axis=0)

    dpca = dPCA.dPCA(labels='pcdkt', n_components=2, regularizer=1/X.shape[0])
    dpca.protect = ['t']

    axes = (1, 2, 3, 4)
    X_dpca_avg -= np.mean(X_dpca_avg, axis=axes, keepdims=True)

    Z = dpca.fit_transform(X_dpca_avg, X_dpca)
    Z_day.append(Z)
#+end_src

#+RESULTS:

#+begin_src ipython
Z_all = defaultdict(list)
for d in Z_day:
    for k, v in d.items():
        Z_all[k].append(v)

print(np.array(Z_all['p']).shape)
#+end_src

#+RESULTS:
: (2, 2, 2, 2, 2, 3, 84)

#+begin_src ipython
from src.common.plot_utils import add_vlines

ls = ['-', '--', ':']
color = ['r', 'b', 'g']
label = ['Lick', 'No Lick']
pc = ['Sample', 'Test', 'Choice']
xtime = np.linspace(0, 14, 84)

fig, ax = plt.subplots(1, 3, figsize=[3* width, height])

n_comp = 0
pair = 0
k = 0
l=1

for i in range(2):
        for j in range(2):
                ax[0].plot(xtime, (Z_day[j]['p'][n_comp][pair][i][k][l] - Z_day[j]['p'][n_comp][1][i][k][l])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)
                ax[1].plot(xtime, (Z_day[j]['c'][n_comp][pair][i][k][l] + Z_day[j]['c'][n_comp][1][i][k][l])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)
                ax[2].plot(xtime, (Z_day[j]['d'][n_comp][pair][i][k][l] + Z_day[j]['p'][j][n_comp][1][i][k][l])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)

for k in range(3):
        add_vlines(ax[k])
        ax[k].set_xlim([0, 12])
        ax[k].axhline(0, ls=':')
        ax[k].set_xlabel('Time (s)')
        ax[k].set_ylabel('%s' %pc[k])

plt.legend(fontsize=14, frameon=0)
plt.show()
#+end_src

#+RESULTS:
[[file:./figures/dpca/figure_65.png]]

#+begin_src ipython
from src.common.plot_utils import add_vlines

ls = ['-', '--', ':']
color = ['r', 'b', 'g']
label = ['Lick', 'No Lick']
pc = ['Sample', 'Choice', 'Task']
xtime = np.linspace(0, 14, 84)

fig, ax = plt.subplots(1, 3, figsize=[3* width, height])

n_comp = 0
pair = 0
k = 1

for i in range(2):
        for j in range(2):
                ax[0].plot(xtime, (Z_day[j]['p'][n_comp][pair][i][k] - Z_day[j]['p'][n_comp][1][i][k])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)
                ax[1].plot(xtime, (Z_day[j]['c'][n_comp][pair][i][k] + Z_day[j]['c'][n_comp][1][i][k])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)
                ax[2].plot(xtime, (Z_day[j]['d'][n_comp][pair][i][k] + Z_day[j]['p'][j][n_comp][1][i][k])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)

for k in range(3):
        add_vlines(ax[k])
        ax[k].set_xlim([0, 12])
        ax[k].axhline(0, ls=':')
        ax[k].set_xlabel('Time (s)')
        ax[k].set_ylabel('%s' %pc[k])

plt.legend(fontsize=14, frameon=0)
plt.show()
#+end_src


#+begin_src ipython
from src.common.plot_utils import add_vlines

ls = ['-', '--', ':']
color = ['r', 'b', 'g']
label = ['Lick', 'No Lick']
pc = ['Sample', 'Choice', 'Task']
xtime = np.linspace(0, 14, 84)

fig, ax = plt.subplots(1, 3, figsize=[3* width, height])

n_comp = 0
pair = 0
k = -1

for i in range(2):
        for j in range(2):
                ax[0].plot(xtime, (Z_day[j]['p'][n_comp][pair][i][k] - Z_day[j]['p'][n_comp][1][i][k])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)
                ax[1].plot(xtime, (Z_day[j]['c'][n_comp][pair][i][k] + Z_day[j]['c'][n_comp][1][i][k])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)
                ax[2].plot(xtime, (Z_day[j]['d'][n_comp][pair][i][k] + Z_day[j]['p'][j][n_comp][1][i][k])/2, ls = ls[i], label=label[i], color=color[k], alpha=j/2+0.5)

for k in range(3):
        add_vlines(ax[k])
        ax[k].set_xlim([0, 12])
        ax[k].axhline(0, ls=':')
        ax[k].set_xlabel('Time (s)')
        ax[k].set_ylabel('%s' %pc[k])

plt.legend(fontsize=14, frameon=0)
plt.show()
#+end_src



#+begin_src ipython

#+end_src

#+RESULTS:
